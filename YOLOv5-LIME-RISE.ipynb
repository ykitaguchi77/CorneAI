{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5-LIME-RISE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJcOrPki6NXf"
      },
      "source": [
        "#**YOLOv5 LIME CorneAI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd_yXvLX6WEA"
      },
      "source": [
        "##**Setup YOLOv5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "decXjLeF5tvO",
        "outputId": "8b8454e2-de0b-4fc6-a3e5-22a519036e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "62qgDIr46oS9",
        "outputId": "3c1d5d51-21a4-49c8-907d-c2e922739ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-204-bf4a4497fc0f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip uninstall deep_utils -y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch --q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torchvision --q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MjZmDMc6oaL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuseY-WV67TG"
      },
      "source": [
        "#**LIME**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgNy8U2a72ss",
        "outputId": "6b5ff298-acba-492e-b127-85a92cfd99d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install lime --q\n",
        "!pip install scikit-image --q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZNSmW3HHhtO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import cv2\n",
        "import time\n",
        "import traceback\n",
        "import torchvision\n",
        "from lime import lime_image\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "%matplotlib inline\n",
        "\n",
        "# 角膜AIのクラス定義\n",
        "CORNEA_CLASSES = [\n",
        "    \"infection\",\n",
        "    \"normal\",\n",
        "    \"non-infection\",\n",
        "    \"scar\",\n",
        "    \"tumor\",\n",
        "    \"deposit\",\n",
        "    \"APAC\",\n",
        "    \"lens opacity\",\n",
        "    \"bullous\"\n",
        "]\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"\n",
        "    GPUが利用可能な場合はGPUを、そうでない場合はCPUを設定\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    device : torch.device\n",
        "        使用するデバイス\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"GPU not available, using CPU\")\n",
        "    return device\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size=(640, 640),\n",
        "                 names=CORNEA_CLASSES,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        # モデルのロード\n",
        "        print(\"[INFO] Loading cornea detection model...\")\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model loaded successfully\")\n",
        "\n",
        "        # モデルのクラス数を取得と確認\n",
        "        self.nc = int(self.model.nc)\n",
        "        print(f\"[INFO] Number of classes: {self.nc}\")\n",
        "\n",
        "        # クラス名の設定と検証\n",
        "        self.names = names\n",
        "        if len(self.names) != self.nc:\n",
        "            print(f\"[WARNING] Number of class names ({len(self.names)}) does not match model classes ({self.nc})\")\n",
        "        print(f\"[INFO] Using class names: {self.names}\")\n",
        "\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        # Cold start prevention\n",
        "        print(\"[INFO] Performing cold start prevention...\")\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "        print(\"[INFO] Initialization complete\")\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096\n",
        "        max_nms = 30000\n",
        "        time_limit = 10.0\n",
        "        redundant = True\n",
        "        multi_label &= nc > 1\n",
        "        merge = False\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        try:\n",
        "            prediction, logits, _ = self.model(img, augment=False)\n",
        "            prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                          classes=None,\n",
        "                                                          agnostic=self.agnostic)\n",
        "\n",
        "            batch_size = img.shape[0]\n",
        "            self.boxes = [[] for _ in range(batch_size)]\n",
        "            self.class_names = [[] for _ in range(batch_size)]\n",
        "            self.classes = [[] for _ in range(batch_size)]\n",
        "            self.confidences = [[] for _ in range(batch_size)]\n",
        "\n",
        "            for i, det in enumerate(prediction):\n",
        "                if len(det):\n",
        "                    for *xyxy, conf, cls in det:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(self.img_size[1], xyxy[2])\n",
        "                        xyxy[3] = min(self.img_size[0], xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(float(conf.item()))\n",
        "                        cls_idx = int(cls.item())\n",
        "\n",
        "                        if cls_idx >= len(self.names):\n",
        "                            print(f\"[WARNING] Class index {cls_idx} is out of range\")\n",
        "                            cls_idx = 0\n",
        "\n",
        "                        self.classes[i].append(cls_idx)\n",
        "                        self.class_names[i].append(self.names[cls_idx])\n",
        "\n",
        "            return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in forward pass: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return [[[]], [[]], [[]], [[]]], None\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        try:\n",
        "            if len(img.shape) != 4:\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "            im0 = img.astype(np.uint8)\n",
        "            img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "            img = img.transpose((0, 3, 1, 2))\n",
        "            img = np.ascontiguousarray(img)\n",
        "            img = torch.from_numpy(img).to(self.device)\n",
        "            img = img / 255.0\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preprocessing: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "class YOLOLimeExplainer:\n",
        "    def __init__(self, yolo_model, device='cuda', img_size=(640, 640)):\n",
        "        self.model = yolo_model\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    def predict_fn(self, images):\n",
        "        try:\n",
        "            batch_predictions = []\n",
        "            for img in images:\n",
        "                processed_img = self.model.preprocessing(np.expand_dims(img, 0))\n",
        "                if processed_img is None:\n",
        "                    raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    predictions, _ = self.model(processed_img)\n",
        "\n",
        "                class_scores = np.zeros(len(CORNEA_CLASSES))\n",
        "\n",
        "                if predictions[0][0]:\n",
        "                    for cls_idx, conf in zip(predictions[1][0], predictions[3][0]):\n",
        "                        if cls_idx < len(CORNEA_CLASSES):\n",
        "                            class_scores[cls_idx] = max(class_scores[cls_idx], conf)\n",
        "\n",
        "                batch_predictions.append(class_scores)\n",
        "\n",
        "            return np.array(batch_predictions)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction_fn: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return np.zeros((len(images), len(CORNEA_CLASSES)))\n",
        "\n",
        "    def explain_instance(self, image, num_samples=1000, top_labels=5):\n",
        "        try:\n",
        "            if isinstance(image, Image.Image):\n",
        "                image = np.array(image)\n",
        "            if len(image.shape) == 2:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "            explanation = self.explainer.explain_instance(\n",
        "                image,\n",
        "                self.predict_fn,\n",
        "                labels=range(len(CORNEA_CLASSES)),\n",
        "                top_labels=top_labels,\n",
        "                hide_color=0,\n",
        "                num_samples=num_samples\n",
        "            )\n",
        "            return explanation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in explain_instance: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "def visualize_results(explanation, image, class_names=CORNEA_CLASSES, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize LIME explanation results overlaid on the original image\n",
        "\n",
        "    Args:\n",
        "        explanation: LIME explanation object\n",
        "        image: Original image array\n",
        "        class_names: List of class names\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get predictions\n",
        "        prediction = explanation.predict_fn(np.array([image]))[0]\n",
        "\n",
        "        # Sort labels by prediction confidence\n",
        "        sorted_labels = sorted(range(len(prediction)),\n",
        "                             key=lambda x: prediction[x],\n",
        "                             reverse=True)[:2]\n",
        "\n",
        "        # Create a figure with subplots\n",
        "        plt.figure(figsize=(15, 7))\n",
        "\n",
        "        for idx, label in enumerate(sorted_labels):\n",
        "            plt.subplot(1, len(sorted_labels), idx + 1)\n",
        "\n",
        "            # Get the original image and mask from LIME\n",
        "            mask = explanation.local_exp[label]\n",
        "\n",
        "            # Convert the sparse mask to a dense array\n",
        "            dense_mask = np.zeros(explanation.segments.shape, dtype=float)\n",
        "            for i, v in mask:\n",
        "                dense_mask[explanation.segments == i] = v\n",
        "\n",
        "            # Normalize the mask to [0, 1] range\n",
        "            if dense_mask.max() != dense_mask.min():\n",
        "                dense_mask = (dense_mask - dense_mask.min()) / (dense_mask.max() - dense_mask.min())\n",
        "\n",
        "            # Create a colormap (red for positive contributions)\n",
        "            heatmap = np.zeros((dense_mask.shape[0], dense_mask.shape[1], 4))\n",
        "            heatmap[:, :, 2] = dense_mask  # Blue channel\n",
        "            heatmap[:, :, 3] = dense_mask * 1.0  # Alpha channel\n",
        "\n",
        "            # Display original image\n",
        "            plt.imshow(image, alpha=0.8)\n",
        "\n",
        "            # Overlay heatmap\n",
        "            plt.imshow(heatmap, alpha=0.6)\n",
        "\n",
        "            class_name = class_names[label] if label < len(class_names) else f\"Unknown Class {label}\"\n",
        "            plt.title(f'{class_name}\\nConfidence: {prediction[label]:.3f}', fontsize=12)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # if save_path:\n",
        "        #     plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "        #     print(f\"Visualization saved to {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nDetailed Confidence Scores:\")\n",
        "        for label in sorted_labels:\n",
        "            print(f\"{class_names[label]}: {prediction[label]:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in visualization: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "class YOLOLimeExplainer:\n",
        "    def __init__(self, yolo_model, device='cuda', img_size=(640, 640)):\n",
        "        self.model = yolo_model\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    def predict_fn(self, images):\n",
        "        \"\"\"\n",
        "        Modified prediction function with improved image processing\n",
        "        \"\"\"\n",
        "        try:\n",
        "            batch_predictions = []\n",
        "            for img in images:\n",
        "                # Normalize image if needed\n",
        "                if img.max() > 1.0:\n",
        "                    img = img / 255.0\n",
        "\n",
        "                # Convert image format if needed\n",
        "                if len(img.shape) == 2:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "                elif img.shape[2] == 4:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "                # Ensure image is uint8 for preprocessing\n",
        "                img_uint8 = (img * 255).astype(np.uint8)\n",
        "                processed_img = self.model.preprocessing(np.expand_dims(img_uint8, 0))\n",
        "\n",
        "                if processed_img is None:\n",
        "                    raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    predictions, logits = self.model(processed_img)\n",
        "\n",
        "                class_scores = np.zeros(len(CORNEA_CLASSES))\n",
        "\n",
        "                if predictions[0][0]:  # If there are any detections\n",
        "                    if logits is not None and len(logits[0]) > 0:\n",
        "                        # Use logits if available\n",
        "                        probs = torch.nn.functional.softmax(logits[0], dim=1)\n",
        "                        class_scores = probs.mean(dim=0).cpu().numpy()\n",
        "                    else:\n",
        "                        # Fallback to confidence scores\n",
        "                        for cls_idx, conf in zip(predictions[1][0], predictions[3][0]):\n",
        "                            if cls_idx < len(CORNEA_CLASSES):\n",
        "                                class_scores[cls_idx] = max(class_scores[cls_idx], conf)\n",
        "\n",
        "                batch_predictions.append(class_scores)\n",
        "\n",
        "            return np.array(batch_predictions)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction_fn: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return np.zeros((len(images), len(CORNEA_CLASSES)))\n",
        "\n",
        "    def explain_instance(self, image, num_samples=1000, top_labels=5):\n",
        "        \"\"\"\n",
        "        Generate LIME explanation with improved image handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert PIL Image to numpy array if needed\n",
        "            if isinstance(image, Image.Image):\n",
        "                image = np.array(image)\n",
        "\n",
        "            # Normalize image if needed\n",
        "            if image.max() > 1.0:\n",
        "                image = image.astype(float) / 255.0\n",
        "\n",
        "            # Convert image format if needed\n",
        "            if len(image.shape) == 2:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "            # Generate explanation\n",
        "            explanation = self.explainer.explain_instance(\n",
        "                image,\n",
        "                self.predict_fn,\n",
        "                labels=range(len(CORNEA_CLASSES)),\n",
        "                top_labels=top_labels,\n",
        "                hide_color=0,\n",
        "                num_samples=num_samples\n",
        "            )\n",
        "\n",
        "            # Store predict_fn for later use\n",
        "            explanation.predict_fn = self.predict_fn\n",
        "\n",
        "            return explanation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in explain_instance: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "def run_lime_analysis(model_path, img_path, num_samples=500, save_path=None):\n",
        "    \"\"\"\n",
        "    Run LIME analysis with improved visualization\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the YOLO model weights\n",
        "        img_path: Path to the input image\n",
        "        num_samples: Number of samples for LIME analysis\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        print(\"Loading YOLO model...\")\n",
        "        yolo_model = YOLOV5TorchObjectDetector(\n",
        "            model_weight=model_path,\n",
        "            device=device,\n",
        "            img_size=(640, 640),\n",
        "            names=CORNEA_CLASSES\n",
        "        )\n",
        "\n",
        "        print(\"Initializing LIME explainer...\")\n",
        "        lime_explainer = YOLOLimeExplainer(yolo_model)\n",
        "\n",
        "        print(\"Loading and processing image...\")\n",
        "        image = Image.open(img_path)\n",
        "        image_array = np.array(image)\n",
        "\n",
        "        # Normalize image if needed\n",
        "        if image_array.max() > 1.0:\n",
        "            image_array = image_array.astype(float) / 255.0\n",
        "\n",
        "        print(f\"Running LIME analysis with {num_samples} samples...\")\n",
        "        explanation = lime_explainer.explain_instance(\n",
        "            image_array,\n",
        "            num_samples=num_samples,\n",
        "            top_labels=3\n",
        "        )\n",
        "\n",
        "        if explanation is None:\n",
        "            print(\"Failed to generate explanation\")\n",
        "            return None\n",
        "\n",
        "        print(\"Visualizing results...\")\n",
        "        visualize_results(explanation, image_array, save_path=save_path)\n",
        "\n",
        "        return explanation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in run_lime_analysis: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Usage example:\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/1.jpg\"\n",
        "    save_path = \"lime_explanation.png\"  # Optional\n",
        "\n",
        "    explanation = run_lime_analysis(\n",
        "        model_path=model_path,\n",
        "        img_path=img_path,\n",
        "        num_samples=500,\n",
        "        save_path=save_path\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtdw0Rfb7v4"
      },
      "source": [
        "#**RISE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "W5si01fcjXSt"
      },
      "outputs": [],
      "source": [
        "class YOLOv5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOv5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thresh <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thresh  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size, xyxy[2])  # 修正箇所\n",
        "                    xyxy[3] = min(self.img_size, xyxy[3])  # 修正箇所\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEhFanutSd3L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6knjheoqahVk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "class RISEForYOLOv5(nn.Module):\n",
        "    def __init__(self, model, detector, device, gpu_batch=20):\n",
        "        super(RISEForYOLOv5, self).__init__()\n",
        "        self.model = model\n",
        "        self.detector = detector\n",
        "        self.device = device\n",
        "        self.gpu_batch = gpu_batch\n",
        "        self.masks = None\n",
        "        self.N = None\n",
        "        self.p1 = None\n",
        "        self.input_size = None  # 初期値をNoneに設定\n",
        "\n",
        "    def forward(self, img, boxes, scores, classes):\n",
        "        B, C, H, W = img.size()\n",
        "        self.input_size = (H, W)  # 入力画像サイズを取得\n",
        "\n",
        "        # ここで画像をリサイズし、パディング情報を取得\n",
        "        resized_imgs = []\n",
        "        pad_info = []\n",
        "        for b in range(B):\n",
        "            img_np = img[b].cpu().numpy().transpose(1, 2, 0)\n",
        "            img_np, ratio, (dw, dh) = letterbox(img_np, new_shape=self.input_size, auto=False, scaleFill=False)\n",
        "            resized_imgs.append(img_np)\n",
        "            pad_info.append((ratio, dw, dh))\n",
        "\n",
        "        # リサイズされた画像をテンソルに変換\n",
        "        img_resized = np.stack(resized_imgs, axis=0)\n",
        "        img_resized = img_resized.transpose(0, 3, 1, 2)\n",
        "        img_resized = torch.from_numpy(img_resized).to(self.device).float()\n",
        "\n",
        "        # 以降の処理では img_resized を使用\n",
        "        saliency_maps = []\n",
        "\n",
        "        for b in range(B):\n",
        "            if len(boxes[b]) == 0 or len(scores[b]) == 0:\n",
        "                print(f\"[INFO] No detections for batch {b}\")\n",
        "                saliency_maps.append(None)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                scores_tensor = torch.tensor(scores[b])\n",
        "                if scores_tensor.numel() == 0:\n",
        "                    print(f\"[INFO] Empty scores tensor for batch {b}\")\n",
        "                    saliency_maps.append(None)\n",
        "                    continue\n",
        "\n",
        "                top1_idx = torch.argmax(scores_tensor)\n",
        "                cls = classes[b][top1_idx]\n",
        "                score = scores[b][top1_idx]\n",
        "                bbox = boxes[b][top1_idx]\n",
        "\n",
        "                # バウンディングボックスの座標をリサイズ・パディングに合わせて変換\n",
        "                ratio, dw, dh = pad_info[b]\n",
        "                bbox_resized = self.adjust_bbox(bbox, ratio, dw, dh)\n",
        "\n",
        "                saliency_map = self.apply_rise(img_resized[b], cls, score, bbox_resized)\n",
        "\n",
        "                saliency_maps.append(saliency_map)\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Error processing batch {b}: {str(e)}\")\n",
        "                saliency_maps.append(None)\n",
        "\n",
        "        return saliency_maps\n",
        "\n",
        "    def adjust_bbox(self, bbox, ratio, dw, dh):\n",
        "        # 元のバウンディングボックスの座標をリサイズ・パディングに合わせて変換\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        x1 = x1 * ratio[0] + dw\n",
        "        x2 = x2 * ratio[0] + dw\n",
        "        y1 = y1 * ratio[1] + dh\n",
        "        y2 = y2 * ratio[1] + dh\n",
        "        return [int(x1), int(y1), int(x2), int(y2)]\n",
        "\n",
        "\n",
        "    def apply_rise(self, img, cls, score, bbox):\n",
        "        N = self.N\n",
        "        _, H, W = img.size()\n",
        "        self.input_size = (H, W)\n",
        "\n",
        "        if self.masks is None or self.masks.shape[2:] != (H, W):\n",
        "            # マスクを再生成\n",
        "            print(f\"[INFO] Regenerating masks for size ({H}, {W})\")\n",
        "            self.generate_masks(N=self.N, s=8, p1=0.1, img_size=(H, W))\n",
        "\n",
        "        stack = torch.mul(self.masks, img.unsqueeze(0))  # N x C x H x W\n",
        "\n",
        "        scores = torch.zeros(N, device=self.device)\n",
        "\n",
        "        for i in range(0, N, self.gpu_batch):\n",
        "            with torch.no_grad():\n",
        "                batch_imgs = stack[i:min(i + self.gpu_batch, N)]\n",
        "                prediction, logits, _ = self.model(batch_imgs, augment=False)\n",
        "\n",
        "                outputs, logits_output = self.detector.non_max_suppression(\n",
        "                    prediction,\n",
        "                    logits,\n",
        "                    conf_thres=self.detector.confidence,\n",
        "                    iou_thres=self.detector.iou_thresh,\n",
        "                    classes=None,\n",
        "                    agnostic=self.detector.agnostic\n",
        "                )\n",
        "\n",
        "                for j, detections in enumerate(outputs):\n",
        "                    idx = i + j\n",
        "                    if detections is not None and len(detections):\n",
        "                        detections = detections[detections[:, 5] == cls]\n",
        "                        if len(detections):\n",
        "                            bbox_tensor = torch.tensor([bbox], device=self.device).float()\n",
        "                            ious = box_iou(detections[:, :4], bbox_tensor)\n",
        "                            max_iou, max_idx = ious.max(0)\n",
        "                            if max_iou > 0.5:\n",
        "                                scores[idx] = detections[max_idx, 4]\n",
        "\n",
        "        saliency = torch.matmul(scores.unsqueeze(0), self.masks.view(N, -1))  # 1 x (H*W)\n",
        "        saliency = saliency.view(H, W)\n",
        "        saliency = saliency / (N * self.p1)\n",
        "        saliency_map = saliency.cpu().numpy()\n",
        "\n",
        "        if saliency_map.max() > saliency_map.min():\n",
        "            saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "        else:\n",
        "            saliency_map = np.zeros_like(saliency_map)\n",
        "\n",
        "        return saliency_map\n",
        "\n",
        "\n",
        "    def generate_masks(self, N, s, p1, img_size=None, savepath=None):\n",
        "        if img_size is None:\n",
        "            if self.input_size is not None:\n",
        "                img_size = self.input_size  # 既に設定されている場合\n",
        "            else:\n",
        "                raise ValueError(\"img_size must be provided if self.input_size is not set.\")\n",
        "        else:\n",
        "            self.input_size = img_size  # img_sizeをself.input_sizeに設定\n",
        "\n",
        "        # 以降のコードはそのまま\n",
        "        cell_size = np.ceil(np.array(img_size) / s)\n",
        "        up_size = (s + 1) * cell_size\n",
        "\n",
        "        grid = np.random.rand(N, s, s) < p1\n",
        "        grid = grid.astype('float32')\n",
        "\n",
        "        self.masks = np.empty((N, *img_size))\n",
        "\n",
        "        for i in tqdm(range(N), desc='Generating masks'):\n",
        "            upsampled = cv2.resize(grid[i].astype(np.float32),\n",
        "                                   (img_size[1], img_size[0]),\n",
        "                                   interpolation=cv2.INTER_LINEAR)\n",
        "            self.masks[i] = upsampled\n",
        "\n",
        "        self.masks = self.masks.reshape(-1, 1, *img_size)\n",
        "        if savepath is not None:\n",
        "            np.save(savepath, self.masks)\n",
        "        self.masks = torch.from_numpy(self.masks).float().to(self.device)\n",
        "        self.N = N\n",
        "        self.p1 = p1\n",
        "\n",
        "    def load_masks(self, filepath):\n",
        "        self.masks = np.load(filepath)\n",
        "        self.masks = torch.from_numpy(self.masks).float().to(self.device)\n",
        "        self.N = self.masks.shape[0]\n",
        "        self.input_size = self.masks.shape[2:]  # マスクのサイズから入力サイズを取得\n",
        "\n",
        "\n",
        "    def crop_image(self, img, bbox):\n",
        "        \"\"\"\n",
        "        検出されたボックスに基づいて画像をクロップし、letterboxで固定サイズにリサイズします。\n",
        "        \"\"\"\n",
        "        x1, y1, x2, y2 = map(int, bbox)\n",
        "        # ボックスが画像内に収まるように調整\n",
        "        x1 = max(0, x1)\n",
        "        y1 = max(0, y1)\n",
        "        x2 = min(self.input_size[1], x2)  # width\n",
        "        y2 = min(self.input_size[0], y2)  # height\n",
        "\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            return torch.tensor([]).to(self.device), (x1, y1, x2, y2)\n",
        "\n",
        "        # パディングを追加して完全なサイズを維持\n",
        "        cropped = img[:, :, :]  # 全体の画像を保持\n",
        "        # letterboxを適用\n",
        "        cropped_np = cropped.cpu().numpy().transpose(1, 2, 0)\n",
        "        cropped_np = (cropped_np * 255.0).astype(np.uint8)\n",
        "\n",
        "        # letterboxの適用\n",
        "        cropped_np = letterbox(cropped_np, new_shape=self.input_size, stride=32, auto=False)[0]\n",
        "\n",
        "        cropped_np = cropped_np[:, :, ::-1].transpose(2, 0, 1)\n",
        "        cropped_np = np.ascontiguousarray(cropped_np)\n",
        "        cropped_tensor = torch.from_numpy(cropped_np).float().to(self.device) / 255.0\n",
        "        if cropped_tensor.ndimension() == 3:\n",
        "            cropped_tensor = cropped_tensor.unsqueeze(0)\n",
        "        return cropped_tensor.squeeze(0), (x1, y1, x2, y2)\n",
        "\n",
        "\n",
        "\n",
        "    def expand_saliency_map(self, saliency_map, bbox_coords, H, W):\n",
        "        \"\"\"\n",
        "        クロップしたサリエンシーマップを元の画像サイズに拡大します。\n",
        "        \"\"\"\n",
        "        x1, y1, x2, y2 = bbox_coords\n",
        "\n",
        "        # 座標を画像サイズ内に制限\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(W, x2), min(H, y2)\n",
        "\n",
        "        # バウンディングボックスのサイズを計算\n",
        "        box_height = y2 - y1\n",
        "        box_width = x2 - x1\n",
        "\n",
        "        if box_height <= 0 or box_width <= 0:\n",
        "            print(f\"Invalid box dimensions: width={box_width}, height={box_height}\")\n",
        "            return np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "        # cv2.resizeを使用して正確なサイズにリサイズ\n",
        "        try:\n",
        "            sal_map_resized = cv2.resize(saliency_map, (box_width, box_height),\n",
        "                                      interpolation=cv2.INTER_LINEAR)\n",
        "        except Exception as e:\n",
        "            print(f\"Resize error: {e}\")\n",
        "            print(f\"Source shape: {saliency_map.shape}\")\n",
        "            print(f\"Target shape: ({box_height}, {box_width})\")\n",
        "            return np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "        # 出力マップを初期化\n",
        "        sal_map_full = np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "        try:\n",
        "            sal_map_full[y1:y2, x1:x2] = sal_map_resized\n",
        "        except ValueError as e:\n",
        "            print(f\"Assignment error: {e}\")\n",
        "            print(f\"Resized shape: {sal_map_resized.shape}\")\n",
        "            print(f\"Target region shape: ({y2-y1}, {x2-x1})\")\n",
        "            return np.zeros((H, W), dtype=np.float32)\n",
        "\n",
        "        # 正規化\n",
        "        if sal_map_full.max() > sal_map_full.min():\n",
        "            sal_map_full = (sal_map_full - sal_map_full.min()) / (sal_map_full.max() - sal_map_full.min())\n",
        "\n",
        "        return sal_map_full\n",
        "\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 rise_explainer=None,  # RISEインスタンスを追加\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        \"\"\"\n",
        "        YOLOv5オブジェクト検出器を初期化します。\n",
        "\n",
        "        Args:\n",
        "            model_weight (str): YOLOv5の重みファイルのパス\n",
        "            device (str): 使用するデバイス ('cuda' または 'cpu')\n",
        "            img_size (int or tuple): 入力画像のサイズ\n",
        "            rise_explainer (RISEForYOLOv5, optional): RISEインスタンス\n",
        "            names (list, optional): クラス名のリスト\n",
        "            mode (str, optional): モード ('eval' または 'train')\n",
        "            confidence (float, optional): 信頼度閾値\n",
        "            iou_thresh (float, optional): IoU閾値\n",
        "            agnostic_nms (bool, optional): クラス非依存のNMSを使用するか\n",
        "        \"\"\"\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = attempt_load(model_weight)  # map_locationを使用\n",
        "        print(\"[INFO] YOLOv5 model loaded\")\n",
        "        self.model.to(device)\n",
        "        self.model.eval()  # 推論モード\n",
        "        self.rise_explainer = rise_explainer  # RISEインスタンスの保存\n",
        "\n",
        "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        # クラス名の設定\n",
        "        if names is None:\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
        "                          'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
        "                          'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "                          'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "                          'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
        "                          'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
        "                          'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "                          'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # 冷開始防止\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        with torch.no_grad():\n",
        "            self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # 画像インデックス、画像推論\n",
        "            x = x[xc[xi]]  # 信頼度\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # ボックス\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = YOLOV5TorchObjectDetector.xywh2xyxy_custom(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # ボックス数\n",
        "            if not n:  # ボックスなし\n",
        "                continue\n",
        "            elif n > max_nms:  # 過剰ボックス\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # 信頼度順にソート\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else 4096)  # クラス\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # ボックス（クラスでオフセット）、スコア\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # 検出数制限\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS（ボックスを重み付き平均でマージ）\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # IoU行列\n",
        "                weights = iou * scores[None]  # ボックス重み\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # マージされたボックス\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # 冗長性を要求\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # 時間制限を超過\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def xywh2xyxy_custom(x):\n",
        "        \"\"\"\n",
        "        [x, y, w, h] を [x1, y1, x2, y2] に変換します。\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): [x, y, w, h]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: [x1, y1, x2, y2]\n",
        "        \"\"\"\n",
        "        y = x.clone()\n",
        "        y[:, 0] = x[:, 0] - x[:, 2] / 2  # x1\n",
        "        y[:, 1] = x[:, 1] - x[:, 3] / 2  # y1\n",
        "        y[:, 2] = x[:, 0] + x[:, 2] / 2  # x2\n",
        "        y[:, 3] = x[:, 1] + x[:, 3] / 2  # y2\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "    def forward(self, img):\n",
        "        with torch.no_grad():\n",
        "            prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "\n",
        "        batch_size = img.size(0)\n",
        "        self.boxes = [[] for _ in range(batch_size)]\n",
        "        self.class_names = [[] for _ in range(batch_size)]\n",
        "        self.classes = [[] for _ in range(batch_size)]\n",
        "        self.confidences = [[] for _ in range(batch_size)]\n",
        "\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[1], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[0], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(conf.item())\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "            else:\n",
        "                print(f\"No detections for batch {i}\")\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "# 補助関数\n",
        "# Also make sure the get_res_img function properly handles the ratio, dw, and dh parameters\n",
        "def get_res_img(bbox, masks, res_img, ratio, dw, dh):\n",
        "    \"\"\"\n",
        "    サリエンシーマップをオリジナル画像に重ね合わせます。\n",
        "\n",
        "    Args:\n",
        "        bbox (list): [x1, y1, x2, y2] - モデル出力のバウンディングボックス\n",
        "        masks (list): サリエンシーマップのリスト\n",
        "        res_img (np.ndarray): BGR形式の元画像\n",
        "        ratio (tuple): (リサイズ時の幅比率, 高さ比率)\n",
        "        dw (float): 横方向のパディング\n",
        "        dh (float): 縦方向のパディング\n",
        "\n",
        "    Returns:\n",
        "        tuple: 重ね合わせた画像と最後のヒートマップ\n",
        "    \"\"\"\n",
        "    heatmap_resized = None\n",
        "    H, W = res_img.shape[:2]\n",
        "\n",
        "    # バウンディングボックスの座標を元の画像サイズに変換\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    x1 = (x1 - dw) / ratio[0]\n",
        "    x2 = (x2 - dw) / ratio[0]\n",
        "    y1 = (y1 - dh) / ratio[1]\n",
        "    y2 = (y2 - dh) / ratio[1]\n",
        "\n",
        "    # 整数に変換して範囲を調整\n",
        "    x1, y1, x2, y2 = map(int, [max(0, x1), max(0, y1), min(W, x2), min(H, y2)])\n",
        "\n",
        "    box_h = y2 - y1\n",
        "    box_w = x2 - x1\n",
        "\n",
        "    if box_h <= 0 or box_w <= 0:\n",
        "        print(f\"Invalid box dimensions: width={box_w}, height={box_h}\")\n",
        "        return res_img, np.zeros((1, 1, 3), dtype=np.uint8)\n",
        "\n",
        "    for mask in masks:\n",
        "        if mask is None:\n",
        "            continue\n",
        "\n",
        "        mask = mask.squeeze().astype(np.float32)\n",
        "        if mask.max() > mask.min():\n",
        "            mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
        "\n",
        "        try:\n",
        "            resized_mask = cv2.resize(mask, (box_w, box_h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            full_mask = np.zeros((H, W), dtype=np.float32)\n",
        "            full_mask[y1:y2, x1:x2] = resized_mask\n",
        "\n",
        "            heat_colors = cv2.applyColorMap((full_mask * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "\n",
        "            result_img = res_img.copy()\n",
        "            alpha = 0.7\n",
        "            beta = 0.5\n",
        "\n",
        "            result_img[y1:y2, x1:x2] = cv2.addWeighted(\n",
        "                res_img[y1:y2, x1:x2], alpha,\n",
        "                heat_colors[y1:y2, x1:x2], beta, 0\n",
        "            )\n",
        "\n",
        "            gamma = 1.2\n",
        "            result_img[y1:y2, x1:x2] = np.clip(\n",
        "                result_img[y1:y2, x1:x2] * gamma, 0, 255\n",
        "            ).astype(np.uint8)\n",
        "\n",
        "            res_img = result_img\n",
        "            heatmap_resized = heat_colors\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_res_img: {e}\")\n",
        "            print(f\"Mask shape: {mask.shape}\")\n",
        "            print(f\"Box dimensions: width={box_w}, height={box_h}\")\n",
        "\n",
        "    return res_img, heatmap_resized if heatmap_resized is not None else np.zeros((1, 1, 3), dtype=np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    \"\"\"\n",
        "    バウンディングボックスとクラス名を画像に描画します。\n",
        "\n",
        "    Args:\n",
        "        bbox (list): [x1, y1, x2, y2]\n",
        "        cls_name (str): クラス名\n",
        "        res_img (np.ndarray): 画像\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: 描画後の画像\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # ボックスを描画\n",
        "    cv2.rectangle(res_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    # テキストの背景ボックス\n",
        "    (text_width, text_height), _ = cv2.getTextSize(cls_name, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "    cv2.rectangle(res_img, (x1, y1 - text_height - 4), (x1 + text_width, y1), (0, 255, 0), -1)\n",
        "\n",
        "    # テキストを描画\n",
        "    cv2.putText(res_img, cls_name, (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    \"\"\"\n",
        "    画像を縦に連結します。\n",
        "\n",
        "    Args:\n",
        "        images (list): 連結する画像のリスト\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: 連結後の画像\n",
        "    \"\"\"\n",
        "    # 有効な画像のみを連結\n",
        "    valid_images = [img for img in images if img.size != 0]\n",
        "    if not valid_images:\n",
        "        return np.array([])\n",
        "    return np.vstack(valid_images)\n",
        "\n",
        "def main_with_rise(img_path, model, rise_explainer, names, output_dir):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"[ERROR] Failed to read image: {img_path}\")\n",
        "        return\n",
        "\n",
        "    # 画像をリサイズしてパディング情報を取得\n",
        "    img_resized, ratio, (dw, dh) = letterbox(img, new_shape=model.img_size, auto=False, scaleFill=False)\n",
        "\n",
        "    # 画像をモデルの入力フォーマットに変換\n",
        "    img_resized = img_resized[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB and HWC to CHW\n",
        "    img_resized = np.ascontiguousarray(img_resized)\n",
        "    torch_img = torch.from_numpy(img_resized).to(model.device).float().unsqueeze(0) / 255.0\n",
        "\n",
        "    predictions, _ = model(torch_img)\n",
        "    boxes, classes, confidences = predictions[0], predictions[1], predictions[3]\n",
        "\n",
        "    if all(len(b) == 0 for b in boxes):\n",
        "        print(\"[WARNING] No detections found in the image\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        saliency_maps = rise_explainer(torch_img, boxes, confidences, classes)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to generate saliency maps: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    original_bgr = img.copy()\n",
        "    images = [original_bgr]\n",
        "\n",
        "    for b, sal_map in enumerate(saliency_maps):\n",
        "        if sal_map is None:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if len(confidences[b]) == 0:\n",
        "                continue\n",
        "\n",
        "            top1_idx = torch.argmax(torch.tensor(confidences[b])).item()\n",
        "            bbox = boxes[b][top1_idx]\n",
        "            cls = classes[b][top1_idx]\n",
        "            cls_name = names[cls] if names else str(cls)\n",
        "\n",
        "            # バウンディングボックスの座標を元の画像サイズに変換\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            x1 = (x1 - dw) / ratio[0]\n",
        "            x2 = (x2 - dw) / ratio[0]\n",
        "            y1 = (y1 - dh) / ratio[1]\n",
        "            y2 = (y2 - dh) / ratio[1]\n",
        "            bbox_original = [int(x1), int(y1), int(x2), int(y2)]\n",
        "\n",
        "            vis_img = original_bgr.copy()\n",
        "            # Pass all required arguments to get_res_img\n",
        "            vis_img, heatmap = get_res_img(bbox_original, [sal_map], vis_img, ratio, dw, dh)\n",
        "            vis_img = put_text_box(bbox_original, cls_name, vis_img)\n",
        "            images.append(vis_img)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Error processing visualization for batch {b}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    if final_image.size == 0:\n",
        "        print(\"[WARNING] No valid images to display.\")\n",
        "        return\n",
        "\n",
        "    img_name = split_extension(os.path.basename(img_path), suffix='-RISE')\n",
        "    output_path = os.path.join(output_dir, img_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cv2.imwrite(output_path, final_image)\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "\n",
        "\n",
        "# 実行スクリプト\n",
        "\n",
        "def run_rise_on_yolov5():\n",
        "    # パラメータの設定\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"  # YOLOv5の重みファイルのパス\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = 640  # YOLOv5の入力サイズ\n",
        "    maskspath = 'yolov5_rise_masks.npy'  # RISEマスクの保存パス\n",
        "    generate_new_masks = True  # マスクを再生成する場合はTrue\n",
        "\n",
        "    # YOLOv5のオブジェクト検出インスタンスの作成\n",
        "    detector = YOLOV5TorchObjectDetector(\n",
        "        model_weight=model_path,\n",
        "        device=device,\n",
        "        img_size=(img_size, img_size),\n",
        "        names=names,\n",
        "        mode='eval',\n",
        "        confidence=0.25,\n",
        "        iou_thresh=0.45,\n",
        "        agnostic_nms=False\n",
        "    )\n",
        "\n",
        "    # RISEインスタンスの作成（deviceを渡す）\n",
        "    rise_explainer = RISEForYOLOv5(model=None, detector=detector,  device=device, gpu_batch=4)\n",
        "\n",
        "    # マスクの生成またはロード\n",
        "    if generate_new_masks or not os.path.isfile(maskspath):\n",
        "        yolov5_model = attempt_load(model_path)\n",
        "        yolov5_model.to(device)\n",
        "        yolov5_model.eval()\n",
        "\n",
        "        rise_explainer.model = yolov5_model  # RISEにYOLOv5モデルをセット\n",
        "        rise_explainer.generate_masks(N=500, s=8, p1=0.1, img_size=(img_size, img_size), savepath=maskspath)\n",
        "    else:\n",
        "        yolov5_model = attempt_load(model_path)  # map_locationを使用\n",
        "        yolov5_model.to(device)\n",
        "        yolov5_model.eval()\n",
        "\n",
        "        rise_explainer.model = yolov5_model  # RISEにYOLOv5モデルをセット\n",
        "        rise_explainer.load_masks(maskspath)\n",
        "        print('RISE masks loaded.')\n",
        "\n",
        "    # テスト画像のパス\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/24.jpg\"\n",
        "    output_dir = \"/content/output\"  # 出力ディレクトリのパス\n",
        "\n",
        "    # RISEとYOLOv5を統合したメイン処理の実行\n",
        "    main_with_rise(img_path, detector, rise_explainer, names, output_dir)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_rise_on_yolov5()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_lMlhZmT0TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BaXmFN8xT0VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0w3vQKbe8lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ],
      "metadata": {
        "id": "xhmS66Eze8mz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "uuseY-WV67TG"
      ],
      "authorship_tag": "ABX9TyNGZeSAJGSOIxG3aK0BRPYu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}