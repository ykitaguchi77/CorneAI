{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4tWeUuc+zrhhv04XL+OTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/Web_image_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**画像のスクレイピング**\n",
        "\n",
        "準備：\n",
        "- Microsoft AZUREに登録\n",
        "\n",
        "    https://learn.microsoft.com/ja-jp/azure/cognitive-services/bing-web-search/\n",
        "\n",
        "- 左のタブ → リソースの作成 → Bing Search v7を取得\n",
        "\n",
        "- ダッシュボード → キーとエンドポイントからキーを取得する\n"
      ],
      "metadata": {
        "id": "25UMQJsL7UPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "    #text = file.read()\n",
        "    i=1\n",
        "    key = []\n",
        "    while True:\n",
        "        include_break_line = file.readline() #改行が含まれた行\n",
        "        line = include_break_line.rstrip() #改行を取り除く\n",
        "        if line: #keyの読み込み\n",
        "            #print(f'{i}行目：{line}')\n",
        "            key.append(line)\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "bing_api_key = key[13]"
      ],
      "metadata": {
        "id": "Sa4lK_QK7-Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0532dfc8-647a-4f04-c7d3-349dbbf6ccce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import requests\n",
        "import cv2\n",
        "\n",
        "API_KEY = f\"{bing_api_key}\"\n",
        "MAX_RESULTS = 1\n",
        "GROUP_SIZE = 1 #GROUP_SIZE×5が一度にリクエストされる\n",
        "\n",
        "# 取得したエンドポイントURL\n",
        "URL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
        "OUTPUT = '/content/drive/MyDrive/Deep_learning/CorneAI_osaka'\n",
        "\n",
        "if not os.path.isdir(OUTPUT):\n",
        "    os.mkdir(OUTPUT)\n",
        "\n",
        "search_terms = [\"corneal infection\", \"healthy eye\", \"autoimmune keratitis\", \"corneal scar\", \"corneal conjunctival tumor\",\n",
        "                \"corneal deposit\", \"angular closure glaucoma photo\", \"cataract\", \"bullous keratopathy\"]\n",
        "\n",
        "# set the output csv file name\n",
        "csv_file = \"url_list.csv\"\n",
        "\n",
        "# create the csv file and write the headers\n",
        "with open(csv_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Search term', 'Image number', 'Image URL', 'Website URL'])\n",
        "\n",
        "# loop over each search term and download images\n",
        "for term in search_terms:\n",
        "    print(f\"[INFO] searching Bing API for '{term}'\")\n",
        "\n",
        "    # create the directory to save the images for the current search term\n",
        "    output_dir = os.path.join(OUTPUT, term)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "    params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\": \"Photo\", \"color\": \"ColorOnly\"}\n",
        "\n",
        "    # make the search\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "\n",
        "    # grab the results from the search, including the total number of\n",
        "    # estimated results returned by the Bing API\n",
        "    results = search.json()\n",
        "    est_num_results = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "    print(f\"[INFO] {est_num_results} total results for '{term}'\")\n",
        "\n",
        "    # initialize the total number of images downloaded thus far\n",
        "    total = 0\n",
        "    # keep track of downloaded image URLs\n",
        "    downloaded_urls = set()\n",
        "\n",
        "    # loop over each search term and download images\n",
        "for term in search_terms:\n",
        "    print(f\"[INFO] searching Bing API for '{term}'\")\n",
        "\n",
        "    # create the directory to save the images for the current search term\n",
        "    output_dir = os.path.join(OUTPUT, term)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "    params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\": \"Photo\", \"color\": \"ColorOnly\"}\n",
        "\n",
        "    # make the search\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "\n",
        "    # grab the results from the search, including the total number of\n",
        "    # estimated results returned by the Bing API\n",
        "    results = search.json()\n",
        "    est_num_results = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "    print(f\"[INFO] {est_num_results} total results for '{term}'\")\n",
        "\n",
        "    # Determine an appropriate GROUP_SIZE based on the estimated number of results\n",
        "    GROUP_SIZE = est_num_results//5 # In this example, we divide the estimated number of results by 5\n",
        "    if GROUP_SIZE == 0:\n",
        "        GROUP_SIZE = 1 # Make sure that GROUP_SIZE is at least 1\n",
        "    print(f\"[INFO] GROUP_SIZE for '{term}': {GROUP_SIZE}\")\n",
        "\n",
        "    # loop over the estimated number of results in `GROUP_SIZE` groups\n",
        "    total = 0\n",
        "    for offset in range(0, est_num_results, GROUP_SIZE):\n",
        "        # update the search parameters using the current offset, then\n",
        "        # make the request to fetch the results\n",
        "        params[\"offset\"] = offset\n",
        "        params[\"count\"] = GROUP_SIZE # Update the count parameter with the appropriate GROUP_SIZE\n",
        "        search = requests.get(URL, headers=headers, params=params)\n",
        "        search.raise_for_status()\n",
        "        results = search.json()\n",
        "\n",
        "        # loop over the results\n",
        "        for v in results[\"value\"]:\n",
        "            # try to download the image\n",
        "            try:\n",
        "                # make a request to download the image\n",
        "                print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n",
        "                r = requests.get(v[\"contentUrl\"], timeout=30)\n",
        "\n",
        "                # build the path to the output image\n",
        "                ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
        "                filename = f\"{term}_{str(total).zfill(3)}{ext}\"\n",
        "                output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "                # write the image to disk\n",
        "                with open(output_path, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "\n",
        "                # write the URL to the csv file\n",
        "                with open(f\"{OUTPUT}/{csv_file}\", 'a', newline='') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([term, str(total), v[\"contentUrl\"], URL])\n",
        "\n",
        "                # increment the total number of images downloaded\n",
        "                total += 1\n",
        "\n",
        "                # break out of the loop if we have downloaded the maximum number of images\n",
        "                if total == MAX_RESULTS:\n",
        "                    break\n"
      ],
      "metadata": {
        "id": "VVQ5tJTgRI_V",
        "outputId": "32ed244a-2695-486e-a36f-50c0ef89ab0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-f3d6325d8990>\"\u001b[0;36m, line \u001b[0;32m121\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kM1CzjjRJA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMeRNv2iRJCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "from requests import exceptions\n",
        "import argparse\n",
        "import requests\n",
        "import cv2\n",
        "\n",
        "\n",
        "API_KEY = f\"{bing_api_key}\"\n",
        "MAX_RESULTS = 3\n",
        "GROUP_SIZE = 1 #GROUP_SIZE×5が一度にリクエストされる\n",
        "\n",
        "# 取得したエンドポイントURL\n",
        "URL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
        "OUTPUT = '/content/drive/MyDrive/Deep_learning/CorneAI_osaka'\n",
        "\n",
        "if not os.path.isdir(OUTPUT):\n",
        "    os.mkdir(OUTPUT)\n",
        "\n",
        "EXCEPTIONS = set([IOError, FileNotFoundError,\n",
        "                  exceptions.RequestException, exceptions.HTTPError,\n",
        "                  exceptions.ConnectionError, exceptions.Timeout])\n",
        "\n",
        "search_terms = [\"corneal infection\", \"healthy eye\", \"autoimmune keratitis\", \"corneal scar\", \"corneal conjunctival tumor\",\n",
        "                \"corneal deposit\", \"angular closure glaucoma photo\", \"cataract\", \"bullous keratopathy\"]\n",
        "\n",
        "# set the output csv file name\n",
        "csv_file = \"url_list.csv\"\n",
        "\n",
        "# create the csv file and write the headers\n",
        "with open(csv_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Search term', 'Image number', 'Image URL', 'Website URL'])\n",
        "\n",
        "# loop over each search term and download images\n",
        "for term in search_terms:\n",
        "    print(f\"[INFO] searching Bing API for '{term}'\")\n",
        "\n",
        "    # create the directory to save the images for the current search term\n",
        "    output_dir = os.path.join(OUTPUT, term)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "    params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\": \"Photo\", \"color\": \"ColorOnly\"}\n",
        "\n",
        "    # make the search\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "\n",
        "    # grab the results from the search, including the total number of\n",
        "    # estimated results returned by the Bing API\n",
        "    results = search.json()\n",
        "    est_num_results = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "    print(f\"[INFO] {est_num_results} total results for '{term}'\")\n",
        "\n",
        "    # initialize the total number of images downloaded thus far\n",
        "    total = 0\n",
        "\n",
        "\n",
        "\n",
        "# loop over each search term and download images\n",
        "for term in search_terms:\n",
        "    \n",
        "    download_urls = [] #画像重複\n",
        "\n",
        "    print(f\"[INFO] searching Bing API for '{term}'\")\n",
        "\n",
        "    # create the directory to save the images for the current search term\n",
        "    output_dir = os.path.join(OUTPUT, term)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "    params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\": \"Photo\", \"color\": \"ColorOnly\"}\n",
        "\n",
        "    # make the search\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "\n",
        "    # grab the results from the search, including the total number of\n",
        "    # estimated results returned by the Bing API\n",
        "    results = search.json()\n",
        "    est_num_results = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "    print(f\"[INFO] {est_num_results} total results for '{term}'\")\n",
        "\n",
        "    # Determine an appropriate GROUP_SIZE based on the estimated number of results\n",
        "    GROUP_SIZE = est_num_results//5 # In this example, we divide the estimated number of results by 5\n",
        "    if GROUP_SIZE == 0:\n",
        "        GROUP_SIZE = 1 # Make sure that GROUP_SIZE is at least 1\n",
        "    print(f\"[INFO] GROUP_SIZE for '{term}': {GROUP_SIZE}\")\n",
        "\n",
        "    # loop over the estimated number of results in `GROUP_SIZE` groups\n",
        "    total = 0\n",
        "    for offset in range(0, est_num_results, GROUP_SIZE):\n",
        "        # update the search parameters using the current offset, then\n",
        "        # make the request to fetch the results\n",
        "        params[\"offset\"] = offset\n",
        "        params[\"count\"] = GROUP_SIZE # Update the count parameter with the appropriate GROUP_SIZE\n",
        "        search = requests.get(URL, headers=headers, params=params)\n",
        "        search.raise_for_status()\n",
        "        results = search.json()\n",
        "\n",
        "        # loop over the results\n",
        "        for v in results[\"value\"]:\n",
        "            # try to download the image\n",
        "            try:\n",
        "                # make a request to download the image\n",
        "                print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n",
        "\n",
        "                # 重複するURLはダウンロードしない\n",
        "                #print(f\"download_urls: {download_urls}\")\n",
        "                #print(f\"contentsURL: {v['contentUrl']}\")\n",
        "                if v[\"contentUrl\"] in download_urls:\n",
        "                    print(\"duplicated request\")\n",
        "                    continue\n",
        "\n",
        "                r = requests.get(v[\"contentUrl\"], timeout=30)\n",
        "\n",
        "                download_urls.append(v[\"contentUrl\"])\n",
        "\n",
        "\n",
        "                # build the path to the output image\n",
        "                ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
        "                filename = f\"{term}_{str(total).zfill(3)}{ext}\"\n",
        "                output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "                # write the image to disk\n",
        "                with open(output_path, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "\n",
        "                # write the URL to the csv file\n",
        "                with open(f\"{OUTPUT}/{csv_file}\", 'a', newline='') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([term, str(total), v[\"contentUrl\"], URL])\n",
        "\n",
        "                # increment the total number of images downloaded\n",
        "                total += 1\n",
        "\n",
        "                # break out of the loop if we have downloaded the maximum number of images\n",
        "                if total == MAX_RESULTS:\n",
        "                    break\n",
        "\n",
        "            # catch any errors that would not unable us to download the\n",
        "            # image\n",
        "            except Exception as e:\n",
        "                print(f\"[INFO] skipping: {v['contentUrl']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "e4kxcOyVItVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MquylaJQRHYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61xcePzTRHaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vb_9SI6zJebL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HjoGGGuJJecm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import requests\n",
        "from requests import exceptions\n",
        "\n",
        "API_KEY = f\"{bing_api_key}\"\n",
        "MAX_RESULTS = 500\n",
        "GROUP_SIZE = 5\n",
        "URL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
        "OUTPUT = '/content/drive/MyDrive/Deep_learning/CorneAI_osaka'\n",
        "\n",
        "if not os.path.isdir(OUTPUT):\n",
        "    os.mkdir(OUTPUT)\n",
        "\n",
        "EXCEPTIONS = set([IOError, FileNotFoundError,\n",
        "    exceptions.RequestException, exceptions.HTTPError,\n",
        "    exceptions.ConnectionError, exceptions.Timeout])\n",
        "\n",
        "search_terms = [\"corneal infection\", \"healthy eye\", \"autoimmune keratitis\", \"corneal scar\", \"corneal conjunctival tumor\", \"corneal deposit\", \"angular closure glaucoma photo\", \"cataract\", \"bullous keratopathy\"]\n",
        "\n",
        "csv_file = \"url_list.csv\"\n",
        "\n",
        "with open(csv_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Search term', 'Image number', 'Image URL', 'Website URL'])\n",
        "\n",
        "for term in search_terms:\n",
        "    print(f\"[INFO] searching Bing API for '{term}'\")\n",
        "    output_dir = os.path.join(OUTPUT, term)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    \n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "    params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\": \"Photo\", \"color\": \"ColorOnly\"}\n",
        "\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "\n",
        "    results = search.json()\n",
        "    est_num_results = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "    print(f\"[INFO] {est_num_results} total results for '{term}'\")\n",
        "\n",
        "    total = 0\n",
        "    for offset in range(0, est_num_results, GROUP_SIZE):\n",
        "        params[\"offset\"] = offset\n",
        "        search = requests.get(URL, headers=headers, params=params)\n",
        "        search.raise_for_status()\n",
        "        results = search.json()\n",
        "        \n",
        "        for v in results[\"value\"]:\n",
        "            try:\n",
        "                print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n",
        "                r = requests.get(v[\"contentUrl\"], timeout=30)\n",
        "\n",
        "                ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
        "                filename = f\"{term}_{str(total).zfill(3)}{ext}\"\n",
        "                output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "                with open(output_path, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "\n",
        "                with open(f\"{OUTPUT}/{csv_file}\", 'a', newline='') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([term, filename, v[\"contentUrl\"], v[\"hostPageUrl\"]])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[INFO] skipping: {v['contentUrl']}\")\n",
        "\n",
        "            total += 1\n",
        "            print(f\"{total} images downloaded!\")\n",
        "            if total >= MAX_RESULTS:\n",
        "                break\n",
        "\n",
        "        if total >= MAX_RESULTS:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "rnM8rAeoEDuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chromedriverを用いる方法**"
      ],
      "metadata": {
        "id": "e0WP5ZQnAfIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium==4.1.0 #新しいバージョンだとエラーが出るので旧バージョンにする"
      ],
      "metadata": {
        "id": "9frhTgD4BYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoWBLiRVBne3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# これだとサムネイルしか取得できない\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Search query\n",
        "search_query = \"flowers\"\n",
        "\n",
        "# Number of images to download\n",
        "num_images = 10\n",
        "\n",
        "# Create a new folder for the images\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "# URL to search Google Images\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML using Beautiful Soup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all image tags\n",
        "images = soup.find_all('img')\n",
        "\n",
        "# Iterate through the images and download them\n",
        "for i, img in enumerate(images[:num_images]):\n",
        "    url = img['src']\n",
        "    print(i)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        open(f\"{search_query}/{search_query}_{i}.jpg\", \"wb\").write(response.content)\n",
        "    except:\n",
        "        print(\"download error\")"
      ],
      "metadata": {
        "id": "YRrPYIguIEBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!curl -O https://chromedriver.storage.googleapis.com/110.0.5481.77/chromedriver_linux64.zip #Chromeのバージョンに合ったchromedriverのアドレスを設定\n",
        "!unzip chromedriver_linux64.zip\n",
        "!chmod +x chromedriver\n",
        "!mv chromedriver /usr/local/bin/\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "# Chromeドライバーの設定\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--disable-browser-side-navigation')\n",
        "\n",
        "# Googleで検索する\n",
        "search_query = 'flowers'\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "browser = webdriver.Chrome('chromedriver',options=options)\n",
        "browser.get(url)\n",
        "\n",
        "\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import base64\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# 画像のURLを取得する\n",
        "soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
        "img_tags = soup.find_all('img', class_='rg_i')\n",
        "\n",
        "\n",
        "urls = []\n",
        "for img in img_tags:\n",
        "    try:\n",
        "        urls.append(img[\"src\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# 画像をダウンロードする\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "counter = 0\n",
        "for i in range(num_images):\n",
        "    print(urls[i])\n",
        "    image_data = base64.b64decode(urls[i].split(',')[1])\n",
        "\n",
        "    # バイナリデータをBytesIOオブジェクトに書き込む\n",
        "    image_stream = BytesIO(image_data)\n",
        "\n",
        "    # PILで画像オブジェクトを作成する\n",
        "    image = Image.open(image_stream)\n",
        "    image_format = image.format\n",
        "\n",
        "    # 画像のネーミング\n",
        "    num= \"{:04d}\".format(i)\n",
        "    file_name = f\"{search_query}_{num}\"\n",
        "    new_image_path = f\"{search_query}/{file_name}.{image_format}\"\n",
        "\n",
        "\n",
        "    # Save image to file\n",
        "    image.save(new_image_path)\n"
      ],
      "metadata": {
        "id": "RupG3_zyQ7QT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}