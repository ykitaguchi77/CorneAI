{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSEKk205g7GnSpB87tiAEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_interference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 interference**"
      ],
      "metadata": {
        "id": "Un512TpLoNtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F7fjEQUV-pd",
        "outputId": "1a83efad-3dfe-4b71-9697-6df62d4aff0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYRY9egwjuIs",
        "outputId": "4d1ecf91-22c0-49f5-cc63-db522159e7a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "opencv-python-headless>=4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "# torch>=1.7.0\n",
        "# torchvision>=0.8.1\n",
        "tqdm>=4.41.0\n",
        "\n",
        "# Logging -------------------------------------\n",
        "##tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "##seaborn>=0.11.0\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUeMX07NqirS",
        "outputId": "c1651b9b-c4d3-4ee8-cb5c-c300117ea92c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfb6NYZIBGm1",
        "outputId": "44bdacd7-fe04-4456-9ade-9835b8a46d75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python-headless>=4.6.0.66 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (6.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (1.7.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (4.64.1)\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from thop->-r requirements.txt (line 37)) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->thop->-r requirements.txt (line 37)) (4.4.0)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchvision==0.11.2+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESI_x2upsdf4",
        "outputId": "c842d88f-60c7-42da-86b2-84130297f4ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.10.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.1%2Bcu113-cp38-cp38-linux_x86_64.whl (1821.4 MB)\n",
            "\u001b[K     |██████████████▋                 | 834.1 MB 1.2 MB/s eta 0:13:28tcmalloc: large alloc 1147494400 bytes == 0x64f92000 @  0x7f7312252615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |██████████████████▌             | 1055.7 MB 1.9 MB/s eta 0:06:52tcmalloc: large alloc 1434370048 bytes == 0x27ba000 @  0x7f7312252615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |███████████████████████▌        | 1336.2 MB 1.2 MB/s eta 0:06:51tcmalloc: large alloc 1792966656 bytes == 0x57fa6000 @  0x7f7312252615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |█████████████████████████████▊  | 1691.1 MB 1.2 MB/s eta 0:01:49tcmalloc: large alloc 2241208320 bytes == 0xc2d8e000 @  0x7f7312252615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |████████████████████████████████| 1821.4 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1821442048 bytes == 0x27ba000 @  0x7f73122511e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
            "tcmalloc: large alloc 2276802560 bytes == 0x1486f0000 @  0x7f7312252615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91 0x5d8941 0x4fe318\n",
            "\u001b[K     |████████████████████████████████| 1821.4 MB 9.1 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1+cu113) (4.4.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torchvision==0.11.2+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.11.2%2Bcu113-cp38-cp38-linux_x86_64.whl (24.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.10.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (1.10.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1->torchvision==0.11.2+cu113) (4.4.0)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.0+cu116\n",
            "    Uninstalling torchvision-0.14.0+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.0+cu116\n",
            "Successfully installed torchvision-0.11.2+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "weight = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "num = 5\n",
        "img_dir = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data\"\n",
        "img = glob.glob(f\"{img_dir}/*\")[num]\n",
        "img\n",
        "\n",
        "img = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg\""
      ],
      "metadata": {
        "id": "Bnz_lfT_l7NM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py  --weights $weight  --source $img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPsBV4Itjrn6",
        "outputId": "99d25d9e-039b-4a74-8a70-9d4532d444cb"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt'], source=/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 2022-12-26 torch 1.10.1+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients, 15.8 GFLOPs\n",
            "tensor_shape: torch.Size([3, 480, 640])\n",
            "tensor([[[0.16078, 0.13725, 0.11765,  ..., 0.12941, 0.12941, 0.12941],\n",
            "         [0.15686, 0.13333, 0.11765,  ..., 0.12549, 0.12549, 0.12549],\n",
            "         [0.16863, 0.14510, 0.12941,  ..., 0.13333, 0.12157, 0.12157],\n",
            "         ...,\n",
            "         [0.16863, 0.16863, 0.16471,  ..., 0.14118, 0.15294, 0.16471],\n",
            "         [0.17255, 0.17255, 0.16078,  ..., 0.13333, 0.14902, 0.16863],\n",
            "         [0.14510, 0.14510, 0.13333,  ..., 0.11765, 0.13725, 0.15686]],\n",
            "\n",
            "        [[0.06275, 0.03922, 0.01961,  ..., 0.04314, 0.04314, 0.04314],\n",
            "         [0.05882, 0.03529, 0.01961,  ..., 0.03922, 0.03922, 0.03922],\n",
            "         [0.07059, 0.04706, 0.02353,  ..., 0.03922, 0.03529, 0.03529],\n",
            "         ...,\n",
            "         [0.07059, 0.07059, 0.05882,  ..., 0.06667, 0.07843, 0.09020],\n",
            "         [0.07059, 0.07059, 0.05882,  ..., 0.05882, 0.07843, 0.09804],\n",
            "         [0.04314, 0.04314, 0.03137,  ..., 0.04314, 0.06667, 0.08627]],\n",
            "\n",
            "        [[0.22745, 0.20392, 0.18431,  ..., 0.18431, 0.19216, 0.19216],\n",
            "         [0.22353, 0.20000, 0.18431,  ..., 0.18039, 0.18824, 0.18824],\n",
            "         [0.22745, 0.20392, 0.19216,  ..., 0.18039, 0.18431, 0.18431],\n",
            "         ...,\n",
            "         [0.23529, 0.23529, 0.22745,  ..., 0.17647, 0.18824, 0.20000],\n",
            "         [0.24706, 0.24706, 0.23529,  ..., 0.16863, 0.18039, 0.20000],\n",
            "         [0.21961, 0.21961, 0.21569,  ..., 0.15294, 0.16863, 0.18824]]])\n",
            "pred:[tensor([[101.43054,  69.62360, 429.69128, 374.98340,   0.97141,   6.00000]])]\n",
            "image 1/1 /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg: 480x640 1 APAC, Done. (0.323s)\n",
            "Time of prediction from 1 data was 0.3233274989997881\n",
            "Speed: 12.9ms pre-process, 323.4ms inference, 214.7ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp47\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要な部分だけ抜粋"
      ],
      "metadata": {
        "id": "5BD1oZ3nFO6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=\"cpu\", dnn=False)\n",
        "#stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "#imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#             transforms.Resize(size=(480,640)),\n",
        "#             transforms.ToTensor(),\n",
        "#             # transforms.Normalize(\n",
        "#             #     mean=[0.5, 0.5, 0.5],\n",
        "#             #     std=[0.5, 0.5, 0.5]\n",
        "#             #    )\n",
        "#             ])\n",
        "\n",
        "img_pil = Image.open(img)  #PILで開く(streamlitの仕様)\n",
        "img_numpy = np.array(img_pil, dtype=np.uint8)\n",
        "img_cv2 = cv2.cvtColor(img_numpy, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "#img_cv2 = cv2.imread(img)\n",
        "#img_cv2 = cv2.resize(img_cv2, dsize=(480, 640)) #480*600pxにリサイズ\n",
        "img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "#img_tensor = transform(img_np)\n",
        "img_tensor /= 255\n",
        "print(img_tensor.shape)\n",
        "\n",
        "print(img_tensor)\n",
        "img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "print(f\"pred: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "YItGpZeAxb69",
        "outputId": "a5931fa9-ab93-41f6-ac15-843c24b050f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-55b509492fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetectMultiBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n\u001b[1;32m      4\u001b[0m                            increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from utils.plots import Annotator, colors, save_one_box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc26Y9zdzu5G",
        "outputId": "40722689-a3a5-462a-d7f6-50f5fcd62bba"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "診断は APAC、確率は97.1％です。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGKLXhUeG5ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max suppression**"
      ],
      "metadata": {
        "id": "2xfXpysJvfY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "7hniOHwII4JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n"
      ],
      "metadata": {
        "id": "zliE5D4ccIO0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "GyFpRaJfJRrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deploy Streamlit**"
      ],
      "metadata": {
        "id": "sW-sFyCzG0rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "必要ファイル\n",
        "- yolo5_forstreamlit --> もともとのrepositoryから不要なファイルを削除してGitHubに上げる（※おそらく未発表のためprivateにする）\n",
        "- app_streamlit.py\n",
        "- model_streamlit.py\n",
        "- requirement.txt --> もともとのものをベースに。適当に減らしてみて試行錯誤する\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MzsnAuSkG-Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir for_streamlit\n",
        "%cd for_streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yGcpxH4IbJb",
        "outputId": "3bbbd059-b383-4b7e-8d3b-599db0f1a4c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "mkdir: cannot create directory ‘for_streamlit’: File exists\n",
            "/content/for_streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requrements.txt\n",
        "\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "#opencv-python-headless>=4.1.2\n",
        "opencv-python-headless==4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "torch>=1.10.1\n",
        "torchvision>=0.11.2\n",
        "#tqdm>=4.41.0\n",
        "tqdm>=4.63.2\n",
        "\n",
        "\n",
        "# Logging -------------------------------------\n",
        "tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "#seaborn>=0.11.0\n",
        "seaborn>=0.12.1\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-BqPDfDX2u8",
        "outputId": "40b07ada-08f7-4440-8f0c-b0533da9edba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requrements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "\n",
        "def predict(img_cv2):   #input: cv2 image\n",
        "    weight = \"weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'\n",
        "    model = DetectMultiBackend(weight, device=\"cpu\", dnn=False)\n",
        "    class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "    #img_cv2 = cv2.resize(img_cv2, dsize=(480, 640)) #480*600pxにリサイズ\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "    pred = model(img_tensor)\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "    \n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    return [class_name, prob]\n"
      ],
      "metadata": {
        "id": "4QjJ3UqvBGFC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from model_streamlit import predict\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "st.set_option(\"deprecation.showfileUploaderEncoding\", False)\n",
        "\n",
        "st.sidebar.title(\"CorneAI_web\")\n",
        "st.sidebar.write(\"\")\n",
        "\n",
        "img_source = st.sidebar.radio(\"画像のソースを選択してください。\",\n",
        "                              (\"画像をアップロード\", \"カメラで撮影\"))\n",
        "if img_source == \"画像をアップロード\":\n",
        "    img_file = st.sidebar.file_uploader(\"画像を選択してください。\", type=[\"png\", \"jpg\"])\n",
        "elif img_source == \"カメラで撮影\":\n",
        "    img_file = st.camera_input(\"カメラで撮影\")\n",
        "\n",
        "if img_file is not None:\n",
        "    with st.spinner(\"推定中...\"):\n",
        "        img_pil = Image.open(img_file)\n",
        "        st.image(img_pil, caption=\"対象の画像\", width=480)\n",
        "        st.write(\"\")\n",
        "\n",
        "        # 予測\n",
        "        img_numpy = np.array(img_pil, dtype=np.uint8)\n",
        "        img_cv2 = cv2.cvtColor(img_numpy, cv2.COLOR_RGB2BGR)\n",
        "        results = predict(img_cv2)\n",
        "\n",
        "        # 結果の表示\n",
        "        st.subheader(\"判定結果\")\n",
        "        st.write(\"診断は %s、確率は%.1f％です。\" %(result[0], result[1]*100))\n"
      ],
      "metadata": {
        "id": "n4x_Pqs7DxxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit --quiet"
      ],
      "metadata": {
        "id": "kJihY_rsjU1_",
        "outputId": "3498c9bb-3370-4d34-c49d-f25da5ad9ef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 9.2 MB 9.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 55.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 50.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 49.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 47.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.0 MB/s \n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit\n",
        "for dir in \n",
        "    shutil.copytree(dir, \"/content/\")"
      ],
      "metadata": {
        "id": "0pU76cJvk0AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test run\n",
        "from google.colab import files\n",
        "files.view(\"/content\")\n",
        "files.view(\"app_streamlit.py\")\n",
        "!streamlit run app_streamlit.py & sleep 3 && npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "asAX2W9hM-jM",
        "outputId": "88a85595-ce34-40e0-f23d-2e6f45698e00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-30391fd121e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"app_streamlit.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'streamlit run app_streamlit.py & sleep 3 && npx localtunnel --port 8501'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mview\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    292\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: app_streamlit.py"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKsPjFxoNNtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference\n",
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit\n",
        "\n",
        "def predict(img):   #input: PIL image\n",
        "    weight = \"weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'\n",
        "    model = DetectMultiBackend(weight, device=\"cpu\", dnn=False)\n",
        "    class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "    transform = transforms.Compose([\n",
        "                transforms.Resize(size=(640,640)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.5, 0.5, 0.5],\n",
        "                    std=[0.5, 0.5, 0.5]\n",
        "                    )\n",
        "                ])\n",
        "\n",
        "    img_tensor = transform(img)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "    pred = model(img_tensor)\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "    \n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    return [class_name, prob]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rokRA1eYdvFi",
        "outputId": "bd643598-81b4-41d1-8f4d-b96e92d18c22"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(Image.open(\"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data/infection_amoeba_ehm0502_01.jpg\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxEEl71DeAWr",
        "outputId": "6b2da3cc-e557-46e9-dc31-37b40aac0a7e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients, 15.8 GFLOPs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['deposit', 0.8515667915344238]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}