{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeGLSmpNOI0lshvK7+Lc1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_interference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 interference**"
      ],
      "metadata": {
        "id": "Un512TpLoNtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F7fjEQUV-pd",
        "outputId": "aa44da7d-71e7-41bf-8c3e-0bd9699e83e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYRY9egwjuIs",
        "outputId": "103edd7a-25ff-43e2-d2a2-df150ba7ab79"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "opencv-python-headless>=4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "# torch>=1.7.0\n",
        "# torchvision>=0.8.1\n",
        "tqdm>=4.41.0\n",
        "\n",
        "# Logging -------------------------------------\n",
        "##tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "##seaborn>=0.11.0\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUeMX07NqirS",
        "outputId": "e89d713e-4f6b-42ad-b05c-cd1fe0823288"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfb6NYZIBGm1",
        "outputId": "5e9bb29a-992a-45fe-c86c-32adc39319dd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (6.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (1.7.3)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 37)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from thop->-r requirements.txt (line 37)) (1.10.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->thop->-r requirements.txt (line 37)) (4.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchvision==0.11.2+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESI_x2upsdf4",
        "outputId": "29d114c0-329e-4393-f305-1529015c0b48"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.10.1+cu113 in /usr/local/lib/python3.8/dist-packages (1.10.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1+cu113) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torchvision==0.11.2+cu113 in /usr/local/lib/python3.8/dist-packages (0.11.2+cu113)\n",
            "Requirement already satisfied: torch==1.10.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (1.10.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1->torchvision==0.11.2+cu113) (4.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "weight = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "num = 5\n",
        "img_dir = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data\"\n",
        "img = glob.glob(f\"{img_dir}/*\")[num]\n",
        "img\n",
        "\n",
        "img = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg\""
      ],
      "metadata": {
        "id": "Bnz_lfT_l7NM"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py  --weights $weight  --source $img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPsBV4Itjrn6",
        "outputId": "7f298bbc-451c-4c7c-8797-93d00e683b04"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt'], source=/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 üöÄ 2022-12-26 torch 1.10.1+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients, 15.8 GFLOPs\n",
            "tensor_shape: torch.Size([3, 480, 640])\n",
            "tensor([[[0.16078, 0.13725, 0.11765,  ..., 0.12941, 0.12941, 0.12941],\n",
            "         [0.15686, 0.13333, 0.11765,  ..., 0.12549, 0.12549, 0.12549],\n",
            "         [0.16863, 0.14510, 0.12941,  ..., 0.13333, 0.12157, 0.12157],\n",
            "         ...,\n",
            "         [0.16863, 0.16863, 0.16471,  ..., 0.14118, 0.15294, 0.16471],\n",
            "         [0.17255, 0.17255, 0.16078,  ..., 0.13333, 0.14902, 0.16863],\n",
            "         [0.14510, 0.14510, 0.13333,  ..., 0.11765, 0.13725, 0.15686]],\n",
            "\n",
            "        [[0.06275, 0.03922, 0.01961,  ..., 0.04314, 0.04314, 0.04314],\n",
            "         [0.05882, 0.03529, 0.01961,  ..., 0.03922, 0.03922, 0.03922],\n",
            "         [0.07059, 0.04706, 0.02353,  ..., 0.03922, 0.03529, 0.03529],\n",
            "         ...,\n",
            "         [0.07059, 0.07059, 0.05882,  ..., 0.06667, 0.07843, 0.09020],\n",
            "         [0.07059, 0.07059, 0.05882,  ..., 0.05882, 0.07843, 0.09804],\n",
            "         [0.04314, 0.04314, 0.03137,  ..., 0.04314, 0.06667, 0.08627]],\n",
            "\n",
            "        [[0.22745, 0.20392, 0.18431,  ..., 0.18431, 0.19216, 0.19216],\n",
            "         [0.22353, 0.20000, 0.18431,  ..., 0.18039, 0.18824, 0.18824],\n",
            "         [0.22745, 0.20392, 0.19216,  ..., 0.18039, 0.18431, 0.18431],\n",
            "         ...,\n",
            "         [0.23529, 0.23529, 0.22745,  ..., 0.17647, 0.18824, 0.20000],\n",
            "         [0.24706, 0.24706, 0.23529,  ..., 0.16863, 0.18039, 0.20000],\n",
            "         [0.21961, 0.21961, 0.21569,  ..., 0.15294, 0.16863, 0.18824]]])\n",
            "pred:[tensor([[101.43054,  69.62360, 429.69128, 374.98340,   0.97141,   6.00000]])]\n",
            "image 1/1 /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg: 480x640 1 APAC, Done. (0.399s)\n",
            "Time of prediction from 1 data was 0.3986709469991183\n",
            "Speed: 11.4ms pre-process, 398.7ms inference, 274.1ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp46\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÂøÖË¶Å„Å™ÈÉ®ÂàÜ„Å†„ÅëÊäúÁ≤ã"
      ],
      "metadata": {
        "id": "5BD1oZ3nFO6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=\"cpu\", dnn=False)\n",
        "#stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "#imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize(size=(480,640)),\n",
        "            transforms.ToTensor(),\n",
        "            # transforms.Normalize(\n",
        "            #     mean=[0.5, 0.5, 0.5],\n",
        "            #     std=[0.5, 0.5, 0.5]\n",
        "            #    )\n",
        "            ])\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "\n",
        "#img_tensor = transform(img_np)\n",
        "img_tensor /= 255\n",
        "print(img_tensor.shape)\n",
        "img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "print(f\"pred: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YItGpZeAxb69",
        "outputId": "251bb0a7-8add-470e-8a92-62c421fa5e50"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients, 15.8 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 480, 640])\n",
            "pred: [tensor([[101.43054,  69.62360, 429.69128, 374.98340,   0.97141,   6.00000]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" %(class_name, prob*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc26Y9zdzu5G",
        "outputId": "89305aef-4231-47d7-e441-582b1b92fb6d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ë®∫Êñ≠„ÅØ normal„ÄÅÁ¢∫Áéá„ÅØ86.3ÔºÖ„Åß„Åô„ÄÇ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGKLXhUeG5ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max suppression**"
      ],
      "metadata": {
        "id": "2xfXpysJvfY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "7hniOHwII4JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n"
      ],
      "metadata": {
        "id": "zliE5D4ccIO0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "GyFpRaJfJRrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deploy Streamlit**"
      ],
      "metadata": {
        "id": "sW-sFyCzG0rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ÂøÖË¶Å„Éï„Ç°„Ç§„É´\n",
        "- yolo5_forstreamlit --> „ÇÇ„Å®„ÇÇ„Å®„ÅÆrepository„Åã„Çâ‰∏çË¶Å„Å™„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§„Åó„Å¶GitHub„Å´‰∏ä„Åí„ÇãÔºà‚Äª„Åä„Åù„Çâ„ÅèÊú™Áô∫Ë°®„ÅÆ„Åü„ÇÅprivate„Å´„Åô„ÇãÔºâ\n",
        "- app_streamlit.py\n",
        "- model_streamlit.py\n",
        "- requirement.txt --> „ÇÇ„Å®„ÇÇ„Å®„ÅÆ„ÇÇ„ÅÆ„Çí„Éô„Éº„Çπ„Å´„ÄÇÈÅ©ÂΩì„Å´Ê∏õ„Çâ„Åó„Å¶„Åø„Å¶Ë©¶Ë°åÈåØË™§„Åô„Çã\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MzsnAuSkG-Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir for_streamlit\n",
        "%cd for_streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yGcpxH4IbJb",
        "outputId": "4ea343b9-0baf-401d-c58f-bbf68bce7f1b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "mkdir: cannot create directory ‚Äòfor_streamlit‚Äô: File exists\n",
            "/content/for_streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requrements.txt\n",
        "\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "#opencv-python-headless>=4.1.2\n",
        "opencv-python-headless==4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "torch>=1.10.1\n",
        "torchvision>=0.11.2\n",
        "#tqdm>=4.41.0\n",
        "tqdm>=4.63.2\n",
        "\n",
        "\n",
        "# Logging -------------------------------------\n",
        "tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "#seaborn>=0.11.0\n",
        "seaborn>=0.12.1\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-BqPDfDX2u8",
        "outputId": "4f2e3eeb-9f52-406e-c499-3e82ce8f9bae"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requrements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_streamlit.py\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "\n",
        "def predict(img):   #input: PIL image\n",
        "    weight = \"weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'\n",
        "    model = DetectMultiBackend(weight, device=\"cpu\", dnn=False)\n",
        "    class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "    transform = transforms.Compose([\n",
        "                transforms.Resize(size=(640,640)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.5, 0.5, 0.5],\n",
        "                    std=[0.5, 0.5, 0.5]\n",
        "                    )\n",
        "                ])\n",
        "\n",
        "    img_tensor = transform(img)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "    pred = model(img_tensor)\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "    \n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    return [class_name, prob]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QjJ3UqvBGFC",
        "outputId": "bfb4cee6-3bfc-4fe9-c92a-23ccd9a832bf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_streamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_streamlit.py\n",
        "\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from model_streamlit import predict\n",
        "\n",
        "st.set_option(\"deprecation.showfileUploaderEncoding\", False)\n",
        "\n",
        "st.sidebar.title(\"CorneAI_web\")\n",
        "st.sidebar.write(\"\")\n",
        "\n",
        "img_source = st.sidebar.radio(\"ÁîªÂÉè„ÅÆ„ÇΩ„Éº„Çπ„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
        "                              (\"ÁîªÂÉè„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ\", \"„Ç´„É°„É©„ÅßÊíÆÂΩ±\"))\n",
        "if img_source == \"ÁîªÂÉè„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ\":\n",
        "    img_file = st.sidebar.file_uploader(\"ÁîªÂÉè„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\", type=[\"png\", \"jpg\"])\n",
        "elif img_source == \"„Ç´„É°„É©„ÅßÊíÆÂΩ±\":\n",
        "    img_file = st.camera_input(\"„Ç´„É°„É©„ÅßÊíÆÂΩ±\")\n",
        "\n",
        "if img_file is not None:\n",
        "    with st.spinner(\"Êé®ÂÆö‰∏≠...\"):\n",
        "        img = Image.open(img_file).convert('RGB')\n",
        "        st.image(img, caption=\"ÂØæË±°„ÅÆÁîªÂÉè\", width=480)\n",
        "        st.write(\"\")\n",
        "\n",
        "        # ‰∫àÊ∏¨\n",
        "        results = predict(img)\n",
        "\n",
        "        # ÁµêÊûú„ÅÆË°®Á§∫\n",
        "        st.subheader(\"Âà§ÂÆöÁµêÊûú\")\n",
        "        st.write(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" %(result[0], result[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4x_Pqs7DxxC",
        "outputId": "049a55d3-de3a-40aa-d36c-97100268720b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_streamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test run\n",
        "\n",
        "from google.colab import files\n",
        "files.view(\"//gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit\")\n",
        "files.view(\"app_streamlit.py\")\n",
        "!streamlit run app.py & sleep 3 && npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "asAX2W9hM-jM",
        "outputId": "3d8dd7e2-8f62-449c-c077-e407b39903d3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"//gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/for_streamlit/app_streamlit.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: streamlit: command not found\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.173s\n",
            "your url is: https://social-ducks-walk-34-125-77-206.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKsPjFxoNNtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference\n",
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit\n",
        "\n",
        "def predict(img):   #input: PIL image\n",
        "    weight = \"weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'\n",
        "    model = DetectMultiBackend(weight, device=\"cpu\", dnn=False)\n",
        "    class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "    transform = transforms.Compose([\n",
        "                transforms.Resize(size=(640,640)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.5, 0.5, 0.5],\n",
        "                    std=[0.5, 0.5, 0.5]\n",
        "                    )\n",
        "                ])\n",
        "\n",
        "    img_tensor = transform(img)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "    pred = model(img_tensor)\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "    \n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    return [class_name, prob]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rokRA1eYdvFi",
        "outputId": "bd643598-81b4-41d1-8f4d-b96e92d18c22"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forstreamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(Image.open(\"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data/infection_amoeba_ehm0502_01.jpg\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxEEl71DeAWr",
        "outputId": "6b2da3cc-e557-46e9-dc31-37b40aac0a7e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients, 15.8 GFLOPs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['deposit', 0.8515667915344238]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}