{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm4p+1FibrLutqMdJTo/0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/CorneAI_GradCAM%E8%AB%96%E6%96%87_Figure_250722.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_GradCAM_CutPaste_Figure**"
      ],
      "metadata": {
        "id": "L7J1GMyMhrs8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-08xSOvhT8O",
        "outputId": "0879ba98-47bb-4e61-c0d9-266a272fa8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Figure S1**"
      ],
      "metadata": {
        "id": "DaAZ0QG-hUiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Confusion Matrix  —  GroundTruth vs Predict  (Grad‑CAM dataset)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "#\n",
        "# ❶  CSV  ファイルを読み込む\n",
        "# ❷  GroundTruth / Predict をラベル文字列に変換\n",
        "# ❸  pandas.crosstab で混同行列を作成\n",
        "# ❹  seaborn.heatmap で描画・保存（350 dpi）\n",
        "#     フォントサイズは下の設定を変更\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === フォントサイズ設定 ===============================\n",
        "TITLE_FONT   = 16   # タイトル\n",
        "LABEL_FONT   = 14   # 軸ラベル\n",
        "TICK_FONT    = 12   # 目盛りラベル\n",
        "ANNOT_FONT   = 12  # ヒートマップ数値\n",
        "# =====================================================\n",
        "\n",
        "# --- ❶  CSVパス（Google Drive 等に合わせて変更してください） ---\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv'\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "# --- ❷  ラベルマッピング ---------------------------------------\n",
        "label_map = {\n",
        "    'normal'        : 'Normal',\n",
        "    'infection'     : 'Infectious keratitis',\n",
        "    'non-infection' : 'Non-infection keratitis',\n",
        "    'scar'          : 'Scar',\n",
        "    'tumor'         : 'Tumor',\n",
        "    'deposit'       : 'Deposit',\n",
        "    'APAC'          : 'APAC',\n",
        "    'lens opacity'  : 'Lens opacity',\n",
        "    'bullous'       : 'Bullous keratopathy'\n",
        "}\n",
        "df['gt_label']   = df['GroundTruth'].map(label_map)\n",
        "df['pred_label'] = df['Predict'].map(label_map)\n",
        "\n",
        "class_names = [\n",
        "    'Normal',\n",
        "    'Infectious keratitis',\n",
        "    'Non-infection keratitis',\n",
        "    'Scar',\n",
        "    'Tumor',\n",
        "    'Deposit',\n",
        "    'APAC',\n",
        "    'Lens opacity',\n",
        "    'Bullous keratopathy'\n",
        "]\n",
        "\n",
        "# --- ❸  混同行列 ----------------------------------------------\n",
        "conf_matrix = (\n",
        "    pd.crosstab(df['gt_label'], df['pred_label'])\n",
        "      .reindex(index=class_names, columns=class_names, fill_value=0)\n",
        ")\n",
        "\n",
        "# --- ❹  プロット ----------------------------------------------\n",
        "plt.figure(figsize=(8, 7), dpi=350)\n",
        "sns.heatmap(conf_matrix,\n",
        "            annot=True, fmt='d', cmap='Blues',\n",
        "            annot_kws={'size': ANNOT_FONT},\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "plt.title('Confusion Matrix (GroundTruth vs Predict)', fontsize=TITLE_FONT)\n",
        "plt.xlabel('Predicted Class', fontsize=LABEL_FONT)\n",
        "plt.ylabel('Actual Class', fontsize=LABEL_FONT)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=TICK_FONT)\n",
        "plt.yticks(rotation=0, fontsize=TICK_FONT)\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_gradcam_350dpi.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lJ43gKjEiIiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Figure S2**"
      ],
      "metadata": {
        "id": "3qKgiBapmBJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Grad-CAM++ AOI 0.5 可視化フルコード\n",
        "# (x軸ラベルを layer17_cv3_conv 形式, layermodel_除去)\n",
        "# Figure S2を作成\n",
        "# ============================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === パス設定 ===\n",
        "CSV_PATH_DRIVE = \"/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "LOCAL_CSV_PATH = \"/content/Ueno_Mix1039_gradcam++.csv\"\n",
        "!cp \"$CSV_PATH_DRIVE\" \"$LOCAL_CSV_PATH\"\n",
        "\n",
        "# === ライブラリ ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from pathlib import Path, PurePath\n",
        "import re\n",
        "\n",
        "# === 設定 ===\n",
        "CSV_PATH = Path(LOCAL_CSV_PATH)\n",
        "OUT_DIR  = Path(\"/content\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RAW_LAYER_COLS = [\n",
        "    'AOI_0.5_layermodel_17_cv3_conv',\n",
        "    'AOI_0.5_layermodel_20_cv3_conv',\n",
        "    'AOI_0.5_layermodel_23_cv3_conv',\n",
        "    'AOI_0.5_layer24_m_0',\n",
        "    'AOI_0.5_layer24_m_1',\n",
        "    'AOI_0.5_layer24_m_2'\n",
        "]\n",
        "\n",
        "POSSIBLE_CLASSES = [\n",
        "    'normal', 'infection', 'non-infection', 'scar', 'tumor',\n",
        "    'deposit', 'APAC', 'lens opacity', 'bullous'\n",
        "]\n",
        "\n",
        "# === 名称変換 ===\n",
        "def to_axis_label(col: str) -> str:\n",
        "    \"\"\"\n",
        "    入力: AOI_0.5_layermodel_17_cv3_conv → 出力: layer17_cv3_conv\n",
        "    入力: AOI_0.5_layer24_m_0          → 出力: layer24_m_0\n",
        "    \"\"\"\n",
        "    # AOI_0.5_ を除去\n",
        "    name = re.sub(r'^AOI_0\\.5_', '', col)\n",
        "    # layermodel_ を除去\n",
        "    name = name.replace('layermodel_', 'layer')\n",
        "    return name\n",
        "\n",
        "LAYER_AXIS_LABEL = {c: to_axis_label(c) for c in RAW_LAYER_COLS}\n",
        "\n",
        "# === スタイル ===\n",
        "mpl.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'font.size': 11,\n",
        "    'axes.titlesize': 13,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'axes.linewidth': 1.0,\n",
        "})\n",
        "BOX_FACE, EDGE_COLOR, MEDIAN_COLOR = \"#C8C8C8\", \"black\", \"black\"\n",
        "BOX_WIDTH, DPI = 0.8, 350\n",
        "\n",
        "# === データ読み込み ===\n",
        "if not CSV_PATH.exists():\n",
        "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "missing = [c for c in RAW_LAYER_COLS if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
        "if 'GroundTruth' not in df.columns:\n",
        "    raise ValueError(\"Column 'GroundTruth' not found in CSV.\")\n",
        "\n",
        "df_layers = df.dropna(subset=RAW_LAYER_COLS, how='all').copy()\n",
        "df_layers = df_layers[~df_layers['GroundTruth'].isna()].copy()\n",
        "\n",
        "class_order = [c for c in POSSIBLE_CLASSES if c in df_layers['GroundTruth'].unique()]\n",
        "print(\"Class order:\", class_order)\n",
        "\n",
        "# === プロット関数 ===\n",
        "def save_and_show_boxplot(data, labels, title, ylabel, filename,\n",
        "                          rotate=True, ylim=None, showfliers=True):\n",
        "    plt.figure(figsize=(8,5))\n",
        "    bp = plt.boxplot(\n",
        "        data, labels=labels, patch_artist=True, showfliers=showfliers,\n",
        "        widths=BOX_WIDTH,\n",
        "        boxprops=dict(linewidth=1.0, color=EDGE_COLOR),\n",
        "        whiskerprops=dict(linewidth=1.0, color=EDGE_COLOR),\n",
        "        capprops=dict(linewidth=1.0, color=EDGE_COLOR),\n",
        "        medianprops=dict(linewidth=1.4, color=MEDIAN_COLOR),\n",
        "        flierprops=dict(marker='o', markersize=3, markerfacecolor=EDGE_COLOR,\n",
        "                        markeredgecolor=EDGE_COLOR, alpha=0.8)\n",
        "    )\n",
        "    for b in bp['boxes']:\n",
        "        b.set_facecolor(BOX_FACE)\n",
        "    plt.title(title, pad=6)\n",
        "    plt.ylabel(ylabel)\n",
        "    if rotate:\n",
        "        plt.xticks(rotation=30, ha='right')\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    out_path = OUT_DIR / filename\n",
        "    plt.savefig(out_path, dpi=DPI, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return out_path\n",
        "\n",
        "# === 1) 全レイヤ比較 ===\n",
        "# === 1) 全レイヤ比較 ===\n",
        "data_layers   = [df_layers[col].dropna().values for col in RAW_LAYER_COLS]\n",
        "labels_layers = [\"Layer 17\", \"Layer 20\", \"Layer 23\", \"Layer 24-0\", \"Layer 24-1\", \"Layer 24-2\"]  # ← ここを更新\n",
        "overall_png = save_and_show_boxplot(\n",
        "    data_layers, labels_layers,\n",
        "    \"Comparison of AOI_0.5 Values Across Layers\",\n",
        "    \"AOI_0.5\",\n",
        "    \"AOI_0.5_layers_comparison_350dpi.png\"\n",
        ")\n",
        "print(\"Saved:\", overall_png)\n",
        "\n",
        "# === 2) 各レイヤ クラス別 ===\n",
        "per_class_pngs = []\n",
        "for raw_col in RAW_LAYER_COLS:\n",
        "    axis_label = LAYER_AXIS_LABEL[raw_col]  # タイトルと y軸ラベルに使用\n",
        "    df_c = df_layers[['GroundTruth', raw_col]].dropna()\n",
        "    class_data, class_labels = [], []\n",
        "    for cls in class_order:\n",
        "        vals = df_c[df_c['GroundTruth'] == cls][raw_col].values\n",
        "        if len(vals) > 0:\n",
        "            class_data.append(vals)\n",
        "            class_labels.append(cls)\n",
        "    if not class_data:\n",
        "        continue\n",
        "    out = save_and_show_boxplot(\n",
        "        class_data, class_labels,\n",
        "        axis_label,        # ← タイトルを簡略名のみ\n",
        "        \"AOI_0.5\",         # y軸は共通で AOI_0.5\n",
        "        f\"{axis_label}_per_class_350dpi.png\"\n",
        "    )\n",
        "    per_class_pngs.append(out)\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "# === 3) レイヤ全体統計 ===\n",
        "summary_rows = []\n",
        "for col in RAW_LAYER_COLS:\n",
        "    vals = df_layers[col].dropna()\n",
        "    if len(vals) == 0:\n",
        "        continue\n",
        "    q1,q2,q3 = np.percentile(vals, [25,50,75])\n",
        "    summary_rows.append({\n",
        "        \"layer_raw\": col,\n",
        "        \"layer_label\": LAYER_AXIS_LABEL[col],\n",
        "        \"n\": len(vals),\n",
        "        \"median\": q2,\n",
        "        \"q1\": q1,\n",
        "        \"q3\": q3,\n",
        "        \"IQR\": q3 - q1,\n",
        "        \"mean\": vals.mean(),\n",
        "        \"std\": vals.std()\n",
        "    })\n",
        "summary_df = pd.DataFrame(summary_rows).sort_values(\"layer_label\")\n",
        "summary_csv = OUT_DIR / \"AOI_0.5_layers_summary_stats.csv\"\n",
        "summary_df.to_csv(summary_csv, index=False)\n",
        "print(\"Saved:\", summary_csv)\n",
        "display(summary_df)\n",
        "\n",
        "# === 4) 各レイヤ × クラス統計 ===\n",
        "class_stats_paths = []\n",
        "for col in RAW_LAYER_COLS:\n",
        "    axis_label = LAYER_AXIS_LABEL[col]\n",
        "    rows = []\n",
        "    for cls in class_order:\n",
        "        cls_vals = df_layers.loc[df_layers['GroundTruth']==cls, col].dropna()\n",
        "        if len(cls_vals)==0:\n",
        "            continue\n",
        "        q1,q2,q3 = np.percentile(cls_vals,[25,50,75])\n",
        "        rows.append({\n",
        "            \"layer_label\": axis_label,\n",
        "            \"class\": cls,\n",
        "            \"n\": len(cls_vals),\n",
        "            \"median\": q2,\n",
        "            \"q1\": q1,\n",
        "            \"q3\": q3,\n",
        "            \"IQR\": q3-q1,\n",
        "            \"mean\": cls_vals.mean(),\n",
        "            \"std\": cls_vals.std()\n",
        "        })\n",
        "    if rows:\n",
        "        layer_stats_df = pd.DataFrame(rows)\n",
        "        out_csv = OUT_DIR / f\"{axis_label}_class_stats.csv\"\n",
        "        layer_stats_df.to_csv(out_csv, index=False)\n",
        "        class_stats_paths.append(out_csv)\n",
        "        print(\"Saved:\", out_csv)\n",
        "        display(layer_stats_df)\n",
        "\n",
        "# === 5) （任意）ダウンロード ===\n",
        "from google.colab import files\n",
        "files.download(str(overall_png))\n",
        "for p in per_class_pngs: files.download(str(p))\n",
        "# files.download(str(summary_csv))\n",
        "# for p in class_stats_paths: files.download(str(p))\n",
        "\n",
        "print(\"All done.\")"
      ],
      "metadata": {
        "id": "uWNrt1zumFZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Figre 5**"
      ],
      "metadata": {
        "id": "wo_Ojj4oB6S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab FULL SCRIPT — Ueno_Mix1039 Cut‑and‑Paste Background Analysis\n",
        "# =============================================================================\n",
        "# ① Google Drive をマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# ② ライブラリ\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from scipy.stats import chi2_contingency, norm\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ③ パス & データ読込\n",
        "# ------------------------------------------------------------\n",
        "FILE_PATH = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_cutmix.csv'\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ④ ラベル置換（必要に応じて編集）\n",
        "# ------------------------------------------------------------\n",
        "label_map = {\n",
        "    \"infection\":         \"Infectious keratitis\",\n",
        "    \"normal\":            \"Normal\",\n",
        "    \"non-infection\":     \"Non-infectious keratitis\",\n",
        "    \"scar\":              \"Scar\",\n",
        "    \"tumor\":             \"Tumor\",\n",
        "    \"deposit\":           \"Deposit\",\n",
        "    \"apac\":              \"APAC\",\n",
        "    \"lens opacity\":      \"Lens opacity\",\n",
        "    \"bullous\":           \"Bullous keratopathy\"\n",
        "}\n",
        "for col in ['cornea_class', 'background_class', 'cutmix_pred']:\n",
        "    df[col] = df[col].replace(label_map)\n",
        "\n",
        "classes = [\n",
        "    \"Normal\", \"Infectious keratitis\", \"Non-infectious keratitis\",\n",
        "    \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"\n",
        "]\n",
        "\n",
        "# ユーティリティ: 空白・NaN を除外するマスク\n",
        "def valid_mask(series):\n",
        "    return series.notna() & (series.astype(str).str.strip() != '')\n",
        "\n",
        "# Wilson 95 % CI\n",
        "def wilson_ci(successes, n, conf=0.95):\n",
        "    if n == 0: return (np.nan, np.nan)\n",
        "    p_hat, z = successes / n, norm.ppf((1 + conf) / 2)\n",
        "    denom    = 1 + z**2 / n\n",
        "    centre   = (p_hat + z**2 / (2 * n)) / denom\n",
        "    spread   = z * np.sqrt(p_hat * (1 - p_hat) / n + z**2 / (4 * n**2)) / denom\n",
        "    return max(0, centre - spread), min(1, centre + spread)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ⑤ 背景別ヒートマップ（Cut‑and‑Paste 正解率）\n",
        "# ------------------------------------------------------------\n",
        "def acc_func(pred_series):\n",
        "    gt = df.loc[pred_series.index, 'cornea_class']\n",
        "    v  = valid_mask(pred_series)\n",
        "    return (pred_series[v] == gt[v]).mean() if v.sum() else np.nan\n",
        "\n",
        "matrix = df.pivot_table(\n",
        "    index='cornea_class',\n",
        "    columns='background_class',\n",
        "    values='cutmix_pred',\n",
        "    aggfunc=acc_func\n",
        ").reindex(index=classes, columns=classes)\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "sns.heatmap(matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            cbar=False, vmin=0, vmax=1, linewidths=.5,\n",
        "            annot_kws={'size':14})\n",
        "plt.title('Cut‑and‑Paste Accuracy by Background', fontsize=16)\n",
        "plt.xlabel('Background Class', fontsize=14)\n",
        "plt.ylabel('Cornea Class (Ground Truth)', fontsize=16)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cutmix_background_heatmap.png', dpi=350)\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ⑥ 背景差の統計検定（各 Cornea Class ごと）\n",
        "#      • correct / incorrect × background のカイ二乗検定\n",
        "# ------------------------------------------------------------\n",
        "stats_records, p_vals = [], []\n",
        "for cls in classes:\n",
        "    sub = df[(df['cornea_class'] == cls) & valid_mask(df['cutmix_pred'])]\n",
        "    if sub.empty:\n",
        "        continue\n",
        "\n",
        "    counts = []\n",
        "    for bg in classes:\n",
        "        bg_sub = sub[sub['background_class'] == bg]\n",
        "        n      = len(bg_sub)\n",
        "        correct = (bg_sub['cutmix_pred'] == cls).sum()\n",
        "        counts.append([correct, n - correct])\n",
        "\n",
        "    # 背景に該当データがない行は除外\n",
        "    counts = np.array([row for row in counts if sum(row) > 0])\n",
        "    if counts.shape[0] < 2:     # 背景が 1 種類のみ\n",
        "        continue\n",
        "\n",
        "    chi2, p, dof, _ = chi2_contingency(counts)\n",
        "    p_vals.append(p)\n",
        "\n",
        "    stats_records.append([cls, dof, chi2, p])\n",
        "\n",
        "# 多重検定補正 (FDR‑BH)\n",
        "if p_vals:\n",
        "    adj = multipletests(p_vals, method='fdr_bh')[1]\n",
        "    for rec, adj_p in zip(stats_records, adj):\n",
        "        rec.append(adj_p)\n",
        "\n",
        "stats_df = pd.DataFrame(stats_records, columns=[\n",
        "    'Cornea_Class', 'df', 'Chi2', 'P', 'Adj_P'\n",
        "])\n",
        "print('\\n=== Chi‑square: Accuracy × Background (Cut‑and‑Paste) ===')\n",
        "print(stats_df.to_string(index=False, float_format='%.3f'))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# ⑦ 背景別 正解率 + 95 % CI 一覧\n",
        "# ------------------------------------------------------------\n",
        "ci_records = []\n",
        "for cls in classes:\n",
        "    sub = df[(df['cornea_class'] == cls) & valid_mask(df['cutmix_pred'])]\n",
        "    for bg in classes:\n",
        "        bg_sub = sub[sub['background_class'] == bg]\n",
        "        n      = len(bg_sub)\n",
        "        if n == 0:\n",
        "            continue\n",
        "        correct = (bg_sub['cutmix_pred'] == cls).sum()\n",
        "        acc     = correct / n\n",
        "        ci_l, ci_u = wilson_ci(correct, n)\n",
        "        ci_records.append([cls, bg, n, acc, ci_l, ci_u])\n",
        "\n",
        "ci_df = pd.DataFrame(ci_records, columns=[\n",
        "    'Cornea_Class', 'Background_Class', 'N', 'Accuracy', 'CI_L', 'CI_U'\n",
        "]).sort_values(['Cornea_Class', 'Background_Class'])\n",
        "\n",
        "print('\\n=== Accuracy & 95 % CI by Background (Cut‑and‑Paste) ===')\n",
        "print(ci_df.to_string(index=False, float_format='%.3f'))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SYbDYhfpCBcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────\n",
        "# 0) 環境準備（Drive, ライブラリ、データ読み込み）\n",
        "# ─────────────────────────────────────────────\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "FILE_PATH = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_cutmix.csv'\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "# ラベル統一\n",
        "label_map = {\n",
        "    \"infection\":\"Infectious keratitis\", \"normal\":\"Normal\",\n",
        "    \"non-infection\":\"Non-infectious keratitis\", \"scar\":\"Scar\",\n",
        "    \"tumor\":\"Tumor\", \"deposit\":\"Deposit\", \"apac\":\"APAC\",\n",
        "    \"lens opacity\":\"Lens opacity\", \"bullous\":\"Bullous keratopathy\"\n",
        "}\n",
        "for col in ['cornea_class','background_class','cutmix_pred']:\n",
        "    df[col] = df[col].replace(label_map)\n",
        "\n",
        "classes = [\n",
        "    \"Normal\",\"Infectious keratitis\",\"Non-infectious keratitis\",\n",
        "    \"Scar\",\"Tumor\",\"Deposit\",\"APAC\",\"Lens opacity\",\"Bullous keratopathy\"\n",
        "]\n",
        "\n",
        "# 空白や NaN を除外\n",
        "valid = df['cutmix_pred'].notna() & (df['cutmix_pred'].str.strip()!='')\n",
        "df = df[valid].copy()\n",
        "df['Correct'] = (df['cutmix_pred'] == df['cornea_class']).astype(int)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 1) 背景ごとの正解数・枚数（基礎テーブル）\n",
        "# ─────────────────────────────────────────────\n",
        "base_tbl = (\n",
        "    df.groupby(['cornea_class','background_class'])['Correct']\n",
        "      .agg(['sum','count'])\n",
        "      .reset_index()\n",
        "      .rename(columns={'sum':'Correct_Cnt','count':'N'})\n",
        ")\n",
        "base_tbl['Accuracy'] = base_tbl['Correct_Cnt'] / base_tbl['N']\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 2) “同一背景” を基準に Z‑test & 効果量 ΔAcc\n",
        "# ─────────────────────────────────────────────\n",
        "records = []\n",
        "for cls in classes:\n",
        "    # 基準（同一背景）\n",
        "    ref_row = base_tbl[(base_tbl['cornea_class']==cls) & (base_tbl['background_class']==cls)]\n",
        "    if ref_row.empty:               # 同一背景が無い稀ケース\n",
        "        continue\n",
        "    n_ref  = int(ref_row['N'])\n",
        "    s_ref  = int(ref_row['Correct_Cnt'])\n",
        "    acc_ref = s_ref / n_ref\n",
        "\n",
        "    # 他 8 背景との比較\n",
        "    sub = base_tbl[(base_tbl['cornea_class']==cls) & (base_tbl['background_class']!=cls)]\n",
        "    for _, r in sub.iterrows():\n",
        "        bg      = r['background_class']\n",
        "        n_bg    = int(r['N'])\n",
        "        s_bg    = int(r['Correct_Cnt'])\n",
        "        acc_bg  = s_bg / n_bg\n",
        "        delta   = acc_bg - acc_ref\n",
        "        # 片側: 背景で精度が低下するか？  => alternative='smaller'\n",
        "        stat, p = proportions_ztest([s_bg, s_ref], [n_bg, n_ref], alternative='smaller')\n",
        "        records.append([cls, bg, n_ref, acc_ref, n_bg, acc_bg, delta, p])\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 3) FDR 補正 & 出力\n",
        "# ─────────────────────────────────────────────\n",
        "res = pd.DataFrame(records, columns=[\n",
        "    'Cornea_Class','Background','N_ref','Acc_ref',\n",
        "    'N_bg','Acc_bg','ΔAcc','P_raw'\n",
        "])\n",
        "\n",
        "# クラスごとに別々で補正\n",
        "adj_p = []\n",
        "for cls in classes:\n",
        "    pvals = res.loc[res['Cornea_Class']==cls,'P_raw']\n",
        "    if len(pvals):\n",
        "        adj = multipletests(pvals, method='fdr_bh')[1]\n",
        "        adj_p.extend(adj)\n",
        "res['Adj_P'] = adj_p\n",
        "\n",
        "# 整形\n",
        "pd.set_option('display.float_format','{:.3e}'.format)\n",
        "print('\\n=== ΔAcc (bg − same) & one‑sided Z‑test ===')\n",
        "print(res[['Cornea_Class','Background','ΔAcc','Acc_ref','Acc_bg','Adj_P']]\n",
        "        .sort_values(['Cornea_Class','ΔAcc'])\n",
        "        .to_string(index=False))\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 4) CSV 保存（任意）\n",
        "# ─────────────────────────────────────────────\n",
        "res.to_csv('background_vs_sameclass_stats.csv', index=False)\n",
        "print('\\n>> 結果を background_vs_sameclass_stats.csv に保存しました')\n"
      ],
      "metadata": {
        "id": "h-6FwUwOvSaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Figure S4**"
      ],
      "metadata": {
        "id": "Yo9Bve3WFu1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────────\n",
        "# Heat‑map & Statistics script  —  p 値は科学表記で表示\n",
        "#   • Original→Cornea：Binomial test (p=1.0)\n",
        "#   • Cornea→Cut‑and‑Paste：McNemar test\n",
        "# ───────────────────────────────────────────────────────────────\n",
        "import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from scipy.stats import binomtest\n",
        "\n",
        "FILE_PATH = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_cutmix.csv'\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "label_map = {\n",
        "    \"infection\": \"Infectious keratitis\", \"normal\": \"Normal\",\n",
        "    \"non-infection\": \"Non-infectious keratitis\", \"scar\": \"Scar\",\n",
        "    \"tumor\": \"Tumor\", \"deposit\": \"Deposit\", \"apac\": \"APAC\",\n",
        "    \"lens opacity\": \"Lens opacity\", \"bullous\": \"Bullous keratopathy\"\n",
        "}\n",
        "for col in ['cornea_class', 'background_class', 'cornea_pred', 'cutmix_pred']:\n",
        "    df[col] = df[col].replace(label_map)\n",
        "\n",
        "classes = [\n",
        "    \"Normal\", \"Infectious keratitis\", \"Non-infectious keratitis\",\n",
        "    \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"\n",
        "]\n",
        "\n",
        "def valid_mask(s):  # 空欄除去\n",
        "    return s.notna() & (s.astype(str).str.strip() != '')\n",
        "\n",
        "def acc_series(pred_col):  # Accuracy by class\n",
        "    vals = []\n",
        "    for cls in classes:\n",
        "        m = (df['cornea_class'] == cls) & valid_mask(df[pred_col])\n",
        "        hits = (df.loc[m, pred_col] == cls).sum()\n",
        "        vals.append(hits / m.sum() if m.sum() else np.nan)\n",
        "    return pd.Series(vals, index=classes)\n",
        "\n",
        "acc_orig   = pd.Series(1.0, index=classes)\n",
        "acc_cornea = acc_series('cornea_pred')\n",
        "acc_cutmix = acc_series('cutmix_pred')\n",
        "\n",
        "# === Heat‑map 描画（省略可：以前と同じ） ===============================\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "cmap = LinearSegmentedColormap.from_list(\n",
        "    'acc_map', ['#2166ac', '#92c5de', '#f7f7f7', '#f4a582', '#b2182b'], N=256\n",
        ")\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 9))\n",
        "for ax, data, title in zip(\n",
        "        axes, [acc_orig, acc_cornea, acc_cutmix],\n",
        "        ['Original', 'Cornea', 'Copy‑and‑Paste']):\n",
        "    sns.heatmap(data.to_frame(), annot=True, fmt=\".2f\",\n",
        "                cmap=cmap, cbar=False, vmin=0, vmax=1,\n",
        "                linewidths=.5, square=True, ax=ax,\n",
        "                annot_kws={'size': 18})\n",
        "    ax.set_title(title, fontsize=22, pad=20)\n",
        "    ax.set_xlabel('')\n",
        "    ax.tick_params(axis='x', labelbottom=False)\n",
        "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=18)\n",
        "    if title == 'Original':\n",
        "        ax.set_ylabel('Ground Truth', fontsize=20)\n",
        "    else:\n",
        "        ax.set_ylabel('')\n",
        "plt.tight_layout(w_pad=3.0)\n",
        "plt.savefig('accuracy_heatmaps.png', dpi=350, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.show()\n",
        "\n",
        "# === p 値付き統計テーブル ==============================================\n",
        "def fmt_p(p):\n",
        "    if pd.isna(p):\n",
        "        return \"nan\"\n",
        "    return f\"{p:.4f}\" if p >= 1e-4 else f\"{p:.1e}\"\n",
        "\n",
        "rows = []\n",
        "for cls in classes:\n",
        "    sub = df[df['cornea_class'] == cls]\n",
        "\n",
        "    m_c   = valid_mask(sub['cornea_pred'])\n",
        "    m_m   = valid_mask(sub['cutmix_pred'])\n",
        "    m_bth = m_c & m_m\n",
        "\n",
        "    # Cornea\n",
        "    n_c = (sub.loc[m_c, 'cornea_pred'] == cls).sum()\n",
        "    N_c = m_c.sum()\n",
        "    acc_c = n_c / N_c if N_c else np.nan\n",
        "    ci_c  = proportion_confint(n_c, N_c, method='wilson') if N_c else (np.nan, np.nan)\n",
        "\n",
        "    # Cut‑and‑Paste\n",
        "    n_m = (sub.loc[m_m, 'cutmix_pred'] == cls).sum()\n",
        "    N_m = m_m.sum()\n",
        "    acc_m = n_m / N_m if N_m else np.nan\n",
        "    ci_m  = proportion_confint(n_m, N_m, method='wilson') if N_m else (np.nan, np.nan)\n",
        "\n",
        "    # Binomial test (p=1.0, two‑sided)\n",
        "    p_binom = binomtest(n_c, N_c, p=1.0, alternative='two-sided').pvalue if (N_c and n_c < N_c) else 1.0\n",
        "\n",
        "    # McNemar test\n",
        "    if m_bth.sum():\n",
        "        good_c = sub.loc[m_bth, 'cornea_pred'] == cls\n",
        "        good_m = sub.loc[m_bth, 'cutmix_pred'] == cls\n",
        "        a = np.logical_and(good_c,  good_m).sum()\n",
        "        b = np.logical_and(good_c, ~good_m).sum()\n",
        "        c = np.logical_and(~good_c, good_m).sum()\n",
        "        d = np.logical_and(~good_c, ~good_m).sum()\n",
        "        p_mcnemar = mcnemar([[a, b], [c, d]], exact=False, correction=True).pvalue\n",
        "    else:\n",
        "        p_mcnemar = np.nan\n",
        "\n",
        "    rows.append([\n",
        "        cls,\n",
        "        N_c, n_c, f\"{acc_c:.3f}\", f\"[{ci_c[0]:.3f},{ci_c[1]:.3f}]\", fmt_p(p_binom),\n",
        "        N_m, n_m, f\"{acc_m:.3f}\", f\"[{ci_m[0]:.3f},{ci_m[1]:.3f}]\", fmt_p(p_mcnemar)\n",
        "    ])\n",
        "\n",
        "stats_df = pd.DataFrame(\n",
        "    rows, columns=[\n",
        "        \"Class\",\n",
        "        \"N_C\",\"Hit_C\",\"Acc_C\",\"95%CI_C\",\"Binom_p\",\n",
        "        \"N_M\",\"Hit_M\",\"Acc_M\",\"95%CI_M\",\"McNemar_p\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\\n=== Accuracy & p-values (scientific notation for very small p) ===\")\n",
        "print(stats_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "sIpSh_TiOj-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Calculating IoU and Pointing Game**"
      ],
      "metadata": {
        "id": "R4z97V-5lfr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# GradCAM / GradCAM++  Pointing game & IoU 解析フルコード（Colab）\n",
        "# ============================================\n",
        "\n",
        "# ❶ Driveマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# ❷ ライブラリ\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# ❸ パス\n",
        "CSV_GT_PRED = Path(\"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam.csv\")\n",
        "CSV_METHODS = Path(\"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_compare_methods.csv\")\n",
        "OUT_DIR     = Path(\"/content\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ❹ CSV読込\n",
        "df_gt = pd.read_csv(CSV_GT_PRED, encoding=\"latin1\")[[\"image_basename\",\"GroundTruth\",\"Predict\"]]\n",
        "df_m  = pd.read_csv(CSV_METHODS,  encoding=\"latin1\")\n",
        "\n",
        "# ❺ マージ & correct/incorrect判定（空白除外）\n",
        "df = df_m.merge(df_gt, on=\"image_basename\", how=\"left\")\n",
        "gt = df[\"GroundTruth\"].astype(str).str.strip()\n",
        "pr = df[\"Predict\"].astype(str).str.strip()\n",
        "valid = (gt!=\"\") & (pr!=\"\") & (~df[\"GroundTruth\"].isna()) & (~df[\"Predict\"].isna())\n",
        "df = df[valid].copy()\n",
        "df[\"correct\"] = np.where(df[\"GroundTruth\"]==df[\"Predict\"], \"correct\", \"incorrect\")\n",
        "\n",
        "# ❻ 指標列\n",
        "metrics_pg  = [\"GradCAM_pointing_game\", \"GradCAM++_pointing_game\"]\n",
        "metrics_iou = [\"GradCAM_IoU\", \"GradCAM++_IoU\"]\n",
        "for m in metrics_pg + metrics_iou:\n",
        "    df[m] = pd.to_numeric(df[m], errors=\"coerce\")\n",
        "\n",
        "# ❼ スタイル（論文用）\n",
        "plt.style.use('default')\n",
        "mpl.rcParams.update({\n",
        "    # 'font.family': 'Arial',\n",
        "    'font.size': 24,\n",
        "    'axes.titlesize': 26,\n",
        "    'axes.labelsize': 26,\n",
        "    'xtick.labelsize': 22,\n",
        "    'ytick.labelsize': 22,\n",
        "    'axes.linewidth': 1.5,\n",
        "    'legend.fontsize': 22\n",
        "})\n",
        "DPI = 350\n",
        "COLOR_CORRECT   = 'white'\n",
        "COLOR_INCORRECT = 'grey'\n",
        "\n",
        "# ❽ 関数\n",
        "def prop_ci(k, n, alpha=0.05):\n",
        "    if n == 0:\n",
        "        return (np.nan, np.nan)\n",
        "    ci = stats.binomtest(k, n).proportion_ci(confidence_level=1-alpha, method='exact')\n",
        "    return ci.low, ci.high\n",
        "\n",
        "def fisher_pointing(metric):\n",
        "    vals_c = df[df[\"correct\"]==\"correct\"][metric].dropna()\n",
        "    vals_i = df[df[\"correct\"]==\"incorrect\"][metric].dropna()\n",
        "    a = int((vals_c==1).sum())\n",
        "    b = int((vals_c==0).sum())\n",
        "    c = int((vals_i==1).sum())\n",
        "    d = int((vals_i==0).sum())\n",
        "    OR, p = stats.fisher_exact([[a,b],[c,d]])\n",
        "    return a,b,c,d,OR,p\n",
        "\n",
        "def cliffs_delta(a,b):\n",
        "    a = np.array(a); b = np.array(b)\n",
        "    gt_count = sum(x>y for x in a for y in b)\n",
        "    lt_count = sum(x<y for x in a for y in b)\n",
        "    return (gt_count - lt_count) / (len(a)*len(b))\n",
        "\n",
        "def mw_iou(metric):\n",
        "    g1 = df[df[\"correct\"]==\"correct\"][metric].dropna()\n",
        "    g2 = df[df[\"correct\"]==\"incorrect\"][metric].dropna()\n",
        "    u, p = stats.mannwhitneyu(g1, g2, alternative=\"two-sided\")\n",
        "    d = cliffs_delta(g1, g2)\n",
        "    return g1, g2, u, p, d\n",
        "\n",
        "# ❾ Pointing game 正解率テーブル\n",
        "rows_pg = []\n",
        "for m in metrics_pg:\n",
        "    for grp in [\"correct\",\"incorrect\"]:\n",
        "        vals = df[df[\"correct\"]==grp][m].dropna()\n",
        "        k = int((vals==1).sum()); n = int(vals.size)\n",
        "        acc = k/n if n>0 else np.nan\n",
        "        low, high = prop_ci(k,n)\n",
        "        rows_pg.append([m, grp, n, k, acc, low, high])\n",
        "pg_table = pd.DataFrame(rows_pg, columns=[\"metric\",\"group\",\"n\",\"k(=1)\",\"accuracy\",\"CI_low\",\"CI_high\"])\n",
        "pg_table.to_csv(OUT_DIR/\"pointing_game_accuracy.csv\", index=False)\n",
        "\n",
        "# ❿ Pointing game 図（GradCAM & GradCAM++ 横並び）\n",
        "methods = [\"GradCAM\", \"GradCAM++\"]\n",
        "groups  = [\"correct\",\"incorrect\"]\n",
        "bar_width = 0.35\n",
        "x = np.arange(len(methods))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,9))\n",
        "for i, grp in enumerate(groups):\n",
        "    accs = []\n",
        "    lows = []\n",
        "    highs= []\n",
        "    for mth in methods:\n",
        "        col = f\"{mth}_pointing_game\"\n",
        "        row = pg_table[(pg_table[\"metric\"]==col) & (pg_table[\"group\"]==grp)].iloc[0]\n",
        "        accs.append(row[\"accuracy\"])\n",
        "        lows.append(row[\"accuracy\"]-row[\"CI_low\"])\n",
        "        highs.append(row[\"CI_high\"]-row[\"accuracy\"])\n",
        "    yerr = [lows, highs]\n",
        "    ax.bar(x + (i-0.5)*bar_width, accs, bar_width,\n",
        "           color=(COLOR_CORRECT if grp==\"correct\" else COLOR_INCORRECT),\n",
        "           edgecolor='black', linewidth=1.5, label=grp.capitalize())\n",
        "    ax.errorbar(x + (i-0.5)*bar_width, accs, yerr=yerr, fmt='none',\n",
        "                ecolor='black', capsize=5, linewidth=1.5)\n",
        "\n",
        "ax.set_ylim(0,1)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods)\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_title(\"Pointing Game Accuracy\")\n",
        "ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.6)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.legend(loc='upper right')\n",
        "plt.tight_layout(pad=1.2)\n",
        "fig.savefig(OUT_DIR/\"PointingGame_accuracy_grouped.png\", dpi=DPI)\n",
        "plt.show()\n",
        "\n",
        "# ⓫ IoU — Pointing gameと同じレイアウトで1枚図に統合\n",
        "# （x軸: [\"GradCAM\",\"GradCAM++\"], 各位置で correct(白)/incorrect(灰) の箱ひげ図）\n",
        "\n",
        "gc_c    = df[df[\"correct\"]==\"correct\"][\"GradCAM_IoU\"    ].dropna()\n",
        "gc_i    = df[df[\"correct\"]==\"incorrect\"][\"GradCAM_IoU\"  ].dropna()\n",
        "gcpp_c  = df[df[\"correct\"]==\"correct\"][\"GradCAM++_IoU\"  ].dropna()\n",
        "gcpp_i  = df[df[\"correct\"]==\"incorrect\"][\"GradCAM++_IoU\"].dropna()\n",
        "\n",
        "# x位置（barと同じ考え方：中心x=0,1 に correct/incorrect を左右オフセット）\n",
        "methods   = [\"GradCAM\", \"GradCAM++\"]\n",
        "x_centers = np.arange(len(methods))\n",
        "offset    = 0.18          # 両側に少しずらす\n",
        "width     = 0.32          # 箱ひげの幅\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 9))\n",
        "\n",
        "# 便利関数：1箱だけ描画するヘルパ\n",
        "def draw_box(data, pos, facecolor):\n",
        "    return ax.boxplot(\n",
        "        [data], positions=[pos], widths=width, patch_artist=True,\n",
        "        boxprops=dict(facecolor=facecolor, edgecolor='black', linewidth=1.5),\n",
        "        whiskerprops=dict(color='black', linewidth=1.5),\n",
        "        capprops=dict(color='black', linewidth=1.5),\n",
        "        medianprops=dict(color='black', linewidth=2),\n",
        "        showfliers=False\n",
        "    )\n",
        "\n",
        "# GradCAM\n",
        "b_gc_c = draw_box(gc_c,   x_centers[0] - offset, COLOR_CORRECT)\n",
        "b_gc_i = draw_box(gc_i,   x_centers[0] + offset, COLOR_INCORRECT)\n",
        "\n",
        "# GradCAM++\n",
        "b_gcpp_c = draw_box(gcpp_c, x_centers[1] - offset, COLOR_CORRECT)\n",
        "b_gcpp_i = draw_box(gcpp_i, x_centers[1] + offset, COLOR_INCORRECT)\n",
        "\n",
        "# 軸・装飾をPointing gameと合わせる\n",
        "ax.set_ylim(0, 1)  # 必要なら調整\n",
        "ax.set_xticks(x_centers)\n",
        "ax.set_xticklabels(methods)\n",
        "ax.set_ylabel(\"IoU\")\n",
        "ax.set_title(\"IoU\")\n",
        "ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.6)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "# 凡例（1つ）\n",
        "ax.legend([b_gc_c[\"boxes\"][0], b_gc_i[\"boxes\"][0]], ['Correct', 'Incorrect'], loc='upper right')\n",
        "\n",
        "plt.tight_layout(pad=1.2)\n",
        "fig.savefig(OUT_DIR / \"IoU_boxplot_grouped_like_pointing.png\", dpi=DPI)\n",
        "plt.show()\n",
        "\n",
        "# ⓬ 統計出力\n",
        "for m in metrics_pg:\n",
        "    a,b,c,d,OR,p = fisher_pointing(m)\n",
        "    print(f\"\\n=== {m} (Pointing game) ===\")\n",
        "    print(f\"2x2 [[a,b],[c,d]] = [[{a},{b}],[{c},{d}]]  (a=correct & pointing=1)\")\n",
        "    print(f\"Accuracy correct = {a/(a+b):.3f} ({a}/{a+b}), incorrect = {c/(c+d):.3f} ({c}/{c+d})\")\n",
        "    print(f\"Fisher exact: OR = {OR:.3f}, p = {p:.3e}\")\n",
        "\n",
        "rows_iou = []\n",
        "for m in metrics_iou:\n",
        "    g1, g2, u, p, d = mw_iou(m)\n",
        "    print(f\"\\n=== {m} (IoU) ===\")\n",
        "    print(f\"n_correct={len(g1)}, median={g1.median():.3f} [IQR {g1.quantile(0.25):.3f}-{g1.quantile(0.75):.3f}]\")\n",
        "    print(f\"n_incorrect={len(g2)}, median={g2.median():.3f} [IQR {g2.quantile(0.25):.3f}-{g2.quantile(0.75):.3f}]\")\n",
        "    print(f\"Mann–Whitney U={u:.1f}, p={p:.3e}, Cliff's δ={d:.3f}\")\n",
        "    rows_iou.append({\n",
        "        \"metric\": m,\n",
        "        \"n_correct\": len(g1),\n",
        "        \"median_correct\": g1.median(),\n",
        "        \"IQR_correct_low\": g1.quantile(0.25),\n",
        "        \"IQR_correct_high\": g1.quantile(0.75),\n",
        "        \"n_incorrect\": len(g2),\n",
        "        \"median_incorrect\": g2.median(),\n",
        "        \"IQR_incorrect_low\": g2.quantile(0.25),\n",
        "        \"IQR_incorrect_high\": g2.quantile(0.75),\n",
        "        \"U\": u, \"p\": p, \"Cliffs_delta\": d\n",
        "    })\n",
        "\n",
        "pd.DataFrame(rows_iou).to_csv(OUT_DIR/\"iou_summary.csv\", index=False)\n",
        "\n",
        "print(\"\\n=== Done ===\")\n",
        "print(f\"Saved figs & CSV to: {OUT_DIR}\")\n"
      ],
      "metadata": {
        "id": "ne7lcJiimrWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output GradCAM & Expert Mask**"
      ],
      "metadata": {
        "id": "N_49VAcAxJpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colabでの実行用コード\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "# !pip install torch --q\n",
        "# !pip install torchvision --q\n",
        "# !pip install -U opencv-python --q\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "\n",
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "# 必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import torchvision\n",
        "from utils.metrics import box_iou\n",
        "import warnings\n",
        "import sys\n",
        "%matplotlib inline\n",
        "\n"
      ],
      "metadata": {
        "id": "GiEFzcAHzDDZ",
        "outputId": "acb2c62b-e55d-4681-ec42-872645f04238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 134 (delta 36), reused 36 (delta 36), pack-reused 94 (from 2)\u001b[K\n",
            "Receiving objects: 100% (134/134), 5.17 MiB | 7.72 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータ設定\n",
        "Image_path = [\n",
        "    \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/195.jpg\",\n",
        "]\n",
        "Expert_annotation_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Expert_annotation_masks\"\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "device = 'cpu'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_conv'\n",
        "names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# FutureWarningを一時的に抑制（register_backward_hook使用時）\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5の必要な関数をインポート - attempt_loadの場所を修正\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # 修正: models.experimentalから\n",
        "\n",
        "# YOLOv5のカスタムクラスをインポートします\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# これまでに登場したクラスをすべてリストに追加します\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# === 最適化: PYTORCH_CUDA_ALLOC_CONFの詳細設定 ===\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
        "\n",
        "# === 最適化: PyTorch 2.0以上の場合、コンパイルキャッシュをリセット ===\n",
        "if hasattr(torch._dynamo, 'reset'):\n",
        "    torch._dynamo.reset()\n",
        "\n",
        "# YOLOV5TorchObjectDetectorクラスの定義\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self, model_weight, device, img_size, names=None, mode='eval',\n",
        "                 confidence=0.25, iou_thresh=0.45, agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45,\n",
        "                           classes=None, agnostic=False, multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}'\n",
        "\n",
        "        min_wh, max_wh = 2, 4096\n",
        "        max_nms = 30000\n",
        "        time_limit = 10.0\n",
        "        redundant = True\n",
        "        multi_label &= nc > 1\n",
        "        merge = False\n",
        "\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img, im0_shape=None):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence,\n",
        "                                                      self.iou_thresh, classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "\n",
        "        for i, det in enumerate(prediction):\n",
        "            if len(det) and im0_shape is not None:\n",
        "                # スケールボックスを元の画像サイズに戻す\n",
        "                det[:, :4] = self.scale_boxes(img.shape[2:], det[:, :4], im0_shape).round()\n",
        "\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # 座標を整数に変換\n",
        "                    bbox = [int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])]\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
        "        \"\"\"Rescale boxes (xyxy) from img1_shape to img0_shape\"\"\"\n",
        "        if ratio_pad is None:  # calculate from img0_shape\n",
        "            gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
        "            pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
        "        else:\n",
        "            gain = ratio_pad[0][0]\n",
        "            pad = ratio_pad[1]\n",
        "\n",
        "        boxes[:, [0, 2]] -= pad[0]  # x padding\n",
        "        boxes[:, [1, 3]] -= pad[1]  # y padding\n",
        "        boxes[:, :4] /= gain\n",
        "        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(0, img0_shape[1])  # x1, x2\n",
        "        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(0, img0_shape[0])  # y1, y2\n",
        "        return boxes\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# GradCAM関連の関数定義\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, im0_shape=None, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img, im0_shape=im0_shape)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            score.backward(retain_graph=True)\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            if self.method == 'gradcam':\n",
        "                weights = self._gradcam_weights(gradients, b, k)\n",
        "            elif self.method == 'gradcampp':\n",
        "                weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def __call__(self, input_img, im0_shape=None):\n",
        "        return self.forward(input_img, im0_shape)\n",
        "\n",
        "def apply_heatmap_to_image(mask, image, reverse_colormap=False):\n",
        "    \"\"\"ヒートマップを画像に適用\"\"\"\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    if reverse_colormap:\n",
        "        # カラーマップを逆転\n",
        "        heatmap = cv2.applyColorMap(255 - mask, cv2.COLORMAP_JET)\n",
        "    else:\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image_norm = (image / 255).astype(np.float32)\n",
        "    res_img = cv2.addWeighted(image_norm, 0.5, n_heatmap, 0.5, 0)\n",
        "    return (res_img * 255).astype(np.uint8)\n",
        "\n",
        "def apply_heatmap_to_bbox(mask, image, boxes, reverse_colormap=False):\n",
        "    \"\"\"バウンディングボックス内のみにヒートマップを適用\"\"\"\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
        "\n",
        "    # 結果画像の初期化\n",
        "    result_img = image.copy()\n",
        "\n",
        "    # 各バウンディングボックスに対してヒートマップを適用\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "\n",
        "        # 座標が画像範囲内にあることを確認\n",
        "        x1 = max(0, min(x1, image.shape[1]))\n",
        "        y1 = max(0, min(y1, image.shape[0]))\n",
        "        x2 = max(0, min(x2, image.shape[1]))\n",
        "        y2 = max(0, min(y2, image.shape[0]))\n",
        "\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "\n",
        "        # バウンディングボックス内のマスクを取得\n",
        "        bbox_mask = mask[y1:y2, x1:x2]\n",
        "\n",
        "        if bbox_mask.size == 0:\n",
        "            continue\n",
        "\n",
        "        if reverse_colormap:\n",
        "            bbox_heatmap = cv2.applyColorMap(255 - bbox_mask, cv2.COLORMAP_JET)\n",
        "        else:\n",
        "            bbox_heatmap = cv2.applyColorMap(bbox_mask, cv2.COLORMAP_JET)\n",
        "\n",
        "        # バウンディングボックス内の画像を取得\n",
        "        bbox_img = result_img[y1:y2, x1:x2]\n",
        "\n",
        "        # ヒートマップとマージ\n",
        "        n_heatmap = (bbox_heatmap / 255).astype(np.float32)\n",
        "        img_norm = (bbox_img / 255).astype(np.float32)\n",
        "        merged = cv2.addWeighted(img_norm, 0.5, n_heatmap, 0.5, 0)\n",
        "\n",
        "        # 結果画像に戻す\n",
        "        result_img[y1:y2, x1:x2] = (merged * 255).astype(np.uint8)\n",
        "\n",
        "        # バウンディングボックスを描画（オプション）\n",
        "        cv2.rectangle(result_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    return result_img\n",
        "\n",
        "def load_expert_annotation(annotation_dir, image_basename):\n",
        "    \"\"\"Expert annotationマスクを読み込む\"\"\"\n",
        "    # basenameから拡張子を除去\n",
        "    base_name = os.path.splitext(image_basename)[0]\n",
        "    # マスクファイルを探す（異なる拡張子に対応）\n",
        "    for ext in ['.png', '.jpg', '.jpeg', '.bmp']:\n",
        "        mask_path = os.path.join(annotation_dir, base_name + ext)\n",
        "        if os.path.exists(mask_path):\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "            return mask\n",
        "    print(f\"Warning: Expert annotation not found for {base_name}\")\n",
        "    return None\n",
        "\n",
        "def apply_expert_mask_to_image(mask, image):\n",
        "    \"\"\"Expert annotationマスクを画像に適用\"\"\"\n",
        "    if mask is None:\n",
        "        return image\n",
        "    # マスクをリサイズ\n",
        "    mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
        "    # マスクを3チャンネルに変換して色を付ける\n",
        "    colored_mask = np.zeros_like(image)\n",
        "    colored_mask[:, :, 1] = mask  # 緑チャンネルに適用\n",
        "    # 画像とマージ\n",
        "    mask_norm = mask / 255.0\n",
        "    result = image.copy()\n",
        "    result[:, :, 1] = np.clip(result[:, :, 1] + mask_norm * 128, 0, 255)\n",
        "    return result.astype(np.uint8)\n",
        "\n",
        "# モデルの初期化\n",
        "input_size = (img_size, img_size)\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "# 各画像を処理\n",
        "for img_path in Image_path:\n",
        "    print(f\"\\nProcessing: {img_path}\")\n",
        "\n",
        "    # 画像を読み込み\n",
        "    img = cv2.imread(img_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 元画像のサイズを保存\n",
        "    im0_shape = img.shape[:2]  # (height, width)\n",
        "\n",
        "    # 画像の前処理\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    # YOLOで物体検出を実行してbboxを取得（元画像サイズを渡す）\n",
        "    preds, logits = model(torch_img, im0_shape=im0_shape)\n",
        "    boxes_batch, classes_batch, class_names_batch, confidences_batch = preds\n",
        "\n",
        "    # 最初の画像のbboxを取得\n",
        "    if len(boxes_batch) > 0 and len(boxes_batch[0]) > 0:\n",
        "        boxes = boxes_batch[0]  # 最初のバッチの結果\n",
        "        print(f\"Detected {len(boxes)} objects: {class_names_batch[0]}\")\n",
        "        # デバッグ情報を表示\n",
        "        for i, (box, cls_name, conf) in enumerate(zip(boxes, class_names_batch[0], confidences_batch[0])):\n",
        "            print(f\"  {i+1}. {cls_name} (conf: {conf}): {box}\")\n",
        "    else:\n",
        "        boxes = []\n",
        "        print(\"No objects detected\")\n",
        "\n",
        "    # GradCAM\n",
        "    gradcam_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method='gradcam')\n",
        "    masks_gradcam, _, _, _ = gradcam_method(torch_img, im0_shape=im0_shape)\n",
        "\n",
        "    # GradCAM++\n",
        "    gradcampp_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method='gradcampp')\n",
        "    masks_gradcampp, _, _, _ = gradcampp_method(torch_img, im0_shape=im0_shape)\n",
        "\n",
        "    # Expert annotation\n",
        "    image_basename = os.path.basename(img_path)\n",
        "    expert_mask = load_expert_annotation(Expert_annotation_dir, image_basename)\n",
        "\n",
        "    # 画像の準備（元画像はbboxなし）\n",
        "    original_img = img_rgb.copy()\n",
        "\n",
        "    # GradCAMを適用（bbox内のみ、色反転）\n",
        "    if len(masks_gradcam) > 0 and len(boxes) > 0:\n",
        "        gradcam_img = apply_heatmap_to_bbox(masks_gradcam[0], img_rgb, boxes, reverse_colormap=True)\n",
        "    else:\n",
        "        gradcam_img = img_rgb.copy()\n",
        "\n",
        "    # GradCAM++を適用（bbox内のみ、色反転）\n",
        "    if len(masks_gradcampp) > 0 and len(boxes) > 0:\n",
        "        gradcampp_img = apply_heatmap_to_bbox(masks_gradcampp[0], img_rgb, boxes, reverse_colormap=True)\n",
        "    else:\n",
        "        gradcampp_img = img_rgb.copy()\n",
        "\n",
        "    # Expert annotationを適用\n",
        "    expert_img = apply_expert_mask_to_image(expert_mask, img_rgb)\n",
        "\n",
        "    # 4つの画像を表示\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "\n",
        "    # 元画像（bboxなし）\n",
        "    axes[0, 0].imshow(original_img)\n",
        "    axes[0, 0].set_title('Original Image')\n",
        "    axes[0, 0].axis('off')\n",
        "\n",
        "    # GradCAM（bbox内のみ）\n",
        "    axes[0, 1].imshow(gradcam_img)\n",
        "    axes[0, 1].set_title('GradCAM (BBox only, Reversed)')\n",
        "    axes[0, 1].axis('off')\n",
        "\n",
        "    # GradCAM++（bbox内のみ）\n",
        "    axes[1, 0].imshow(gradcampp_img)\n",
        "    axes[1, 0].set_title('GradCAM++ (BBox only, Reversed)')\n",
        "    axes[1, 0].axis('off')\n",
        "\n",
        "    # Expert annotation\n",
        "    axes[1, 1].imshow(expert_img)\n",
        "    axes[1, 1].set_title('Expert Annotation')\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\nProcessing completed!\")"
      ],
      "metadata": {
        "id": "dL4jSiQPxJy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "import warnings\n",
        "\n",
        "# FutureWarningを一時的に抑制（register_backward_hook使用時）\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5の必要な関数をインポート - attempt_loadの場所を修正\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # 修正: models.experimentalから\n",
        "\n",
        "# YOLOv5のカスタムクラスをインポートします\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# これまでに登場したクラスをすべてリストに追加します\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                          'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "                          'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
        "                          'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "                          'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "                          'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "# def get_res_img(bbox, masks, res_img):\n",
        "#     for mask in masks:\n",
        "#         # NaNやInfを0に置き換え\n",
        "#         mask = torch.nan_to_num(mask.squeeze(), nan=0.0, posinf=1.0, neginf=0.0)\n",
        "#         # スケーリングと型変換\n",
        "#         mask = mask.mul(255).add_(0.5).clamp_(0, 255)\n",
        "#         mask = mask.detach().cpu().numpy()\n",
        "#         # 明示的に型変換\n",
        "#         mask = np.clip(mask, 0, 255).astype(np.uint8)\n",
        "\n",
        "#         heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "#         n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "#         res_img = res_img / 255\n",
        "#         res_img = cv2.add(res_img, n_heatmat)\n",
        "#         res_img = (res_img / res_img.max())\n",
        "#     return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img): ##マージしているレイヤー名\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, f\"{cls_names[0]} - {method} - {layer_name}\", res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    #cv2_imshow(final_image)\n",
        "    cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path, method, file_list):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        ##target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv'] ##\n",
        "        #target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "        #target_layers = ['model_17_m_0_cv2_conv', 'model_20_m_0_cv2_conv', 'model_23_m_0_cv2_conv']\n",
        "        target_layers = ['model_23_cv3_conv', 'model_23_m_0_cv2_conv', 'model_23_cv3_act'] ##\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            #print(f\"boxes: {boxes}\")\n",
        "\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, f\"{cls_names[0]}-{'layer'+layer_name.replace('model_', '')}\", res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image) ##画像を表示\n",
        "        #cv2.imwrite(output_path, final_image) ##画像を保存\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer24'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho_layer24'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer_m_0_2_cv2_conv'  # 出力ディレクトリ\n",
        "    output_dir = '/content'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # ファイル名を数字でソート\n",
        "    if os.path.isdir(img_path):\n",
        "        file_list = sorted(os.listdir(img_path), key=lambda x: int(x.split('.')[0]))\n",
        "        ##########必要に応じて、処理するファイルの範囲を指定########\n",
        "        file_list = file_list[200:201]\n",
        "        ############################################################\n",
        "\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "    #for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method, file_list)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "E07elXv1D2kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorchのシリアライズに関する設定\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# YOLOv5のユーティリティ関数をインポート\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# パラメータ設定\n",
        "Image_path = [\n",
        "    \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/195.jpg\",\n",
        "]\n",
        "Expert_annotation_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Expert_annotation_masks\"\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "device = 'cpu'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_conv'\n",
        "names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# FutureWarningを一時的に抑制\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5のカスタムクラスを安全なグローバルとして追加\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# メモリとキャッシュの最適化\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
        "if hasattr(torch._dynamo, 'reset'):\n",
        "    torch._dynamo.reset()\n",
        "\n",
        "# YOLOV5TorchObjectDetectorクラスの定義\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self, model_weight, device, img_size, names=None, mode='eval',\n",
        "                 confidence=0.25, iou_thresh=0.45, agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names\n",
        "\n",
        "        # コールドスタート防止\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45,\n",
        "                           classes=None, agnostic=False, multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}'\n",
        "\n",
        "        min_wh, max_wh = 2, 4096\n",
        "        max_nms = 30000\n",
        "        redundant = True\n",
        "        multi_label &= nc > 1\n",
        "        merge = False\n",
        "\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img, im0_shape=None):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence,\n",
        "                                                      self.iou_thresh, classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "\n",
        "        for i, det in enumerate(prediction):\n",
        "            if len(det) and im0_shape is not None:\n",
        "                det[:, :4] = self.scale_boxes(img.shape[2:], det[:, :4], im0_shape).round()\n",
        "\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    bbox = [int(p) for p in xyxy]\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls] if self.names else cls)\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
        "        if ratio_pad is None:\n",
        "            gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])\n",
        "            pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2\n",
        "        else:\n",
        "            gain = ratio_pad[0][0]\n",
        "            pad = ratio_pad[1]\n",
        "\n",
        "        boxes[:, [0, 2]] -= pad[0]\n",
        "        boxes[:, [1, 3]] -= pad[1]\n",
        "        boxes[:, :4] /= gain\n",
        "        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(0, img0_shape[1])\n",
        "        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(0, img0_shape[0])\n",
        "        return boxes\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# GradCAM関連の関数定義\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = {}\n",
        "        self.activations = {}\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(self.forward_hook)\n",
        "        target_layer.register_backward_hook(self.backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def backward_hook(self, module, grad_input, grad_output):\n",
        "        self.gradients['value'] = grad_output[0]\n",
        "\n",
        "    def forward_hook(self, module, input, output):\n",
        "        self.activations['value'] = output\n",
        "\n",
        "    def forward(self, input_img, im0_shape=None, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img, im0_shape=im0_shape)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            score = logits[0][0][cls] if class_idx else logits[0][0].max()\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            score.backward(retain_graph=True)\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            weights = self._get_cam_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _get_cam_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        if self.method == 'gradcam':\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            return alpha.view(b, k, 1, 1)\n",
        "        elif self.method == 'gradcampp':\n",
        "            alpha_num = gradients.pow(2)\n",
        "            alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "                activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "            alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "            alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "            relu_grad = F.relu(score.exp() * gradients)\n",
        "            return (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "    def __call__(self, input_img, im0_shape=None):\n",
        "        return self.forward(input_img, im0_shape)\n",
        "\n",
        "# ⭐️⭐️⭐️ ここを修正 ⭐️⭐️⭐️\n",
        "def apply_heatmap_to_bbox(mask, image, boxes, reverse_colormap=False):\n",
        "    \"\"\"\n",
        "    バウンディングボックス内のみにヒートマップを適用\n",
        "    (cv2.add + max normalization方式)\n",
        "    \"\"\"\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
        "\n",
        "    result_img = image.copy()\n",
        "\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(image.shape[1], x2), min(image.shape[0], y2)\n",
        "\n",
        "        if x2 <= x1 or y2 <= y1: continue\n",
        "\n",
        "        bbox_mask = mask[y1:y2, x1:x2]\n",
        "        if bbox_mask.size == 0: continue\n",
        "\n",
        "        # ヒートマップ生成\n",
        "        heatmap_color = 255 - bbox_mask if reverse_colormap else bbox_mask\n",
        "        bbox_heatmap = cv2.applyColorMap(heatmap_color, cv2.COLORMAP_JET)\n",
        "\n",
        "        # BBox内の元画像\n",
        "        bbox_img = result_img[y1:y2, x1:x2]\n",
        "\n",
        "        # --- 合成ロジックを修正 ---\n",
        "        n_heatmap = (bbox_heatmap / 255).astype(np.float32)\n",
        "        img_norm = (bbox_img / 255).astype(np.float32)\n",
        "\n",
        "        # 1. cv2.add を使用して合成\n",
        "        merged = cv2.add(img_norm, n_heatmap)\n",
        "\n",
        "        # 2. 最大値で正規化 (ゼロ除算を防止)\n",
        "        max_val = merged.max()\n",
        "        if max_val > 0:\n",
        "            merged = merged / max_val\n",
        "        # --- 修正ここまで ---\n",
        "\n",
        "        # 結果を画像に戻す\n",
        "        result_img[y1:y2, x1:x2] = (merged * 255).astype(np.uint8)\n",
        "\n",
        "        # バウンディングボックスを描画\n",
        "        cv2.rectangle(result_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    return result_img\n",
        "\n",
        "def load_expert_annotation(annotation_dir, image_basename):\n",
        "    \"\"\"Expert annotationマスクを読み込む\"\"\"\n",
        "    base_name = os.path.splitext(image_basename)[0]\n",
        "    for ext in ['.png', '.jpg', '.jpeg', '.bmp']:\n",
        "        mask_path = os.path.join(annotation_dir, base_name + ext)\n",
        "        if os.path.exists(mask_path):\n",
        "            return cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    print(f\"Warning: Expert annotation not found for {base_name}\")\n",
        "    return None\n",
        "\n",
        "def apply_expert_mask_to_image(mask, image):\n",
        "    \"\"\"Expert annotationマスクを画像に適用\"\"\"\n",
        "    if mask is None: return image\n",
        "    mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
        "    # マスクを緑色で半透明に重ねる\n",
        "    colored_mask = np.zeros_like(image)\n",
        "    colored_mask[mask > 0] = [0, 255, 0] # 緑色\n",
        "    return cv2.addWeighted(image, 0.7, colored_mask, 0.3, 0)\n",
        "\n",
        "# --- メイン処理 ---\n",
        "def main():\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=(img_size, img_size), names=names)\n",
        "\n",
        "    for img_path in Image_path:\n",
        "        print(f\"\\nProcessing: {img_path}\")\n",
        "        img = cv2.imread(img_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        im0_shape = img.shape[:2]\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "        preds, _ = model(torch_img, im0_shape=im0_shape)\n",
        "        boxes = preds[0][0] if preds[0] else []\n",
        "\n",
        "        if boxes:\n",
        "            print(f\"Detected {len(boxes)} objects: {preds[2][0]}\")\n",
        "        else:\n",
        "            print(\"No objects detected\")\n",
        "\n",
        "        # GradCAM\n",
        "        gradcam = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=(img_size, img_size), method='gradcam')\n",
        "        masks_gc, _, _, _ = gradcam(torch_img, im0_shape=im0_shape)\n",
        "        gradcam_img = apply_heatmap_to_bbox(masks_gc[0], img_rgb, boxes, reverse_colormap=True) if masks_gc and boxes else img_rgb.copy()\n",
        "\n",
        "        # GradCAM++\n",
        "        gradcampp = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=(img_size, img_size), method='gradcampp')\n",
        "        masks_gcpp, _, _, _ = gradcampp(torch_img, im0_shape=im0_shape)\n",
        "        gradcampp_img = apply_heatmap_to_bbox(masks_gcpp[0], img_rgb, boxes, reverse_colormap=True) if masks_gcpp and boxes else img_rgb.copy()\n",
        "\n",
        "        # Expert annotation\n",
        "        expert_mask = load_expert_annotation(Expert_annotation_dir, os.path.basename(img_path))\n",
        "        expert_img = apply_expert_mask_to_image(expert_mask, img_rgb.copy())\n",
        "\n",
        "        # 4つの画像を表示\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "        axes[0, 0].imshow(img_rgb)\n",
        "        axes[0, 0].set_title('Original Image')\n",
        "        axes[0, 0].axis('off')\n",
        "\n",
        "        axes[0, 1].imshow(gradcam_img)\n",
        "        axes[0, 1].set_title('GradCAM (BBox only, Reversed, Add+MaxNorm)')\n",
        "        axes[0, 1].axis('off')\n",
        "\n",
        "        axes[1, 0].imshow(gradcampp_img)\n",
        "        axes[1, 0].set_title('GradCAM++ (BBox only, Reversed, Add+MaxNorm)')\n",
        "        axes[1, 0].axis('off')\n",
        "\n",
        "        axes[1, 1].imshow(expert_img)\n",
        "        axes[1, 1].set_title('Expert Annotation')\n",
        "        axes[1, 1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    print(\"\\nProcessing completed!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Google ColabなどでYOLOv5リポジトリをクローンし、パスを通す必要があります\n",
        "    # 例: !git clone https://github.com/ultralytics/yolov5\n",
        "    # import sys\n",
        "    # sys.path.append('yolov5')\n",
        "    main()"
      ],
      "metadata": {
        "id": "Gux_P1DjqeDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "import warnings\n",
        "\n",
        "# FutureWarningを一時的に抑制\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5の関連モジュールをインポート\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "\n",
        "# PyTorchのシリアライズのための設定\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names\n",
        "        # コールドスタート防止\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}'\n",
        "\n",
        "        max_nms = 30000\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else 4096)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "\n",
        "        for i, det in enumerate(prediction):\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy_clamped = [max(0, xyxy[0]), max(0, xyxy[1]), min(self.img_size[1], xyxy[2]), min(self.img_size[0], xyxy[3])]\n",
        "                    bbox = Box.box2box(xyxy_clamped, in_source=Box.BoxSource.Torch, to_source=Box.BoxSource.Numpy, return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls_int = int(cls.item())\n",
        "                    self.classes[i].append(cls_int)\n",
        "                    self.class_names[i].append(self.names[cls_int] if self.names else cls_int)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    \"\"\"ヒートマップを半透明に合成する (サイズ不一致エラー対策済み)\"\"\"\n",
        "    blended_img = res_img.copy()\n",
        "    final_heatmap = None\n",
        "\n",
        "    if not masks:\n",
        "        return blended_img, final_heatmap\n",
        "\n",
        "    mask = masks[0]\n",
        "    mask_np = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    # 従来通りのカラーマップ(JET)を適用\n",
        "    heatmap = cv2.applyColorMap(mask_np, cv2.COLORMAP_JET)\n",
        "\n",
        "    # ⭐️⭐️⭐️ Box.fill_outer_boxをより安全な手動実装に置き換え ⭐️⭐️⭐️\n",
        "    # 1. 元画像と同じサイズの黒い背景を作成\n",
        "    heatmap_full_size = np.zeros_like(res_img, dtype=np.uint8)\n",
        "\n",
        "    # 2. BBoxの座標を取得し、画像の範囲内に収める\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    h, w, _ = res_img.shape\n",
        "    x1, y1 = max(0, x1), max(0, y1)\n",
        "    x2, y2 = min(w, x2), min(h, y2)\n",
        "\n",
        "    # 3. BBoxが有効なサイズか確認\n",
        "    if x2 > x1 and y2 > y1:\n",
        "        # 4. ヒートマップからBBox部分を切り出し、黒い背景の対応する位置に貼り付け\n",
        "        #    heatmap自体がres_imgと同じサイズなので、そのまま切り出す\n",
        "        heatmap_roi = heatmap[y1:y2, x1:x2]\n",
        "        heatmap_full_size[y1:y2, x1:x2] = heatmap_roi\n",
        "\n",
        "    # これで heatmap_full_size は res_img と完全に同じサイズになる\n",
        "    blended_img = cv2.addWeighted(res_img, 0.6, heatmap_full_size, 0.4, 0)\n",
        "    final_heatmap = heatmap\n",
        "\n",
        "    return blended_img, final_heatmap\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    # temp.jpgへの書き出し・読み込みは冗長なため、直接描画するように変更\n",
        "    img_with_box = Box.put_box(res_img, bbox)\n",
        "    cv2.putText(img_with_box, cls_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
        "    return img_with_box\n",
        "\n",
        "def concat_images(images):\n",
        "    # すべての画像を同じ高さに正規化してから連結\n",
        "    if not images:\n",
        "        return np.zeros((100, 100, 3), dtype=np.uint8)\n",
        "    h = images[0].shape[0]\n",
        "    resized_images = [cv2.resize(img, (int(img.shape[1] * h / img.shape[0]), h)) for img in images]\n",
        "\n",
        "    # 横に連結 (concatenate horizontally)\n",
        "    return cv2.hconcat(resized_images)\n",
        "\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = {}\n",
        "        self.activations = {}\n",
        "        self.method = method\n",
        "        self.img_size = img_size\n",
        "        self.cls_names = []\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        self.forward_handle = target_layer.register_forward_hook(self.forward_hook)\n",
        "        self.backward_handle = target_layer.register_backward_hook(self.backward_hook)\n",
        "\n",
        "    def forward_hook(self, module, input, output):\n",
        "        self.activations['value'] = output\n",
        "\n",
        "    def backward_hook(self, module, grad_input, grad_output):\n",
        "        self.gradients['value'] = grad_output[0]\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        self.forward_handle.remove()\n",
        "        self.backward_handle.remove()\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        for cls in preds[1][0]:\n",
        "            score = logits[0][0][cls] if class_idx else logits[0][0].max()\n",
        "            self.model.zero_grad()\n",
        "            score.backward(retain_graph=True)\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            if self.method == 'gradcam':\n",
        "                alpha = gradients.view(b, k, -1).mean(2)\n",
        "                weights = alpha.view(b, k, 1, 1)\n",
        "            elif self.method == 'gradcampp':\n",
        "                alpha_num = gradients.pow(2)\n",
        "                alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "                    activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "                alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "                alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "                relu_grad = F.relu(score.exp() * gradients)\n",
        "                weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            if saliency_map_max > saliency_map_min:\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- パラメータ設定 ---\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'\n",
        "    img_size = 640\n",
        "    output_dir = '/content/gradcam_results'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 処理したい画像のフルパスをリストで指定\n",
        "    image_paths_to_process = [\n",
        "        \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/195.jpg\",\n",
        "        \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/196.jpg\",\n",
        "    ]\n",
        "\n",
        "    # --- メイン処理 ---\n",
        "    input_size_tuple = (img_size, img_size)\n",
        "\n",
        "    # モデルを一度だけロード\n",
        "    print(\"--- Loading Model ---\")\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size_tuple, names=names)\n",
        "\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "        print(f\"\\n--- Running Method: {method.upper()} ---\")\n",
        "\n",
        "        for img_path in image_paths_to_process:\n",
        "            print(f\"Processing image: {os.path.basename(img_path)}\")\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Warning: Could not read image at {img_path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # 元の画像の縦横比を維持しながらリサイズ & パディング\n",
        "            h, w = img.shape[:2]\n",
        "            r = img_size / max(h, w)\n",
        "            new_w, new_h = int(w * r), int(h * r)\n",
        "            resized_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            top = bottom = (img_size - new_h) // 2\n",
        "            left = right = (img_size - new_w) // 2\n",
        "            padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "\n",
        "            torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "            target_layers = ['model_23_cv3_conv', 'model_23_m_0_cv2_conv', 'model_23_cv3_act']\n",
        "            result_images = [padded_img]\n",
        "\n",
        "            for layer_name in target_layers:\n",
        "                saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size_tuple, method=method)\n",
        "                masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "                saliency_method.remove_hooks()\n",
        "\n",
        "                res_img = padded_img.copy()\n",
        "                if masks and boxes and boxes[0]:\n",
        "                    bbox = boxes[0][0]\n",
        "                    mask = masks[0]\n",
        "                    # ⭐️⭐️⭐️ 修正された関数呼び出し ⭐️⭐️⭐️\n",
        "                    # get_res_imgは合成済みのuint8画像を返す\n",
        "                    blended_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    label = f\"{cls_names[0]}-layer:{layer_name.replace('model_', '')}\"\n",
        "                    # put_text_boxには直接blended_img(uint8)を渡す\n",
        "                    res_img = put_text_box(bbox, label, blended_img)\n",
        "\n",
        "                result_images.append(res_img)\n",
        "\n",
        "            # 結果画像を横に連結して保存/表示\n",
        "            final_image = concat_images(result_images)\n",
        "            img_basename = os.path.basename(img_path)\n",
        "            img_name_without_ext = os.path.splitext(img_basename)[0]\n",
        "            output_filename = f'{img_name_without_ext}-res-{method}.jpg'\n",
        "            output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "            print(f'Saving the final image at {output_path}')\n",
        "            cv2_imshow(final_image)\n",
        "            cv2.imwrite(output_path, final_image)\n",
        "\n",
        "            # メモリ解放\n",
        "            del saliency_method, masks, logits, boxes, result_images, final_image\n",
        "            gc.collect()\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5NJ5R8tAu9G9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}