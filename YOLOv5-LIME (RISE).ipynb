{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkO0EZU5lBu92a6yVN8IUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5-LIME%20(RISE).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 LIME CorneAI**"
      ],
      "metadata": {
        "id": "mJcOrPki6NXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "Jd_yXvLX6WEA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "decXjLeF5tvO",
        "outputId": "6c65eb9a-209c-427f-9bf9-de9432b53783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62qgDIr46oS9",
        "outputId": "7433e87c-296a-4610-effe-911e31f273e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 29.03 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ],
      "metadata": {
        "id": "-MjZmDMc6oaL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LIME**"
      ],
      "metadata": {
        "id": "uuseY-WV67TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime --q\n",
        "!pip install scikit-image --q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgNy8U2a72ss",
        "outputId": "6b5ff298-acba-492e-b127-85a92cfd99d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import cv2\n",
        "import time\n",
        "import traceback\n",
        "import torchvision\n",
        "from lime import lime_image\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "%matplotlib inline\n",
        "\n",
        "# 角膜AIのクラス定義\n",
        "CORNEA_CLASSES = [\n",
        "    \"infection\",\n",
        "    \"normal\",\n",
        "    \"non-infection\",\n",
        "    \"scar\",\n",
        "    \"tumor\",\n",
        "    \"deposit\",\n",
        "    \"APAC\",\n",
        "    \"lens opacity\",\n",
        "    \"bullous\"\n",
        "]\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"\n",
        "    GPUが利用可能な場合はGPUを、そうでない場合はCPUを設定\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    device : torch.device\n",
        "        使用するデバイス\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"GPU not available, using CPU\")\n",
        "    return device\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size=(640, 640),\n",
        "                 names=CORNEA_CLASSES,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        # モデルのロード\n",
        "        print(\"[INFO] Loading cornea detection model...\")\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model loaded successfully\")\n",
        "\n",
        "        # モデルのクラス数を取得と確認\n",
        "        self.nc = int(self.model.nc)\n",
        "        print(f\"[INFO] Number of classes: {self.nc}\")\n",
        "\n",
        "        # クラス名の設定と検証\n",
        "        self.names = names\n",
        "        if len(self.names) != self.nc:\n",
        "            print(f\"[WARNING] Number of class names ({len(self.names)}) does not match model classes ({self.nc})\")\n",
        "        print(f\"[INFO] Using class names: {self.names}\")\n",
        "\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        # Cold start prevention\n",
        "        print(\"[INFO] Performing cold start prevention...\")\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "        print(\"[INFO] Initialization complete\")\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096\n",
        "        max_nms = 30000\n",
        "        time_limit = 10.0\n",
        "        redundant = True\n",
        "        multi_label &= nc > 1\n",
        "        merge = False\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        try:\n",
        "            prediction, logits, _ = self.model(img, augment=False)\n",
        "            prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                          classes=None,\n",
        "                                                          agnostic=self.agnostic)\n",
        "\n",
        "            batch_size = img.shape[0]\n",
        "            self.boxes = [[] for _ in range(batch_size)]\n",
        "            self.class_names = [[] for _ in range(batch_size)]\n",
        "            self.classes = [[] for _ in range(batch_size)]\n",
        "            self.confidences = [[] for _ in range(batch_size)]\n",
        "\n",
        "            for i, det in enumerate(prediction):\n",
        "                if len(det):\n",
        "                    for *xyxy, conf, cls in det:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(self.img_size[1], xyxy[2])\n",
        "                        xyxy[3] = min(self.img_size[0], xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(float(conf.item()))\n",
        "                        cls_idx = int(cls.item())\n",
        "\n",
        "                        if cls_idx >= len(self.names):\n",
        "                            print(f\"[WARNING] Class index {cls_idx} is out of range\")\n",
        "                            cls_idx = 0\n",
        "\n",
        "                        self.classes[i].append(cls_idx)\n",
        "                        self.class_names[i].append(self.names[cls_idx])\n",
        "\n",
        "            return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in forward pass: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return [[[]], [[]], [[]], [[]]], None\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        try:\n",
        "            if len(img.shape) != 4:\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "            im0 = img.astype(np.uint8)\n",
        "            img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "            img = img.transpose((0, 3, 1, 2))\n",
        "            img = np.ascontiguousarray(img)\n",
        "            img = torch.from_numpy(img).to(self.device)\n",
        "            img = img / 255.0\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preprocessing: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "class YOLOLimeExplainer:\n",
        "    def __init__(self, yolo_model, device='cuda', img_size=(640, 640)):\n",
        "        self.model = yolo_model\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    def predict_fn(self, images):\n",
        "        try:\n",
        "            batch_predictions = []\n",
        "            for img in images:\n",
        "                processed_img = self.model.preprocessing(np.expand_dims(img, 0))\n",
        "                if processed_img is None:\n",
        "                    raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    predictions, _ = self.model(processed_img)\n",
        "\n",
        "                class_scores = np.zeros(len(CORNEA_CLASSES))\n",
        "\n",
        "                if predictions[0][0]:\n",
        "                    for cls_idx, conf in zip(predictions[1][0], predictions[3][0]):\n",
        "                        if cls_idx < len(CORNEA_CLASSES):\n",
        "                            class_scores[cls_idx] = max(class_scores[cls_idx], conf)\n",
        "\n",
        "                batch_predictions.append(class_scores)\n",
        "\n",
        "            return np.array(batch_predictions)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction_fn: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return np.zeros((len(images), len(CORNEA_CLASSES)))\n",
        "\n",
        "    def explain_instance(self, image, num_samples=1000, top_labels=5):\n",
        "        try:\n",
        "            if isinstance(image, Image.Image):\n",
        "                image = np.array(image)\n",
        "            if len(image.shape) == 2:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "            explanation = self.explainer.explain_instance(\n",
        "                image,\n",
        "                self.predict_fn,\n",
        "                labels=range(len(CORNEA_CLASSES)),\n",
        "                top_labels=top_labels,\n",
        "                hide_color=0,\n",
        "                num_samples=num_samples\n",
        "            )\n",
        "            return explanation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in explain_instance: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "def visualize_results(explanation, image, class_names=CORNEA_CLASSES, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize LIME explanation results overlaid on the original image\n",
        "\n",
        "    Args:\n",
        "        explanation: LIME explanation object\n",
        "        image: Original image array\n",
        "        class_names: List of class names\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get predictions\n",
        "        prediction = explanation.predict_fn(np.array([image]))[0]\n",
        "\n",
        "        # Sort labels by prediction confidence\n",
        "        sorted_labels = sorted(range(len(prediction)),\n",
        "                             key=lambda x: prediction[x],\n",
        "                             reverse=True)[:2]\n",
        "\n",
        "        # Create a figure with subplots\n",
        "        plt.figure(figsize=(15, 7))\n",
        "\n",
        "        for idx, label in enumerate(sorted_labels):\n",
        "            plt.subplot(1, len(sorted_labels), idx + 1)\n",
        "\n",
        "            # Get the original image and mask from LIME\n",
        "            mask = explanation.local_exp[label]\n",
        "\n",
        "            # Convert the sparse mask to a dense array\n",
        "            dense_mask = np.zeros(explanation.segments.shape, dtype=float)\n",
        "            for i, v in mask:\n",
        "                dense_mask[explanation.segments == i] = v\n",
        "\n",
        "            # Normalize the mask to [0, 1] range\n",
        "            if dense_mask.max() != dense_mask.min():\n",
        "                dense_mask = (dense_mask - dense_mask.min()) / (dense_mask.max() - dense_mask.min())\n",
        "\n",
        "            # Create a colormap (red for positive contributions)\n",
        "            heatmap = np.zeros((dense_mask.shape[0], dense_mask.shape[1], 4))\n",
        "            heatmap[:, :, 2] = dense_mask  # Blue channel\n",
        "            heatmap[:, :, 3] = dense_mask * 1.0  # Alpha channel\n",
        "\n",
        "            # Display original image\n",
        "            plt.imshow(image, alpha=0.8)\n",
        "\n",
        "            # Overlay heatmap\n",
        "            plt.imshow(heatmap, alpha=0.6)\n",
        "\n",
        "            class_name = class_names[label] if label < len(class_names) else f\"Unknown Class {label}\"\n",
        "            plt.title(f'{class_name}\\nConfidence: {prediction[label]:.3f}', fontsize=12)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # if save_path:\n",
        "        #     plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "        #     print(f\"Visualization saved to {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nDetailed Confidence Scores:\")\n",
        "        for label in sorted_labels:\n",
        "            print(f\"{class_names[label]}: {prediction[label]:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in visualization: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "class YOLOLimeExplainer:\n",
        "    def __init__(self, yolo_model, device='cuda', img_size=(640, 640)):\n",
        "        self.model = yolo_model\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    def predict_fn(self, images):\n",
        "        \"\"\"\n",
        "        Modified prediction function with improved image processing\n",
        "        \"\"\"\n",
        "        try:\n",
        "            batch_predictions = []\n",
        "            for img in images:\n",
        "                # Normalize image if needed\n",
        "                if img.max() > 1.0:\n",
        "                    img = img / 255.0\n",
        "\n",
        "                # Convert image format if needed\n",
        "                if len(img.shape) == 2:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "                elif img.shape[2] == 4:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "                # Ensure image is uint8 for preprocessing\n",
        "                img_uint8 = (img * 255).astype(np.uint8)\n",
        "                processed_img = self.model.preprocessing(np.expand_dims(img_uint8, 0))\n",
        "\n",
        "                if processed_img is None:\n",
        "                    raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    predictions, logits = self.model(processed_img)\n",
        "\n",
        "                class_scores = np.zeros(len(CORNEA_CLASSES))\n",
        "\n",
        "                if predictions[0][0]:  # If there are any detections\n",
        "                    if logits is not None and len(logits[0]) > 0:\n",
        "                        # Use logits if available\n",
        "                        probs = torch.nn.functional.softmax(logits[0], dim=1)\n",
        "                        class_scores = probs.mean(dim=0).cpu().numpy()\n",
        "                    else:\n",
        "                        # Fallback to confidence scores\n",
        "                        for cls_idx, conf in zip(predictions[1][0], predictions[3][0]):\n",
        "                            if cls_idx < len(CORNEA_CLASSES):\n",
        "                                class_scores[cls_idx] = max(class_scores[cls_idx], conf)\n",
        "\n",
        "                batch_predictions.append(class_scores)\n",
        "\n",
        "            return np.array(batch_predictions)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction_fn: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return np.zeros((len(images), len(CORNEA_CLASSES)))\n",
        "\n",
        "    def explain_instance(self, image, num_samples=1000, top_labels=5):\n",
        "        \"\"\"\n",
        "        Generate LIME explanation with improved image handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert PIL Image to numpy array if needed\n",
        "            if isinstance(image, Image.Image):\n",
        "                image = np.array(image)\n",
        "\n",
        "            # Normalize image if needed\n",
        "            if image.max() > 1.0:\n",
        "                image = image.astype(float) / 255.0\n",
        "\n",
        "            # Convert image format if needed\n",
        "            if len(image.shape) == 2:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "            # Generate explanation\n",
        "            explanation = self.explainer.explain_instance(\n",
        "                image,\n",
        "                self.predict_fn,\n",
        "                labels=range(len(CORNEA_CLASSES)),\n",
        "                top_labels=top_labels,\n",
        "                hide_color=0,\n",
        "                num_samples=num_samples\n",
        "            )\n",
        "\n",
        "            # Store predict_fn for later use\n",
        "            explanation.predict_fn = self.predict_fn\n",
        "\n",
        "            return explanation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in explain_instance: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "def run_lime_analysis(model_path, img_path, num_samples=500, save_path=None):\n",
        "    \"\"\"\n",
        "    Run LIME analysis with improved visualization\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the YOLO model weights\n",
        "        img_path: Path to the input image\n",
        "        num_samples: Number of samples for LIME analysis\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        print(\"Loading YOLO model...\")\n",
        "        yolo_model = YOLOV5TorchObjectDetector(\n",
        "            model_weight=model_path,\n",
        "            device=device,\n",
        "            img_size=(640, 640),\n",
        "            names=CORNEA_CLASSES\n",
        "        )\n",
        "\n",
        "        print(\"Initializing LIME explainer...\")\n",
        "        lime_explainer = YOLOLimeExplainer(yolo_model)\n",
        "\n",
        "        print(\"Loading and processing image...\")\n",
        "        image = Image.open(img_path)\n",
        "        image_array = np.array(image)\n",
        "\n",
        "        # Normalize image if needed\n",
        "        if image_array.max() > 1.0:\n",
        "            image_array = image_array.astype(float) / 255.0\n",
        "\n",
        "        print(f\"Running LIME analysis with {num_samples} samples...\")\n",
        "        explanation = lime_explainer.explain_instance(\n",
        "            image_array,\n",
        "            num_samples=num_samples,\n",
        "            top_labels=3\n",
        "        )\n",
        "\n",
        "        if explanation is None:\n",
        "            print(\"Failed to generate explanation\")\n",
        "            return None\n",
        "\n",
        "        print(\"Visualizing results...\")\n",
        "        visualize_results(explanation, image_array, save_path=save_path)\n",
        "\n",
        "        return explanation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in run_lime_analysis: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Usage example:\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/1.jpg\"\n",
        "    save_path = \"lime_explanation.png\"  # Optional\n",
        "\n",
        "    explanation = run_lime_analysis(\n",
        "        model_path=model_path,\n",
        "        img_path=img_path,\n",
        "        num_samples=500,\n",
        "        save_path=save_path\n",
        "    )"
      ],
      "metadata": {
        "id": "4ZNSmW3HHhtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**RISE**"
      ],
      "metadata": {
        "id": "yCtdw0Rfb7v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import cv2\n",
        "import time\n",
        "import traceback\n",
        "import torchvision\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "%matplotlib inline\n",
        "\n",
        "# 角膜AIのクラス定義\n",
        "CORNEA_CLASSES = [\n",
        "    \"infection\",\n",
        "    \"normal\",\n",
        "    \"non-infection\",\n",
        "    \"scar\",\n",
        "    \"tumor\",\n",
        "    \"deposit\",\n",
        "    \"APAC\",\n",
        "    \"lens opacity\",\n",
        "    \"bullous\"\n",
        "]\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"\n",
        "    GPUが利用可能な場合はGPUを、そうでない場合はCPUを設定\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    device : torch.device\n",
        "        使用するデバイス\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"GPU not available, using CPU\")\n",
        "    return device\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size=(640, 640),\n",
        "                 names=CORNEA_CLASSES,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        # モデルのロード\n",
        "        print(\"[INFO] Loading cornea detection model...\")\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model loaded successfully\")\n",
        "\n",
        "        # モデルのクラス数を取得と確認\n",
        "        self.nc = int(self.model.nc)\n",
        "        print(f\"[INFO] Number of classes: {self.nc}\")\n",
        "\n",
        "        # クラス名の設定と検証\n",
        "        self.names = names\n",
        "        if len(self.names) != self.nc:\n",
        "            print(f\"[WARNING] Number of class names ({len(self.names)}) does not match model classes ({self.nc})\")\n",
        "        print(f\"[INFO] Using class names: {self.names}\")\n",
        "\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        # Cold start prevention\n",
        "        print(\"[INFO] Performing cold start prevention...\")\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "        print(\"[INFO] Initialization complete\")\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096\n",
        "        max_nms = 30000\n",
        "        time_limit = 10.0\n",
        "        redundant = True\n",
        "        multi_label &= nc > 1\n",
        "        merge = False\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        try:\n",
        "            prediction, logits, _ = self.model(img, augment=False)\n",
        "            prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                          classes=None,\n",
        "                                                          agnostic=self.agnostic)\n",
        "\n",
        "            batch_size = img.shape[0]\n",
        "            self.boxes = [[] for _ in range(batch_size)]\n",
        "            self.class_names = [[] for _ in range(batch_size)]\n",
        "            self.classes = [[] for _ in range(batch_size)]\n",
        "            self.confidences = [[] for _ in range(batch_size)]\n",
        "\n",
        "            for i, det in enumerate(prediction):\n",
        "                if len(det):\n",
        "                    for *xyxy, conf, cls in det:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(self.img_size[1], xyxy[2])\n",
        "                        xyxy[3] = min(self.img_size[0], xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(float(conf.item()))\n",
        "                        cls_idx = int(cls.item())\n",
        "\n",
        "                        if cls_idx >= len(self.names):\n",
        "                            print(f\"[WARNING] Class index {cls_idx} is out of range\")\n",
        "                            cls_idx = 0\n",
        "\n",
        "                        self.classes[i].append(cls_idx)\n",
        "                        self.class_names[i].append(self.names[cls_idx])\n",
        "\n",
        "            return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in forward pass: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return [[[]], [[]], [[]], [[]]], None\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        try:\n",
        "            if len(img.shape) != 4:\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "            im0 = img.astype(np.uint8)\n",
        "            img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "            img = img.transpose((0, 3, 1, 2))\n",
        "            img = np.ascontiguousarray(img)\n",
        "            img = torch.from_numpy(img).to(self.device)\n",
        "            img = img / 255.0\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preprocessing: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None"
      ],
      "metadata": {
        "id": "W5si01fcjXSt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "\n",
        "class YOLORISE:\n",
        "    def __init__(self, model, input_size=(640, 640), gpu_batch=32):\n",
        "        self.model = model\n",
        "        self.input_size = input_size\n",
        "        self.gpu_batch = gpu_batch\n",
        "        self.N = None\n",
        "        self.masks = None\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def generate_masks(self, N=6000, s=8, p1=0.1, savepath=None, target_shape=None):\n",
        "        \"\"\"Generate random binary masks for RISE\"\"\"\n",
        "        if target_shape is None:\n",
        "            target_shape = self.input_size\n",
        "\n",
        "        cell_height = int(np.ceil(target_shape[0] / s))\n",
        "        cell_width = int(np.ceil(target_shape[1] / s))\n",
        "\n",
        "        self.N = N\n",
        "        self.masks = np.zeros((N, target_shape[0], target_shape[1]), dtype=np.float32)\n",
        "\n",
        "        print(f\"Generating {N} masks for shape {target_shape} with cell size {cell_height}x{cell_width}...\")\n",
        "        for i in tqdm(range(N), desc=\"Generating masks\"):\n",
        "            small_mask = np.random.choice(\n",
        "                [0, 1],\n",
        "                size=(cell_height, cell_width),\n",
        "                p=[1-p1, p1]\n",
        "            ).astype(np.float32)\n",
        "\n",
        "            small_mask_3ch = np.repeat(small_mask[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "            mask = cv2.resize(\n",
        "                small_mask_3ch,\n",
        "                (target_shape[1], target_shape[0]),\n",
        "                interpolation=cv2.INTER_LINEAR\n",
        "            )\n",
        "\n",
        "            self.masks[i] = (mask[:, :, 0] > 0.5).astype(np.float32)\n",
        "\n",
        "        if savepath:\n",
        "            np.save(savepath, self.masks)\n",
        "            print(f\"Masks saved to {savepath} with shape {self.masks.shape}\")\n",
        "\n",
        "    def load_masks(self, path, target_shape=None):\n",
        "        \"\"\"Load pre-generated masks with validation\"\"\"\n",
        "        try:\n",
        "            loaded_masks = np.load(path)\n",
        "\n",
        "            if target_shape is None:\n",
        "                target_shape = self.input_size\n",
        "\n",
        "            expected_shape = (self.N if self.N is not None else 6000, target_shape[0], target_shape[1])\n",
        "\n",
        "            if loaded_masks.shape != expected_shape:\n",
        "                print(f\"Warning: Loaded masks shape {loaded_masks.shape} differs from expected {expected_shape}\")\n",
        "                print(\"Regenerating masks...\")\n",
        "                self.generate_masks(N=expected_shape[0], savepath=path, target_shape=target_shape)\n",
        "                return\n",
        "\n",
        "            self.masks = loaded_masks\n",
        "            self.N = loaded_masks.shape[0]\n",
        "            print(f\"Loaded {self.N} masks with shape {self.masks.shape}\")\n",
        "\n",
        "        except (ValueError, IOError) as e:\n",
        "            print(f\"Error loading masks: {e}\")\n",
        "            print(\"Regenerating masks...\")\n",
        "            self.generate_masks(N=6000 if self.N is None else self.N, savepath=path, target_shape=target_shape)\n",
        "\n",
        "    def __call__(self, images):\n",
        "        \"\"\"\n",
        "        Generate saliency maps for YOLOv5 predictions\n",
        "\n",
        "        Args:\n",
        "            images: Input images tensor [B, C, H, W]\n",
        "        \"\"\"\n",
        "        if self.masks is None:\n",
        "            raise ValueError(\"Masks not generated. Call generate_masks() first.\")\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "        input_shape = (images.shape[2], images.shape[3])\n",
        "        print(f\"Processing batch of {batch_size} images with shape {images.shape}\")\n",
        "\n",
        "        # Regenerate masks if input shape doesn't match\n",
        "        if self.masks.shape[1:] != input_shape:\n",
        "            print(f\"Regenerating masks for input shape {input_shape}\")\n",
        "            mask_path = os.path.join('.', f'rise_masks_{input_shape[0]}x{input_shape[1]}.npy')\n",
        "            self.generate_masks(N=self.N if self.N is not None else 6000, target_shape=input_shape, savepath=mask_path)\n",
        "\n",
        "        # Get baseline predictions\n",
        "        with torch.no_grad():\n",
        "            baseline_preds, baseline_logits = self.model(images)\n",
        "\n",
        "        # Initialize saliency maps\n",
        "        class_saliency = torch.zeros(\n",
        "            (batch_size, len(self.model.names), *input_shape),\n",
        "            device=self.device\n",
        "        )\n",
        "        box_saliency = torch.zeros(\n",
        "            (batch_size, *input_shape),\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Convert masks to tensor\n",
        "        masks_tensor = torch.from_numpy(self.masks).float().to(self.device)\n",
        "\n",
        "        # Process masks in batches\n",
        "        for i in range(0, self.N, self.gpu_batch):\n",
        "            batch_end = min(i + self.gpu_batch, self.N)\n",
        "            batch_masks = masks_tensor[i:batch_end]\n",
        "\n",
        "            # Apply masks to images\n",
        "            masked_batch = []\n",
        "            for img in images:\n",
        "                masked_images = img.unsqueeze(0) * batch_masks.view(-1, 1, *input_shape)\n",
        "                masked_batch.append(masked_images)\n",
        "\n",
        "            masked_batch = torch.cat(masked_batch, dim=0)\n",
        "\n",
        "            # Get predictions for masked images\n",
        "            with torch.no_grad():\n",
        "                predictions, logits = self.model(masked_batch)\n",
        "\n",
        "            # Process each image in the batch\n",
        "            for b in range(batch_size):\n",
        "                # Get predictions for current image\n",
        "                base_boxes = baseline_preds[0][b]\n",
        "                base_classes = baseline_preds[1][b]\n",
        "                base_scores = baseline_preds[3][b]\n",
        "\n",
        "                if len(base_classes) > 0:\n",
        "                    # Update class saliency\n",
        "                    for cls, score in zip(base_classes, base_scores):\n",
        "                        cls_idx = int(cls)\n",
        "                        if cls_idx < len(self.model.names):\n",
        "                            # マスクの形状を明示的に合わせる\n",
        "                            for mask_idx in range(len(batch_masks)):\n",
        "                                class_saliency[b, cls_idx] += batch_masks[mask_idx] * float(score)\n",
        "\n",
        "                    # Update box saliency\n",
        "                    for box in base_boxes:\n",
        "                        score = self._calculate_box_score(box)\n",
        "                        # マスクの形状を明示的に合わせる\n",
        "                        for mask_idx in range(len(batch_masks)):\n",
        "                            box_saliency[b] += batch_masks[mask_idx] * score\n",
        "\n",
        "        # Normalize saliency maps\n",
        "        class_saliency = self._normalize_saliency(class_saliency)\n",
        "        box_saliency = self._normalize_saliency(box_saliency)\n",
        "\n",
        "        return {\n",
        "            'class_saliency': class_saliency,\n",
        "            'box_saliency': box_saliency\n",
        "        }\n",
        "\n",
        "    def _calculate_box_score(self, box):\n",
        "        \"\"\"Calculate score for bounding box based on area\"\"\"\n",
        "        x1, y1, x2, y2 = box\n",
        "        area = (x2 - x1) * (y2 - y1)\n",
        "        return float(area) / (self.input_size[0] * self.input_size[1])\n",
        "\n",
        "    def _normalize_saliency(self, saliency):\n",
        "        \"\"\"Normalize saliency maps to [0, 1] range\"\"\"\n",
        "        saliency_min = torch.min(saliency)\n",
        "        saliency_max = torch.max(saliency)\n",
        "\n",
        "        if saliency_max > saliency_min:\n",
        "            saliency = (saliency - saliency_min) / (saliency_max - saliency_min)\n",
        "\n",
        "        return saliency\n",
        "\n",
        "def apply_heatmap(image, saliency_map, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Apply saliency map as a heatmap overlay on the original image\n",
        "\n",
        "    Args:\n",
        "        image: Original image (H, W, 3) in RGB format\n",
        "        saliency_map: Tensor containing saliency values (H, W)\n",
        "        alpha: Transparency of the heatmap overlay (0-1)\n",
        "\n",
        "    Returns:\n",
        "        overlayed_image: Image with heatmap overlay\n",
        "    \"\"\"\n",
        "    # Convert saliency map to numpy if it's a tensor\n",
        "    if torch.is_tensor(saliency_map):\n",
        "        saliency_map = saliency_map.cpu().numpy()\n",
        "\n",
        "    # Normalize saliency map to 0-1\n",
        "    saliency_norm = (saliency_map - np.min(saliency_map)) / (np.max(saliency_map) - np.min(saliency_map))\n",
        "\n",
        "    # Convert to uint8 for cv2\n",
        "    heatmap = (saliency_norm * 255).astype(np.uint8)\n",
        "\n",
        "    # Apply colormap\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    # Convert to RGB\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize heatmap if sizes don't match\n",
        "    if image.shape[:2] != heatmap.shape[:2]:\n",
        "        heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "\n",
        "    # Overlay heatmap on original image\n",
        "    overlayed = cv2.addWeighted(image, 1-alpha, heatmap, alpha, 0)\n",
        "\n",
        "    return overlayed\n",
        "\n",
        "def visualize_results(image, results, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize RISE analysis results with heatmaps\n",
        "\n",
        "    Args:\n",
        "        image: Original image array\n",
        "        results: Dictionary containing RISE analysis results\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    fig.suptitle('RISE Analysis Results', fontsize=16)\n",
        "\n",
        "    # Plot original image with detections\n",
        "    axes[0, 0].imshow(image)\n",
        "    axes[0, 0].set_title('Original Image with Detections')\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    for box, cls_name, conf in zip(results['boxes'], results['class_names'], results['confidences']):\n",
        "        x1, y1, x2, y2 = box\n",
        "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, color='red', linewidth=2)\n",
        "        axes[0, 0].add_patch(rect)\n",
        "        axes[0, 0].text(\n",
        "            x1, y1-5,\n",
        "            f'{cls_name}: {conf:.2f}',\n",
        "            color='red',\n",
        "            fontsize=8,\n",
        "            bbox=dict(facecolor='white', alpha=0.7)\n",
        "        )\n",
        "\n",
        "    # Plot box saliency heatmap\n",
        "    box_saliency = results['saliency_maps']['box_saliency'][0].cpu()\n",
        "    box_heatmap = apply_heatmap(image, box_saliency)\n",
        "    axes[0, 1].imshow(box_heatmap)\n",
        "    axes[0, 1].set_title('Box Saliency Heatmap')\n",
        "\n",
        "    # Plot combined class saliency heatmap\n",
        "    class_saliency = results['saliency_maps']['class_saliency'][0]\n",
        "    combined_saliency = class_saliency.mean(dim=0).cpu()\n",
        "    combined_heatmap = apply_heatmap(image, combined_saliency)\n",
        "    axes[0, 2].imshow(combined_heatmap)\n",
        "    axes[0, 2].set_title('Combined Class Saliency Heatmap')\n",
        "\n",
        "    # Plot top 3 class-specific saliency heatmaps\n",
        "    top_classes = torch.topk(torch.tensor(results['confidences']), k=min(3, len(results['confidences'])))\n",
        "\n",
        "    for idx, (cls_idx, conf) in enumerate(zip(top_classes.indices, top_classes.values)):\n",
        "        class_heatmap = apply_heatmap(image, class_saliency[cls_idx].cpu())\n",
        "        axes[1, idx].imshow(class_heatmap)\n",
        "        axes[1, idx].set_title(f'Class Saliency: {results[\"class_names\"][idx]}\\nConf: {conf:.2f}')\n",
        "\n",
        "    # Turn off axes\n",
        "    for ax in axes.flat:\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Adjust layout and save\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "def display_inference_results(image, predictions, class_names, save_path=None):\n",
        "    \"\"\"\n",
        "    Display inference results on the image\n",
        "\n",
        "    Args:\n",
        "        image: Original image array\n",
        "        predictions: List containing [boxes, classes, class_names, confidences]\n",
        "        class_names: List of class names\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(image)\n",
        "\n",
        "    # Draw boxes and labels\n",
        "    boxes, classes, pred_class_names, confidences = predictions\n",
        "\n",
        "    for box, cls_name, conf in zip(boxes[0], pred_class_names[0], confidences[0]):\n",
        "        x1, y1, x2, y2 = box\n",
        "        plt.gca().add_patch(plt.Rectangle(\n",
        "            (x1, y1), x2-x1, y2-y1,\n",
        "            fill=False, color='red', linewidth=2\n",
        "        ))\n",
        "        plt.text(\n",
        "            x1, y1-5,\n",
        "            f'{cls_name}: {conf:.2f}',\n",
        "            color='red',\n",
        "            fontsize=10,\n",
        "            bbox=dict(facecolor='white', alpha=0.7)\n",
        "        )\n",
        "\n",
        "    plt.title('YOLOv5 Detection Results')\n",
        "    plt.axis('off')\n",
        "\n",
        "    if save_path:\n",
        "        inference_path = os.path.splitext(save_path)[0] + '_inference.png'\n",
        "        plt.savefig(inference_path, bbox_inches='tight', dpi=300)\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def run_rise_analysis(model_path, img_path, num_masks=6000, mask_size=8, mask_prob=0.1, save_path=None):\n",
        "    \"\"\"Run RISE analysis on a YOLOv5 model and image\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        device = setup_device()\n",
        "\n",
        "        # Load YOLO model\n",
        "        print(\"Loading YOLOv5 model...\")\n",
        "        yolo_model = YOLOV5TorchObjectDetector(\n",
        "            model_weight=model_path,\n",
        "            device=device,\n",
        "            img_size=(640, 640),\n",
        "            names=CORNEA_CLASSES\n",
        "        )\n",
        "\n",
        "        # Initialize RISE explainer\n",
        "        print(\"Initializing RISE explainer...\")\n",
        "        rise_explainer = YOLORISE(yolo_model, input_size=(640, 640))\n",
        "\n",
        "        # Generate or load masks\n",
        "        mask_path = os.path.join(os.path.dirname(save_path) if save_path else '.', 'rise_masks.npy')\n",
        "\n",
        "        if os.path.exists(mask_path):\n",
        "            print(\"Loading pre-generated masks...\")\n",
        "            rise_explainer.load_masks(mask_path)\n",
        "        else:\n",
        "            print(f\"Generating {num_masks} masks...\")\n",
        "            rise_explainer.generate_masks(N=num_masks, s=mask_size, p1=mask_prob, savepath=mask_path)\n",
        "\n",
        "        # Load and preprocess image\n",
        "        print(\"Loading and preprocessing image...\")\n",
        "        original_image = Image.open(img_path)\n",
        "        image_array = np.array(original_image)\n",
        "\n",
        "        if len(image_array.shape) == 2:\n",
        "            image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)\n",
        "        elif image_array.shape[2] == 4:\n",
        "            image_array = cv2.cvtColor(image_array, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "        processed_image = yolo_model.preprocessing(np.expand_dims(image_array, 0))\n",
        "\n",
        "        if processed_image is None:\n",
        "            raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "        print(f\"Input image shape: {processed_image.shape}\")\n",
        "\n",
        "        # Get baseline predictions\n",
        "        print(\"Running inference...\")\n",
        "        with torch.no_grad():\n",
        "            predictions, logits = yolo_model(processed_image)\n",
        "\n",
        "        # Display inference results\n",
        "        print(\"Displaying inference results...\")\n",
        "        display_inference_results(image_array, predictions, CORNEA_CLASSES, save_path)\n",
        "\n",
        "        # Run RISE analysis\n",
        "        print(\"Running RISE analysis...\")\n",
        "        saliency_maps = rise_explainer(processed_image)\n",
        "\n",
        "        results = {\n",
        "            'boxes': predictions[0][0],\n",
        "            'classes': predictions[1][0],\n",
        "            'class_names': predictions[2][0],\n",
        "            'confidences': predictions[3][0],\n",
        "            'saliency_maps': saliency_maps,\n",
        "            'processing_time': time.time() - start_time\n",
        "        }\n",
        "\n",
        "        if save_path:\n",
        "            print(\"Generating visualizations...\")\n",
        "            # Generate original visualization\n",
        "            _generate_detailed_visualization(\n",
        "                image_array,\n",
        "                results,\n",
        "                saliency_maps,\n",
        "                class_names=CORNEA_CLASSES,\n",
        "                save_path=save_path\n",
        "            )\n",
        "\n",
        "            # Generate heatmap visualization\n",
        "            heatmap_path = os.path.splitext(save_path)[0] + '_heatmap.png'\n",
        "            visualize_results(image_array, results, save_path=heatmap_path)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in RISE analysis: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def _generate_detailed_visualization(image, results, saliency_maps, class_names, save_path):\n",
        "    \"\"\"\n",
        "    Generate detailed visualization of RISE analysis results with proper cleanup\n",
        "    \"\"\"\n",
        "    try:\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # Plot original image with detections\n",
        "        plt.subplot(231)\n",
        "        plt.title('Original Image with Detections')\n",
        "        plt.imshow(image)\n",
        "\n",
        "        # Draw bounding boxes\n",
        "        for box, cls_name, conf in zip(results['boxes'], results['class_names'], results['confidences']):\n",
        "            x1, y1, x2, y2 = box\n",
        "            plt.gca().add_patch(plt.Rectangle(\n",
        "                (x1, y1), x2-x1, y2-y1,\n",
        "                fill=False, color='red', linewidth=2\n",
        "            ))\n",
        "            plt.text(\n",
        "                x1, y1-5,\n",
        "                f'{cls_name}: {conf:.2f}',\n",
        "                color='red',\n",
        "                fontsize=8,\n",
        "                bbox=dict(facecolor='white', alpha=0.7)\n",
        "            )\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Plot class saliency for top classes\n",
        "        class_saliency = saliency_maps['class_saliency']\n",
        "        top_classes = torch.topk(torch.tensor(results['confidences']), k=min(3, len(results['confidences'])))\n",
        "\n",
        "        for idx, (cls_idx, conf) in enumerate(zip(top_classes.indices, top_classes.values)):\n",
        "            plt.subplot(2, 3, idx+2)\n",
        "            plt.title(f'Class Saliency: {class_names[cls_idx]}\\nConf: {conf:.2f}')\n",
        "            plt.imshow(image, alpha=0.6)\n",
        "            plt.imshow(\n",
        "                class_saliency[0, cls_idx].cpu(),\n",
        "                cmap='jet',\n",
        "                alpha=0.4\n",
        "            )\n",
        "            plt.axis('off')\n",
        "\n",
        "        # Plot box saliency\n",
        "        plt.subplot(234)\n",
        "        plt.title('Box Saliency')\n",
        "        plt.imshow(image, alpha=0.6)\n",
        "        plt.imshow(\n",
        "            saliency_maps['box_saliency'][0].cpu(),\n",
        "            cmap='jet',\n",
        "            alpha=0.4\n",
        "        )\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Plot combined saliency\n",
        "        plt.subplot(235)\n",
        "        plt.title('Combined Saliency')\n",
        "        combined_saliency = (\n",
        "            saliency_maps['box_saliency'][0] +\n",
        "            class_saliency[0].mean(dim=0)\n",
        "        ) / 2\n",
        "        plt.imshow(image, alpha=0.6)\n",
        "        plt.imshow(\n",
        "            combined_saliency.cpu(),\n",
        "            cmap='jet',\n",
        "            alpha=0.4\n",
        "        )\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Add processing information\n",
        "        plt.subplot(236)\n",
        "        plt.axis('off')\n",
        "        info_text = f\"\"\"Processing Information:\n",
        "\n",
        "    Total Time: {results['processing_time']:.2f}s\n",
        "    Number of Detections: {len(results['boxes'])}\n",
        "\n",
        "    Top Detections:\n",
        "    \"\"\"\n",
        "        for cls_name, conf in zip(results['class_names'], results['confidences']):\n",
        "            info_text += f\"- {cls_name}: {conf:.2f}\\n\"\n",
        "\n",
        "        plt.text(0.1, 0.5, info_text, fontsize=10, va='center')\n",
        "\n",
        "        # Save and cleanup\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close(fig)\n",
        "        plt.close('all')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in visualization: {e}\")\n",
        "        plt.close('all')\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "def batch_process_images(model_path, image_dir, output_dir, batch_size=1, **rise_params):\n",
        "    \"\"\"\n",
        "    Process multiple images with RISE analysis\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to YOLOv5 model weights\n",
        "        image_dir: Directory containing images\n",
        "        output_dir: Directory to save results\n",
        "        batch_size: Number of images to process in parallel\n",
        "        rise_params: Additional parameters for RISE analysis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Get list of images\n",
        "        image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        # Process images\n",
        "        for i in tqdm(range(0, len(image_files), batch_size), desc=\"Processing images\"):\n",
        "            batch_files = image_files[i:i+batch_size]\n",
        "\n",
        "            for img_file in batch_files:\n",
        "                img_path = os.path.join(image_dir, img_file)\n",
        "                save_path = os.path.join(output_dir, f'rise_{os.path.splitext(img_file)[0]}.png')\n",
        "\n",
        "                # Run RISE analysis\n",
        "                results = run_rise_analysis(\n",
        "                    model_path=model_path,\n",
        "                    img_path=img_path,\n",
        "                    save_path=save_path,\n",
        "                    **rise_params\n",
        "                )\n",
        "\n",
        "                if results is None:\n",
        "                    print(f\"Failed to process {img_file}\")\n",
        "                    continue\n",
        "\n",
        "                # Save raw results if needed\n",
        "                results_path = os.path.join(output_dir, f'rise_{os.path.splitext(img_file)[0]}_results.npz')\n",
        "                np.savez_compressed(\n",
        "                    results_path,\n",
        "                    boxes=results['boxes'],\n",
        "                    classes=results['classes'],\n",
        "                    confidences=results['confidences'],\n",
        "                    class_saliency=results['saliency_maps']['class_saliency'].cpu().numpy(),\n",
        "                    box_saliency=results['saliency_maps']['box_saliency'].cpu().numpy()\n",
        "                )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in batch processing: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/1.jpg\"\n",
        "    save_path = \"rise_analysis.png\"\n",
        "\n",
        "    results = run_rise_analysis(\n",
        "        model_path=model_path,\n",
        "        img_path=img_path,\n",
        "        save_path=save_path,\n",
        "        num_masks=6000,\n",
        "        mask_size=8,\n",
        "        mask_prob=0.1\n",
        "    )\n",
        "\n",
        "\n",
        "    # # Batch processing\n",
        "    # batch_process_images(\n",
        "    #     model_path=model_path,\n",
        "    #     image_dir=\"path/to/image/directory\",\n",
        "    #     output_dir=\"path/to/output/directory\",\n",
        "    #     batch_size=1,\n",
        "    #     num_masks=6000,\n",
        "    #     mask_size=8,\n",
        "    #     mask_prob=0.1\n",
        "    # )\n",
        "\n"
      ],
      "metadata": {
        "id": "Fqwpw3L8E4YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "import traceback\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "\n",
        "# YOLOv5のクラス定義\n",
        "CORNEA_CLASSES = [\n",
        "    \"infection\",\n",
        "    \"normal\",\n",
        "    \"non-infection\",\n",
        "    \"scar\",\n",
        "    \"tumor\",\n",
        "    \"deposit\",\n",
        "    \"APAC\",\n",
        "    \"lens opacity\",\n",
        "    \"bullous\"\n",
        "]\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"GPUが利用可能な場合はGPUを、そうでない場合はCPUを設定\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"GPU not available, using CPU\")\n",
        "    return device\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size=(640, 640),\n",
        "                 names=CORNEA_CLASSES,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45):\n",
        "        \"\"\"YOLOv5モデルの初期化\"\"\"\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.names = names\n",
        "\n",
        "        # モデルのロード\n",
        "        print(\"[INFO] Loading model...\")\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model loaded successfully\")\n",
        "\n",
        "        # モデルのクラス数を取得と確認\n",
        "        self.nc = int(self.model.nc)\n",
        "        print(f\"[INFO] Number of classes: {self.nc}\")\n",
        "\n",
        "        if len(self.names) != self.nc:\n",
        "            print(f\"[WARNING] Number of class names ({len(self.names)}) does not match model classes ({self.nc})\")\n",
        "        print(f\"[INFO] Using class names: {self.names}\")\n",
        "\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'eval':\n",
        "            self.model.eval()\n",
        "\n",
        "        # Cold start prevention\n",
        "        print(\"[INFO] Performing cold start prevention...\")\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "        print(\"[INFO] Initialization complete\")\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        \"\"\"画像の前処理\"\"\"\n",
        "        try:\n",
        "            if len(img.shape) != 4:\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "            im0 = img.astype(np.uint8)\n",
        "            img = np.array([letterbox(im, new_shape=self.img_size)[0] for im in im0])\n",
        "            img = img.transpose((0, 3, 1, 2))\n",
        "            img = np.ascontiguousarray(img)\n",
        "            img = torch.from_numpy(img).to(self.device)\n",
        "            img = img / 255.0\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preprocessing: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"モデルの推論実行\"\"\"\n",
        "        try:\n",
        "            prediction = self.model(img)[0]\n",
        "\n",
        "            # NMSの適用\n",
        "            prediction = non_max_suppression(\n",
        "                prediction,\n",
        "                conf_thres=self.confidence,\n",
        "                iou_thres=self.iou_thresh\n",
        "            )\n",
        "\n",
        "            batch_size = img.shape[0]\n",
        "            self.boxes = [[] for _ in range(batch_size)]\n",
        "            self.class_names = [[] for _ in range(batch_size)]\n",
        "            self.classes = [[] for _ in range(batch_size)]\n",
        "            self.confidences = [[] for _ in range(batch_size)]\n",
        "\n",
        "            for i, det in enumerate(prediction):\n",
        "                if len(det):\n",
        "                    # 信頼度でソート\n",
        "                    det = det[det[:, 4].argsort(descending=True)]\n",
        "\n",
        "                    for *xyxy, conf, cls in det:\n",
        "                        # バウンディングボックスの座標を画像サイズ内に制限\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(self.img_size[1], xyxy[2])\n",
        "                        xyxy[3] = min(self.img_size[0], xyxy[3])\n",
        "\n",
        "                        self.boxes[i].append(xyxy)\n",
        "                        self.confidences[i].append(float(conf.item()))\n",
        "                        cls_idx = int(cls.item())\n",
        "\n",
        "                        if cls_idx >= len(self.names):\n",
        "                            print(f\"[WARNING] Class index {cls_idx} is out of range\")\n",
        "                            cls_idx = 0\n",
        "\n",
        "                        self.classes[i].append(cls_idx)\n",
        "                        self.class_names[i].append(self.names[cls_idx])\n",
        "\n",
        "            return [self.boxes, self.classes, self.class_names, self.confidences]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in forward pass: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return [[[]], [[]], [[]], [[]]]\n",
        "\n",
        "class YOLORISE:\n",
        "    def __init__(self, model, N=6000, s=8, p1=0.1, batch_size=32):\n",
        "        \"\"\"YOLORISEの初期化\"\"\"\n",
        "        self.model = model\n",
        "        self.N = N\n",
        "        self.s = s\n",
        "        self.p1 = p1\n",
        "        self.batch_size = batch_size\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.masks = None\n",
        "        # input_sizeはイニシャライザから削除し、実際の入力画像のサイズに基づいて決定する\n",
        "\n",
        "    def generate_masks(self, savepath=None, image_size=(640, 640)):\n",
        "        \"\"\"RISEマスクの生成\"\"\"\n",
        "        print(f\"Generating {self.N} masks for size {image_size}...\")\n",
        "\n",
        "        # マスクのグリッドサイズを計算\n",
        "        cell_height = int(np.ceil(image_size[0] / self.s))\n",
        "        cell_width = int(np.ceil(image_size[1] / self.s))\n",
        "\n",
        "        # マスクの生成\n",
        "        self.masks = np.zeros((self.N, image_size[0], image_size[1]), dtype=np.float32)\n",
        "\n",
        "        for i in tqdm(range(self.N), desc=\"Generating masks\"):\n",
        "            # 小さいサイズのマスクを生成\n",
        "            small_mask = np.random.choice(\n",
        "                [0, 1],\n",
        "                size=(cell_height, cell_width),\n",
        "                p=[1-self.p1, self.p1]\n",
        "            ).astype(np.float32)\n",
        "\n",
        "            # マスクをリサイズ\n",
        "            mask = cv2.resize(\n",
        "                small_mask,\n",
        "                (self.input_size[1], self.input_size[0]),\n",
        "                interpolation=cv2.INTER_LINEAR\n",
        "            )\n",
        "\n",
        "            self.masks[i] = (mask > 0.5).astype(np.float32)\n",
        "\n",
        "        if savepath:\n",
        "            np.save(savepath, self.masks)\n",
        "            print(f\"Masks saved to {savepath}\")\n",
        "\n",
        "    def load_masks(self, path):\n",
        "        \"\"\"保存されたマスクの読み込み\"\"\"\n",
        "        try:\n",
        "            self.masks = np.load(path)\n",
        "            self.N = self.masks.shape[0]\n",
        "            print(f\"Loaded {self.N} masks with shape {self.masks.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading masks: {e}\")\n",
        "            print(\"Regenerating masks...\")\n",
        "            self.generate_masks(savepath=path)\n",
        "\n",
        "    def __call__(self, image):\n",
        "        \"\"\"RISEの実行\"\"\"\n",
        "        if self.masks is None:\n",
        "            raise ValueError(\"Masks not generated. Call generate_masks() first.\")\n",
        "\n",
        "        # 入力画像のサイズを取得\n",
        "        _, _, H, W = image.shape\n",
        "\n",
        "        # マスクをリサイズ\n",
        "        if self.masks.shape[1:] != (H, W):\n",
        "            print(f\"Resizing masks from {self.masks.shape[1:]} to {(H, W)}\")\n",
        "            resized_masks = np.zeros((self.N, H, W), dtype=np.float32)\n",
        "            for i in range(self.N):\n",
        "                resized_masks[i] = cv2.resize(self.masks[i], (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "            masks_tensor = torch.from_numpy(resized_masks).float().to(self.device)\n",
        "        else:\n",
        "            masks_tensor = torch.from_numpy(self.masks).float().to(self.device)\n",
        "\n",
        "        saliency_map = torch.zeros((H, W), device=self.device)\n",
        "\n",
        "        # バッチ処理でマスクを適用\n",
        "        for i in range(0, self.N, self.batch_size):\n",
        "            batch_masks = masks_tensor[i:i+self.batch_size]\n",
        "\n",
        "            # マスクを3チャンネルに拡張\n",
        "            batch_masks = batch_masks.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "\n",
        "            # マスクを適用した画像の生成\n",
        "            masked_images = image.repeat(len(batch_masks), 1, 1, 1) * batch_masks\n",
        "\n",
        "            # 推論実行\n",
        "            with torch.no_grad():\n",
        "                predictions = self.model(masked_images)\n",
        "\n",
        "            # Top1の信頼度を取得してサリエンシーマップを更新\n",
        "            for j, preds in enumerate(predictions[3]):  # predictions[3]は信頼度のリスト\n",
        "                if preds:  # 検出結果がある場合\n",
        "                    top1_conf = preds[0]  # 最も信頼度の高い検出のスコア\n",
        "                    saliency_map += batch_masks[j, 0] * top1_conf\n",
        "\n",
        "        # サリエンシーマップの正規化\n",
        "        saliency_map /= N\n",
        "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min() + 1e-8)\n",
        "\n",
        "        return saliency_map.cpu().numpy()\n",
        "\n",
        "def apply_heatmap(image, saliency_map, alpha=0.4):\n",
        "    \"\"\"サリエンシーマップのヒートマップ表示\"\"\"\n",
        "    # サリエンシーマップの正規化\n",
        "    saliency_norm = (saliency_map - np.min(saliency_map)) / (np.max(saliency_map) - np.min(saliency_map))\n",
        "\n",
        "    # ヒートマップの生成\n",
        "    heatmap = (saliency_norm * 255).astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # サイズの調整\n",
        "    if image.shape[:2] != heatmap.shape[:2]:\n",
        "        heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "\n",
        "    # オーバーレイ\n",
        "    overlayed = cv2.addWeighted(image, 1-alpha, heatmap, alpha, 0)\n",
        "    return overlayed\n",
        "\n",
        "def run_rise_analysis(model_path, img_path, num_masks=6000, mask_size=8, mask_prob=0.1, save_path=None):\n",
        "    \"\"\"RISE分析の実行\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        device = setup_device()\n",
        "\n",
        "        # YOLOv5モデルのロード\n",
        "        print(\"Loading YOLOv5 model...\")\n",
        "        yolo_model = YOLOV5TorchObjectDetector(\n",
        "            model_weight=model_path,\n",
        "            device=device,\n",
        "            img_size=(640, 640)\n",
        "        )\n",
        "\n",
        "        # 画像の読み込みと前処理\n",
        "        print(\"Loading and preprocessing image...\")\n",
        "        original_image = Image.open(img_path)\n",
        "        image_array = np.array(original_image)\n",
        "        processed_image = yolo_model.preprocessing(np.expand_dims(image_array, 0))\n",
        "\n",
        "        if processed_image is None:\n",
        "            raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "        # RISEの初期化（画像サイズを取得後）\n",
        "        print(\"Initializing RISE...\")\n",
        "        rise_explainer = YOLORISE(yolo_model)\n",
        "\n",
        "        # マスクの生成またはロード\n",
        "        image_h, image_w = processed_image.shape[2:4]\n",
        "        mask_path = os.path.join(os.path.dirname(save_path) if save_path else '.', f'rise_masks_{image_h}x{image_w}.npy')\n",
        "\n",
        "        if os.path.exists(mask_path):\n",
        "            print(\"Loading pre-generated masks...\")\n",
        "            rise_explainer.load_masks(mask_path)\n",
        "        else:\n",
        "            print(f\"Generating {num_masks} masks...\")\n",
        "            rise_explainer.generate_masks(savepath=mask_path, image_size=(image_h, image_w))\n",
        "\n",
        "        # 画像の読み込みと前処理\n",
        "        print(\"Loading and preprocessing image...\")\n",
        "        original_image = Image.open(img_path)\n",
        "        image_array = np.array(original_image)\n",
        "        processed_image = yolo_model.preprocessing(np.expand_dims(image_array, 0))\n",
        "\n",
        "        if processed_image is None:\n",
        "            raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "        # RISEの実行\n",
        "        print(\"Running RISE analysis...\")\n",
        "        saliency_map = rise_explainer(processed_image)\n",
        "\n",
        "        # ヒートマップの生成と保存\n",
        "        if save_path:\n",
        "            print(\"Generating visualization...\")\n",
        "            heatmap = apply_heatmap(image_array, saliency_map)\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(heatmap, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        print(f\"Total processing time: {time.time() - start_time:.2f}s\")\n",
        "        return saliency_map\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in RISE analysis: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "# 使用例\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/1.jpg\"\n",
        "    save_path = \"rise_analysis.png\"\n",
        "\n",
        "    saliency_map = run_rise_analysis(\n",
        "        model_path=model_path,\n",
        "        img_path=img_path,\n",
        "        save_path=save_path,\n",
        "        num_masks=6000,\n",
        "        mask_size=8,\n",
        "        mask_prob=0.1\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9GJ5LN1nK47",
        "outputId": "d1e87870-a365-4b25-bf54-16cdf1d9384c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5-gradcam/models/experimental.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(attempt_download(w), map_location=device)\n",
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "Available GPU memory: 14.75 GB\n",
            "Loading YOLOv5 model...\n",
            "[INFO] Loading model...\n",
            "[INFO] Model loaded successfully\n",
            "[INFO] Number of classes: 9\n",
            "[INFO] Using class names: ['infection', 'normal', 'non-infection', 'scar', 'tumor', 'deposit', 'APAC', 'lens opacity', 'bullous']\n",
            "[INFO] Performing cold start prevention...\n",
            "[INFO] Initialization complete\n",
            "Loading and preprocessing image...\n",
            "Initializing RISE...\n",
            "Loading pre-generated masks...\n",
            "Loaded 6000 masks with shape (6000, 448, 640)\n",
            "Loading and preprocessing image...\n",
            "Running RISE analysis...\n",
            "Error in RISE analysis: CUDA out of memory. Tried to allocate 6.41 GiB. GPU 0 has a total capacity of 14.75 GiB of which 4.39 GiB is free. Process 52228 has 10.36 GiB memory in use. Of the allocated memory 7.00 GiB is allocated by PyTorch, and 3.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-19-c20c868d3295>\", line 312, in run_rise_analysis\n",
            "    saliency_map = rise_explainer(processed_image)\n",
            "  File \"<ipython-input-19-c20c868d3295>\", line 215, in __call__\n",
            "    masks_tensor = torch.from_numpy(self.masks).float().to(self.device)\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.41 GiB. GPU 0 has a total capacity of 14.75 GiB of which 4.39 GiB is free. Process 52228 has 10.36 GiB memory in use. Of the allocated memory 7.00 GiB is allocated by PyTorch, and 3.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WuCMceghtAEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}