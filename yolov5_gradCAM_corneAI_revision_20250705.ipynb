{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/yolov5_gradCAM_corneAI_revision_20250705.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42M6k9QpvSq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJCn-Rvesvlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cakhs2BZLRA"
      },
      "source": [
        "#**YOLOv5_GradCAM_CorneAI**\n",
        "\n",
        "è«–æ–‡revisionç”¨\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUdHWgh79uc"
      },
      "source": [
        "###**â­ï¸â­ï¸Area of interestã®è¨ˆç®—**\n",
        "\n",
        "çµæœã‚’csvã«ä¿å­˜ã™ã‚‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNjX_T_cVa-9",
        "outputId": "a96e513a-c62b-4a8e-dd06-70893a4bb7a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 134 (delta 36), reused 36 (delta 36), pack-reused 94 (from 2)\u001b[K\n",
            "Receiving objects: 100% (134/134), 5.17 MiB | 22.08 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "\n",
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Colabç’°å¢ƒã§cv2_imshowã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã«ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "# FutureWarningã‚’ä¸€æ™‚çš„ã«æŠ‘åˆ¶ï¼ˆregister_backward_hookä½¿ç”¨æ™‚ï¼‰\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5ã®å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ - attempt_loadã®å ´æ‰€ã‚’ä¿®æ­£\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # ä¿®æ­£: models.experimentalã‹ã‚‰\n",
        "\n",
        "# YOLOv5ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# ã“ã‚Œã¾ã§ã«ç™»å ´ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFã‚’è¨­å®š\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# ========== æ–°è¦è¿½åŠ ï¼šãƒ¡ãƒ¢ãƒªç®¡ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==========\n",
        "class GPUMemoryMonitor:\n",
        "    \"\"\"GPU ãƒ¡ãƒ¢ãƒªã®ç›£è¦–ã¨ç®¡ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹\"\"\"\n",
        "\n",
        "    def __init__(self, warning_threshold=0.8, critical_threshold=0.9):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            warning_threshold: è­¦å‘Šã‚’å‡ºã™ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®é–¾å€¤ (0-1)\n",
        "            critical_threshold: ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«è­¦å‘Šã‚’å‡ºã™ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®é–¾å€¤ (0-1)\n",
        "        \"\"\"\n",
        "        self.warning_threshold = warning_threshold\n",
        "        self.critical_threshold = critical_threshold\n",
        "        self.device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "        self.total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
        "\n",
        "    def get_memory_stats(self):\n",
        "        \"\"\"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’å–å¾—\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return {\n",
        "                'allocated': 0,\n",
        "                'reserved': 0,\n",
        "                'free': 0,\n",
        "                'total': 0,\n",
        "                'usage_ratio': 0\n",
        "            }\n",
        "\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "        total = self.total_memory\n",
        "        free = total - reserved\n",
        "        usage_ratio = reserved / total if total > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'allocated': allocated,\n",
        "            'reserved': reserved,\n",
        "            'free': free,\n",
        "            'total': total,\n",
        "            'usage_ratio': usage_ratio\n",
        "        }\n",
        "\n",
        "    def display_memory_status(self, prefix=\"\"):\n",
        "        \"\"\"ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’è¦–è¦šçš„ã«è¡¨ç¤º\"\"\"\n",
        "        stats = self.get_memory_stats()\n",
        "\n",
        "        # ä½¿ç”¨ç‡ã«å¿œã˜ãŸè‰²ä»˜ã‘ï¼ˆANSIã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚³ãƒ¼ãƒ‰ï¼‰\n",
        "        if stats['usage_ratio'] > self.critical_threshold:\n",
        "            color = \"\\033[91m\"  # èµ¤\n",
        "            status = \"âš ï¸  CRITICAL\"\n",
        "        elif stats['usage_ratio'] > self.warning_threshold:\n",
        "            color = \"\\033[93m\"  # é»„\n",
        "            status = \"âš ï¸  WARNING\"\n",
        "        else:\n",
        "            color = \"\\033[92m\"  # ç·‘\n",
        "            status = \"âœ… OK\"\n",
        "        reset_color = \"\\033[0m\"\n",
        "\n",
        "        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ä½œæˆ\n",
        "        bar_length = 30\n",
        "        filled_length = int(bar_length * stats['usage_ratio'])\n",
        "        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)\n",
        "\n",
        "        print(f\"\\n{prefix}GPU Memory Status ({self.device_name}) {status}\")\n",
        "        print(f\"â”œâ”€ Usage: {color}[{bar}] {stats['usage_ratio']*100:.1f}%{reset_color}\")\n",
        "        print(f\"â”œâ”€ Allocated: {stats['allocated']:.2f} GB\")\n",
        "        print(f\"â”œâ”€ Reserved:  {stats['reserved']:.2f} GB\")\n",
        "        print(f\"â”œâ”€ Free:      {stats['free']:.2f} GB\")\n",
        "        print(f\"â””â”€ Total:     {stats['total']:.2f} GB\")\n",
        "\n",
        "    def check_memory_health(self):\n",
        "        \"\"\"ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’æ¨å¥¨\"\"\"\n",
        "        stats = self.get_memory_stats()\n",
        "\n",
        "        if stats['usage_ratio'] > self.critical_threshold:\n",
        "            return 'critical', stats\n",
        "        elif stats['usage_ratio'] > self.warning_threshold:\n",
        "            return 'warning', stats\n",
        "        else:\n",
        "            return 'ok', stats\n",
        "\n",
        "    def cleanup_if_needed(self, force=False):\n",
        "        \"\"\"å¿…è¦ã«å¿œã˜ã¦ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ\"\"\"\n",
        "        health, stats = self.check_memory_health()\n",
        "\n",
        "        if health == 'critical' or force:\n",
        "            print(f\"\\nğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œä¸­... (ä½¿ç”¨ç‡: {stats['usage_ratio']*100:.1f}%)\")\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "            gc.collect()\n",
        "\n",
        "            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
        "            new_stats = self.get_memory_stats()\n",
        "            freed = stats['reserved'] - new_stats['reserved']\n",
        "            print(f\"âœ¨ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {freed:.2f} GB è§£æ”¾ã•ã‚Œã¾ã—ãŸ\")\n",
        "\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# ========== æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã¯å¤‰æ›´ãªã— ==========\n",
        "# YOLOV5TorchObjectDetectorã‚¯ãƒ©ã‚¹ï¼ˆå¤‰æ›´ãªã—ï¼‰\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # === å¤‰æ›´ç‚¹: ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®requires_grad_(True)ã‚’æ˜ç¤ºçš„ã«è¨­å®š ===\n",
        "        # Grad-CAMã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            print(\"[INFO] fetching names from coco file\")\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\",\n",
        "                          \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\",\n",
        "                          \"cow\",\n",
        "                          \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "                          \"frisbee\",\n",
        "                          \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "                          \"surfboard\",\n",
        "                          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "                          \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "                          \"couch\",\n",
        "                          \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "                          \"keyboard\", \"cell phone\",\n",
        "                          \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "                          \"teddy bear\",\n",
        "                          \"hair drier\", \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # === å¤‰æ›´ç‚¹: æ¨è«–éƒ¨åˆ†ã®with torch.no_grad()ã‚’å‰Šé™¤ ===\n",
        "        # Grad-CAMã®ãŸã‚ã«å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "\n",
        "        # === æ”¹å–„ç‚¹: NMSå¾Œã®å‡¦ç†ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’ä½¿ç”¨ ===\n",
        "        with torch.no_grad():\n",
        "            # ä»¥ä¸‹ã®å‡¦ç†ã¯CPUã§è¡Œã†ã“ã¨ã§GPUãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„\n",
        "            self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                            range(4)]\n",
        "            for i, det in enumerate(prediction):  # detections per image\n",
        "                if len(det):\n",
        "                    # detã‚’CPUã«ç§»å‹•ã—ã€å…ƒã®GPUãƒ†ãƒ³ã‚½ãƒ«ã¯å³åº§ã«å‰Šé™¤\n",
        "                    det_cpu = det.cpu()\n",
        "                    del det  # GPUãƒ¡ãƒ¢ãƒªã‚’å³åº§ã«è§£æ”¾\n",
        "\n",
        "                    for *xyxy, conf, cls in det_cpu:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(img_size, xyxy[2])\n",
        "                        xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(round(conf.item(), 2))\n",
        "                        cls = int(cls.item())\n",
        "                        self.classes[i].append(cls)\n",
        "                        if self.names is not None:\n",
        "                            self.class_names[i].append(self.names[cls])\n",
        "                        else:\n",
        "                            self.class_names[i].append(cls)\n",
        "\n",
        "            # predictionã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "            del prediction\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        del img  # å…ƒã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\n",
        "\n",
        "        # ãƒªã‚µã‚¤ã‚ºå‡¦ç†ã‚’åŠ¹ç‡åŒ–\n",
        "        resized_imgs = []\n",
        "        for im in im0:\n",
        "            resized_img = self.yolo_resize(im, new_shape=self.img_size)[0]\n",
        "            resized_imgs.append(resized_img)\n",
        "\n",
        "        img = np.array(resized_imgs)\n",
        "        del resized_imgs, im0  # ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\n",
        "\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# YOLOV5GradCAMã‚¯ãƒ©ã‚¹ï¼ˆå¤‰æ›´ãªã—ï¼‰\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\", debug=False):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.debug = debug  # ãƒ‡ãƒãƒƒã‚°ãƒ•ãƒ©ã‚°\n",
        "        self.layer_name = layer_name  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¿å­˜\n",
        "\n",
        "        # === æ”¹å–„ç‚¹1: ãƒ•ãƒƒã‚¯ãƒãƒ³ãƒ‰ãƒ«ã‚’ä¿å­˜ ===\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            # === å¤‰æ›´ç‚¹: detach()ã—ã¦ã‹ã‚‰clone()ã§ã‚ˆã‚Šå®‰å…¨ã« ===\n",
        "            try:\n",
        "                # detach()ã§è¨ˆç®—ã‚°ãƒ©ãƒ•ã‹ã‚‰åˆ‡ã‚Šé›¢ã—ã¦ã‹ã‚‰clone()\n",
        "                self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "                if self.debug:\n",
        "                    print(f\"[DEBUG] Successfully detached and cloned grad_output for {layer_name}\")\n",
        "            except Exception as e:\n",
        "                if self.debug:\n",
        "                    print(f\"[DEBUG] Error in backward_hook: {e}\")\n",
        "                raise\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            # === å¤‰æ›´ç‚¹: outputã‚‚detach().clone()ã—ã¦ã‚ˆã‚Šå®‰å…¨ã« ===\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        # === å¤‰æ›´ç‚¹: target_layerã®requires_gradã‚’Trueã«è¨­å®š ===\n",
        "        for param in self.target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # === æ”¹å–„ç‚¹2: ãƒ•ãƒƒã‚¯ãƒãƒ³ãƒ‰ãƒ«ã‚’ä¿å­˜ ===\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"ãƒ•ãƒƒã‚¯ã¨ãƒ¡ãƒ¢ãƒªã‚’æ˜ç¤ºçš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹\"\"\"\n",
        "        # === æ”¹å–„ç‚¹3: ãƒ•ãƒƒã‚¯ã®å‰Šé™¤ ===\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "            if self.debug:\n",
        "                print(f\"[DEBUG] Forward hook removed for {self.layer_name}\")\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "            if self.debug:\n",
        "                print(f\"[DEBUG] Backward hook removed for {self.layer_name}\")\n",
        "\n",
        "        # === æ”¹å–„ç‚¹4: è¾æ›¸ã®ã‚¯ãƒªã‚¢ ===\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "\n",
        "        # === æ”¹å–„ç‚¹5: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã¸ã®å‚ç…§ã‚’å‰Šé™¤ ===\n",
        "        self.target_layer = None\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] GradCAM cleanup completed for {self.layer_name}\")\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ\"\"\"\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] Entering GradCAM context for {self.layer_name}\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†ãƒã‚¤ãƒ³ãƒˆï¼ˆè‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰\"\"\"\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] Exiting GradCAM context for {self.layer_name}\")\n",
        "            if exc_type is not None:\n",
        "                print(f\"[DEBUG] Exception occurred: {exc_type.__name__}: {exc_val}\")\n",
        "        self.cleanup()\n",
        "        # Falseã‚’è¿”ã™ã“ã¨ã§ä¾‹å¤–ã‚’å†ç™ºç”Ÿã•ã›ã‚‹\n",
        "        return False\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ã‚‚å¿µã®ãŸã‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\"\"\"\n",
        "        try:\n",
        "            self.cleanup()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # === æ”¹å–„ç‚¹: å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«é †ä¼æ’­ã‚’å†å®Ÿè¡Œ ===\n",
        "        # ã¾ãšã€ä¸€åº¦ã ã‘ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¦top3_indicesã‚’å–å¾—\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            # top3_indicesã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "            del top3_indices\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            # EigenCAMã®å ´åˆã¯ä¸€åº¦ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§è¨ˆç®—\n",
        "            with torch.no_grad():  # EigenCAMã¯å‹¾é…ä¸è¦\n",
        "                saliency_map = self._eigencam()\n",
        "                saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            # GradCAMã¾ãŸã¯GradCAM++ã®å ´åˆã€å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«å‡¦ç†\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                # === é‡è¦ãªå¤‰æ›´: å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œ ===\n",
        "                # ãƒ¢ãƒ‡ãƒ«ã®å‹¾é…ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
        "                self.model.zero_grad()\n",
        "\n",
        "                # === è¿½åŠ : ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã‚’é¿ã‘ã‚‹ãŸã‚ã«input_imgã‚’clone ===\n",
        "                input_img_clone = input_img.clone()\n",
        "\n",
        "                # æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
        "                _, new_logits = self.model(input_img_clone)\n",
        "\n",
        "                # å³åº§ã«ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤\n",
        "                del input_img_clone\n",
        "\n",
        "                if class_idx:\n",
        "                    score = new_logits[0][0][cls]\n",
        "                else:\n",
        "                    score = new_logits[0][0].max()\n",
        "\n",
        "                # === å¤‰æ›´ç‚¹: retain_graph=Trueã‚’å‰Šé™¤ ===\n",
        "                score.backward()  # retain_graph=Trueã‚’å‰Šé™¤\n",
        "\n",
        "                # === è¿½åŠ : gradientsã¨activationsãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª ===\n",
        "                if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                    print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                    del score, new_logits  # å­˜åœ¨ã—ãªã„å ´åˆã¯ã“ã“ã§å‰Šé™¤\n",
        "                    continue\n",
        "\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # scoreã¨new_logitsã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "                del score\n",
        "                del new_logits\n",
        "\n",
        "                # saliency_mapã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    del weights  # weightsã‚’å³åº§ã«å‰Šé™¤\n",
        "\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«epsilonã‚’è¿½åŠ \n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                    # min/maxã¯ä¸è¦ãªã®ã§å‰Šé™¤\n",
        "                    del saliency_map_min, saliency_map_max\n",
        "\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "                # === è¿½åŠ : å„ã‚¯ãƒ©ã‚¹ã®å‡¦ç†å¾Œã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "                # å‹¾é…ã¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å³åº§ã«å‰Šé™¤\n",
        "                if \"value\" in self.gradients:\n",
        "                    del self.gradients[\"value\"]\n",
        "                if \"value\" in self.activations:\n",
        "                    del self.activations[\"value\"]\n",
        "\n",
        "                # ä¸è¦ãªä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "                del gradients, activations\n",
        "\n",
        "                # CUDAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        # viewæ“ä½œã¯æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆã—ãªã„ã®ã§åŠ¹ç‡çš„\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha  # alphaã‚’å‰Šé™¤\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        del cov  # å…±åˆ†æ•£è¡Œåˆ—ã‚’å‰Šé™¤\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        del eigenvalues, eigenvectors  # ä¸è¦ãªå›ºæœ‰å€¤ãƒ»å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        del activations_reshaped, leading_eigenvector  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        del eigen_cam_min, eigen_cam_max  # min/maxã‚’å‰Šé™¤\n",
        "\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# find_yolo_layeré–¢æ•°\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "# get_aoié–¢æ•°ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ç‰ˆï¼‰\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "    total_intersect_pixels = 0\n",
        "\n",
        "    with torch.no_grad():  # å…¨ä½“ã‚’å‹¾é…è¨ˆç®—ä¸è¦ã§å›²ã‚€\n",
        "        for mask in masks:\n",
        "            # ãƒã‚¹ã‚¯ã‚’CPUã§å‡¦ç†ï¼ˆGPUãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
        "            mask_cpu = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "            del mask  # å…ƒã®ãƒã‚¹ã‚¯ã¯å³åº§ã«å‰Šé™¤\n",
        "\n",
        "            mask_cpu = np.nan_to_num(mask_cpu, nan=0.0).astype(np.uint8)\n",
        "\n",
        "            # é–¾å€¤å‡¦ç†ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ–¹æ³•ï¼‰\n",
        "            binary_mask = mask_cpu >= (threshold * 255)\n",
        "            del mask_cpu  # å‡¦ç†æ¸ˆã¿ã®ãƒã‚¹ã‚¯ã¯å‰Šé™¤\n",
        "\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "            intersect_pixels = np.sum(mask_bbox)\n",
        "            total_intersect_pixels += intersect_pixels\n",
        "\n",
        "            del mask_bbox, binary_mask  # ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "        AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "    return AOI\n",
        "\n",
        "# ========== æ”¹å–„ç‰ˆï¼šcalculate_aoié–¢æ•°ï¼ˆãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ãï¼‰ ==========\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                  start_index=None, end_index=None, skip_processed=True, debug_mode=False):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒ¼ã®åˆæœŸåŒ–\n",
        "    memory_monitor = GPUMemoryMonitor(warning_threshold=0.75, critical_threshold=0.85)\n",
        "\n",
        "    # é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’è¡¨ç¤º\n",
        "    memory_monitor.display_memory_status(\"ğŸš€ å‡¦ç†é–‹å§‹æ™‚\")\n",
        "\n",
        "    # AOIã‚«ãƒ©ãƒ ã®å®šç¾©\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ \n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    # å‡¦ç†æ¸ˆã¿ç”»åƒã®åˆ¤å®šé–¢æ•°\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š\n",
        "    if skip_processed and start_index is None and end_index is None:\n",
        "        # æœªå‡¦ç†ã®ç”»åƒã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹\n",
        "        unprocessed_indices = []\n",
        "        for idx, row in df.iterrows():\n",
        "            if not is_processed(row):\n",
        "                unprocessed_indices.append(idx)\n",
        "\n",
        "        if not unprocessed_indices:\n",
        "            print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "            return\n",
        "\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}/{len(df)}\")\n",
        "        print(f\"æœ€åˆã®æœªå‡¦ç†ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {unprocessed_indices[0]}\")\n",
        "\n",
        "        # å‡¦ç†å¯¾è±¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
        "        target_indices = unprocessed_indices\n",
        "\n",
        "    else:\n",
        "        # æ‰‹å‹•ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’æŒ‡å®šã—ãŸå ´åˆ\n",
        "        if start_index is None:\n",
        "            start_index = 0\n",
        "        if end_index is None:\n",
        "            end_index = len(df)\n",
        "\n",
        "        target_indices = list(range(start_index, end_index))\n",
        "\n",
        "        # skip_processedãŒTrueã®å ´åˆã€ç¯„å›²å†…ã§ã‚‚å‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
        "        if skip_processed:\n",
        "            target_indices = [idx for idx in target_indices\n",
        "                            if idx < len(df) and not is_processed(df.iloc[idx])]\n",
        "\n",
        "    print(f\"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(target_indices)}\")\n",
        "\n",
        "    # é€²æ—çŠ¶æ³ã®è¡¨ç¤º\n",
        "    processed_count = 0\n",
        "    skipped_count = len(df) - len(target_indices) if skip_processed else 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "    cuda_error_occurred = False\n",
        "    memory_cleanup_count = 0\n",
        "\n",
        "    # ã‚¨ãƒ©ãƒ¼è©³ç´°ã®è¨˜éŒ²ç”¨\n",
        "    error_details = {}\n",
        "    # æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "    successful_layers = 0\n",
        "\n",
        "    # å®šæœŸçš„ãªä¿å­˜ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
        "    save_interval = 5  # 5ç”»åƒã”ã¨ã«ä¿å­˜ï¼ˆã‚ˆã‚Šé »ç¹ã«ï¼‰\n",
        "    memory_check_interval = 3  # 3ç”»åƒã”ã¨ã«ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯\n",
        "\n",
        "    # ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ãƒ«ãƒ¼ãƒ—\n",
        "    for i, index in enumerate(tqdm(target_indices, desc=\"Processing images\")):\n",
        "        # å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯\n",
        "        if i % memory_check_interval == 0 and i > 0:\n",
        "            health, stats = memory_monitor.check_memory_health()\n",
        "\n",
        "            if health == 'critical':\n",
        "                print(f\"\\nâš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ« ({stats['usage_ratio']*100:.1f}%) ã§ã™ï¼\")\n",
        "                memory_monitor.display_memory_status(\"ğŸ“Š ç¾åœ¨\")\n",
        "\n",
        "                # å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "                if memory_monitor.cleanup_if_needed(force=True):\n",
        "                    memory_cleanup_count += 1\n",
        "                    time.sleep(2)  # GPUã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³\n",
        "\n",
        "                    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®çŠ¶æ…‹ã‚’è¡¨ç¤º\n",
        "                    memory_monitor.display_memory_status(\"ğŸ”„ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ\")\n",
        "\n",
        "            elif health == 'warning':\n",
        "                if i % (memory_check_interval * 3) == 0:  # è­¦å‘Šæ™‚ã¯é »åº¦ã‚’ä¸‹ã’ã¦è¡¨ç¤º\n",
        "                    memory_monitor.display_memory_status(\"âš ï¸  ãƒ¡ãƒ¢ãƒªè­¦å‘Š\")\n",
        "\n",
        "                # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ\n",
        "                if memory_monitor.cleanup_if_needed():\n",
        "                    memory_cleanup_count += 1\n",
        "\n",
        "        row = df.iloc[index]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # === æ”¹å–„ç‚¹: å‰å‡¦ç†å¾Œã€å…ƒã®ç”»åƒã¯å³åº§ã«å‰Šé™¤ ===\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "                del img  # å…ƒã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯debug=Trueã‚’æ¸¡ã™\n",
        "                        debug_flag = debug_mode and processed_count < 5\n",
        "\n",
        "                        # === æ”¹å–„ç‚¹: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨ ===\n",
        "                        with YOLOV5GradCAM(model=model, layer_name=layer_name,\n",
        "                                          img_size=input_size, method=\"gradcampp\",\n",
        "                                          debug=debug_flag) as saliency_method:\n",
        "\n",
        "                            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                            if len(masks) > 0 and len(boxes) > 0:\n",
        "                                mask = masks[0][0]\n",
        "                                bbox = boxes[0][0]\n",
        "\n",
        "                                aoi = get_aoi(bbox, [mask], threshold)\n",
        "                                df.at[index, col_name] = aoi\n",
        "                                successful_layers += 1\n",
        "\n",
        "                                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šAOIå€¤ãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
        "                                if processed_count < 5:  # æœ€åˆã®5ç”»åƒã®ã¿ãƒ­ã‚°å‡ºåŠ›\n",
        "                                    print(f\"  Layer {layer_name}: AOI = {aoi:.4f}\")\n",
        "\n",
        "                                # === æ”¹å–„ç‚¹: ãƒã‚¹ã‚¯ã¨ãƒœãƒƒã‚¯ã‚¹ã‚’å³åº§ã«å‰Šé™¤ ===\n",
        "                                del mask, bbox\n",
        "                            else:\n",
        "                                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒã‚¹ã‚¯ã¾ãŸã¯ãƒœãƒƒã‚¯ã‚¹ãŒç©ºã®å ´åˆ\n",
        "                                if processed_count < 5:\n",
        "                                    print(f\"  Layer {layer_name}: No masks or boxes detected\")\n",
        "\n",
        "                            # === æ”¹å–„ç‚¹: çµæœã‚’å³åº§ã«å‰Šé™¤ ===\n",
        "                            del masks, logits, boxes, cls_names\n",
        "\n",
        "                        # withæ–‡ã‚’æŠœã‘ãŸæ™‚ç‚¹ã§è‡ªå‹•çš„ã«cleanup()ãŒå‘¼ã°ã‚Œã‚‹\n",
        "                        # è¿½åŠ ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "                        if device == \"cuda\":\n",
        "                            torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # CUDA out of memoryã‚¨ãƒ©ãƒ¼ã®ãƒã‚§ãƒƒã‚¯\n",
        "                        if \"CUDA out of memory\" in str(e):\n",
        "                            print(f\"\\n\\nCUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\")\n",
        "                            print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}\")\n",
        "                            print(f\"ç¾åœ¨ã®ç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "                            print(f\"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ¬ã‚¤ãƒ¤ãƒ¼: {layer_name}\")\n",
        "                            memory_monitor.display_memory_status(\"ğŸ’¥ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚\")\n",
        "                            cuda_error_occurred = True\n",
        "                            break\n",
        "                        else:\n",
        "                            error_count += 1\n",
        "                            # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¨˜éŒ²\n",
        "                            error_key = f\"{img_basename}_{layer_name}\"\n",
        "                            if error_key not in error_details:\n",
        "                                error_details[error_key] = str(e)\n",
        "\n",
        "                            if error_count <= 5:\n",
        "                                print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                # CUDA out of memoryã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
        "                if cuda_error_occurred:\n",
        "                    break\n",
        "\n",
        "                # === æ”¹å–„ç‚¹: å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†å¾Œã«torch_imgã‚’å‰Šé™¤ ===\n",
        "                del torch_img\n",
        "\n",
        "                processed_count += 1\n",
        "\n",
        "                # é€²æ—è¡¨ç¤ºã®æ”¹å–„ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚‚è¡¨ç¤ºï¼‰\n",
        "                if processed_count % 10 == 0:\n",
        "                    stats = memory_monitor.get_memory_stats()\n",
        "                    print(f\"\\nğŸ“ˆ é€²æ—: {processed_count}/{len(target_indices)} ç”»åƒå‡¦ç†æ¸ˆã¿ \"\n",
        "                          f\"(GPUä½¿ç”¨ç‡: {stats['usage_ratio']*100:.1f}%)\")\n",
        "\n",
        "                # å®šæœŸçš„ãªä¸­é–“ä¿å­˜\n",
        "                if (i + 1) % save_interval == 0:\n",
        "                    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šä¿å­˜å‰ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
        "                    saved_count = sum(1 for idx in target_indices[:i+1]\n",
        "                                    if idx < len(df) and is_processed(df.iloc[idx]))\n",
        "                    print(f\"\\nğŸ’¾ ä¸­é–“ä¿å­˜: {saved_count}å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\")\n",
        "\n",
        "                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ­ã‚°\n",
        "                    memory_monitor.display_memory_status(\"ğŸ’¾ ä¿å­˜æ™‚\")\n",
        "\n",
        "                    df.to_csv(csv_path, index=False)\n",
        "                    print(f\"âœ… ä¸­é–“ä¿å­˜å®Œäº†: {i + 1}/{len(target_indices)} ç”»åƒå‡¦ç†æ¸ˆã¿\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # CUDA out of memoryã‚¨ãƒ©ãƒ¼ã®ãƒã‚§ãƒƒã‚¯\n",
        "                if \"CUDA out of memory\" in str(e):\n",
        "                    print(f\"\\n\\nğŸ’¥ CUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\")\n",
        "                    print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}\")\n",
        "                    print(f\"ç¾åœ¨ã®ç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "                    memory_monitor.display_memory_status(\"ğŸ’¥ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚\")\n",
        "                    cuda_error_occurred = True\n",
        "                    break\n",
        "                else:\n",
        "                    error_count += 1\n",
        "                    if error_count <= 5:\n",
        "                        print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "        # CUDA out of memoryã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
        "        if cuda_error_occurred:\n",
        "            break\n",
        "\n",
        "    # æœ€çµ‚ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚å«ã‚€ï¼‰\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
        "    memory_monitor.display_memory_status(\"ğŸ å‡¦ç†çµ‚äº†æ™‚\")\n",
        "\n",
        "    if cuda_error_occurred:\n",
        "        print(f\"\\n\\n========== CUDA OUT OF MEMORY ã‚¨ãƒ©ãƒ¼ã§å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ ==========\")\n",
        "        print(f\"æœ€å¾Œã«å‡¦ç†ã—ã‚ˆã†ã¨ã—ãŸç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "        print(f\"å‡¦ç†æ¸ˆã¿ç”»åƒæ•°: {processed_count}\")\n",
        "        print(f\"æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†: {successful_layers}å€‹\")\n",
        "        print(f\"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œå›æ•°: {memory_cleanup_count}å›\")\n",
        "        print(f\"æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: {len(target_indices) - i}\")\n",
        "        print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "        print(f\"\\næ¬¡å›å®Ÿè¡Œæ™‚ã¯è‡ªå‹•çš„ã«ç¶šãã‹ã‚‰å‡¦ç†ã•ã‚Œã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"\\nâœ¨ å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼:\")\n",
        "        print(f\"â”œâ”€ å‡¦ç†ã—ãŸç”»åƒ: {processed_count}å€‹\")\n",
        "        print(f\"â”œâ”€ æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†: {successful_layers}å€‹\")\n",
        "        print(f\"â”œâ”€ ã‚¹ã‚­ãƒƒãƒ—ã—ãŸç”»åƒ: {skipped_count}å€‹\")\n",
        "        print(f\"â”œâ”€ è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "        print(f\"â”œâ”€ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "        print(f\"â””â”€ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œå›æ•°: {memory_cleanup_count}å›\")\n",
        "\n",
        "        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º\n",
        "        if error_details:\n",
        "            print(f\"\\nâš ï¸  ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ï¼ˆæœ€åˆã®10ä»¶ï¼‰:\")\n",
        "            for i, (key, error_msg) in enumerate(list(error_details.items())[:10]):\n",
        "                print(f\"  {i+1}. {key}: {error_msg[:100]}...\")\n",
        "\n",
        "        # æœ€çµ‚çš„ãªå®Œå…¨å‡¦ç†æ¸ˆã¿ç”»åƒæ•°ã‚’ç¢ºèª\n",
        "        fully_processed = sum(1 for idx, row in df.iterrows() if is_processed(row))\n",
        "        print(f\"\\nğŸ“Š å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒæ•°: {fully_processed}/{len(df)}\")\n",
        "\n",
        "        print(f\"\\nğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ========== å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰è¨­å®š ==========\n",
        "# ä»¥ä¸‹ã®å¤‰æ•°ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã§ãã¾ã™\n",
        "\n",
        "# MODE = \"auto\"ï¼šæœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†\n",
        "# MODE = \"manual\"ï¼šstart_indexã¨end_indexã§ç¯„å›²ã‚’æŒ‡å®šï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "# MODE = \"force\"ï¼šstart_indexã¨end_indexã§ç¯„å›²ã‚’æŒ‡å®šï¼ˆå‡¦ç†æ¸ˆã¿ã‚‚å†å‡¦ç†ï¼‰\n",
        "# MODE = \"debug\"ï¼šæœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "MODE = \"auto\"  # \"auto\", \"manual\", \"force\", \"debug\" ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®š\n",
        "\n",
        "# manualãƒ¢ãƒ¼ãƒ‰ã¾ãŸã¯forceãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ç¯„å›²æŒ‡å®š\n",
        "MANUAL_START_INDEX = 0\n",
        "MANUAL_END_INDEX = 100\n",
        "\n",
        "# ========== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ† ==========\n",
        "# ãƒ‡ãƒãƒƒã‚°é–¢æ•°ï¼šCSVãƒ•ã‚¡ã‚¤ãƒ«ã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
        "def check_csv_status(csv_path, threshold=0.5):\n",
        "    \"\"\"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†çŠ¶æ³ã‚’ç¢ºèªã™ã‚‹é–¢æ•°\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    all_layers_cols = [\n",
        "        f\"AOI_{threshold}_layermodel_17_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layermodel_20_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layermodel_23_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layer24_m_0\",\n",
        "        f\"AOI_{threshold}_layer24_m_1\",\n",
        "        f\"AOI_{threshold}_layer24_m_2\"\n",
        "    ]\n",
        "\n",
        "    print(f\"CSVãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}\")\n",
        "    print(f\"ç·è¡Œæ•°: {len(df)}\")\n",
        "\n",
        "    # å„ã‚«ãƒ©ãƒ ã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    for col in all_layers_cols:\n",
        "        if col in df.columns:\n",
        "            non_null_count = df[col].notna().sum()\n",
        "            print(f\"  {col}: {non_null_count}/{len(df)} ({non_null_count/len(df)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"  {col}: ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n",
        "\n",
        "    # å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®è¡Œæ•°\n",
        "    fully_processed = 0\n",
        "    for idx, row in df.iterrows():\n",
        "        if all(pd.notna(row[col]) and row[col] is not None for col in all_layers_cols if col in df.columns):\n",
        "            fully_processed += 1\n",
        "\n",
        "    print(f\"\\nå®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒ: {fully_processed}/{len(df)} ({fully_processed/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # æœ€åˆã®5è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
        "    print(\"\\næœ€åˆã®5è¡Œã®AOIå€¤:\")\n",
        "    for idx in range(min(5, len(df))):\n",
        "        print(f\"  è¡Œ{idx}: \", end=\"\")\n",
        "        for col in all_layers_cols:\n",
        "            if col in df.columns:\n",
        "                val = df.iloc[idx][col]\n",
        "                if pd.notna(val):\n",
        "                    print(f\"{val:.4f} \", end=\"\")\n",
        "                else:\n",
        "                    print(\"NaN \", end=\"\")\n",
        "        print()\n",
        "\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",\n",
        "    \"model_20_cv3_conv\",\n",
        "    \"model_23_cv3_conv\",\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š\n",
        "USE_BATCH_MODE = False\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # å‡¦ç†å‰ã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å‰ã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # is_processedé–¢æ•°ã‚’å®šç¾©ï¼ˆcalculate_aoié–¢æ•°å¤–ã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ï¼‰\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        all_layers_cols = [\n",
        "            (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "            (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "            (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "            (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "            (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "            (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "        ]\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # æœªå‡¦ç†ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    unprocessed_indices = []\n",
        "\n",
        "    for idx, row in df_check.iterrows():\n",
        "        if not is_processed(row):\n",
        "            unprocessed_indices.append(idx)\n",
        "\n",
        "    if not unprocessed_indices:\n",
        "        print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}\")\n",
        "\n",
        "        # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†\n",
        "        for i in range(0, len(unprocessed_indices), BATCH_SIZE):\n",
        "            batch_indices = unprocessed_indices[i:i+BATCH_SIZE]\n",
        "            batch_start = batch_indices[0] if batch_indices else 0\n",
        "            batch_end = batch_indices[-1] + 1 if batch_indices else 0\n",
        "\n",
        "            print(f\"\\nãƒãƒƒãƒ {i//BATCH_SIZE + 1}/{(len(unprocessed_indices)-1)//BATCH_SIZE + 1}: \"\n",
        "                  f\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {batch_indices}\")\n",
        "\n",
        "            # ãƒãƒƒãƒå†…ã®ç”»åƒã‚’ã¾ã¨ã‚ã¦å‡¦ç†\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=None, end_index=None, skip_processed=True, debug_mode=False)\n",
        "\n",
        "            # å‡¦ç†å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(f\"ãƒãƒƒãƒ {i//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "            # ãƒãƒƒãƒãŒå®Œäº†ã—ãŸã‚‰ã€å†åº¦æœªå‡¦ç†ç”»åƒã‚’ç¢ºèªï¼ˆä¸­æ–­ã•ã‚ŒãŸå ´åˆã®å¯¾ç­–ï¼‰\n",
        "            df_check = pd.read_csv(csv_path)\n",
        "            remaining_unprocessed = sum(1 for idx, row in df_check.iterrows() if not is_processed(row))\n",
        "            print(f\"æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: {remaining_unprocessed}\")\n",
        "\n",
        "            if remaining_unprocessed == 0:\n",
        "                break\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "    # å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "    print(f\"å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {MODE}\")\n",
        "\n",
        "    # å‡¦ç†å‰ã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å‰ã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    if MODE == \"auto\":\n",
        "        # è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆæœªå‡¦ç†ç”»åƒã‚’æ¤œå‡ºï¼‰\n",
        "        print(\"\\næœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     skip_processed=True, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"manual\":\n",
        "        # æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆç¯„å›²æŒ‡å®šã€å‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {MANUAL_START_INDEX} ã‹ã‚‰ {MANUAL_END_INDEX-1} ã¾ã§å‡¦ç†ã—ã¾ã™ï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=MANUAL_START_INDEX, end_index=MANUAL_END_INDEX,\n",
        "                     skip_processed=True, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"force\":\n",
        "        # å¼·åˆ¶å†å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆç¯„å›²æŒ‡å®šã€å‡¦ç†æ¸ˆã¿ã‚‚å†å‡¦ç†ï¼‰\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {MANUAL_START_INDEX} ã‹ã‚‰ {MANUAL_END_INDEX-1} ã¾ã§å¼·åˆ¶çš„ã«å†å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=MANUAL_START_INDEX, end_index=MANUAL_END_INDEX,\n",
        "                     skip_processed=False, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"debug\":\n",
        "        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ï¼‰\n",
        "        print(\"\\nãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ã—ã¾ã™...\")\n",
        "        # æœªå‡¦ç†ç”»åƒã‚’æ¢ã™\n",
        "        df_debug = pd.read_csv(csv_path)\n",
        "        debug_idx = None\n",
        "        for idx, row in df_debug.iterrows():\n",
        "            all_layers_cols = [\n",
        "                f\"AOI_{threshold}_layermodel_17_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layermodel_20_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layermodel_23_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layer24_m_0\",\n",
        "                f\"AOI_{threshold}_layer24_m_1\",\n",
        "                f\"AOI_{threshold}_layer24_m_2\"\n",
        "            ]\n",
        "            if any(pd.isna(row[col]) or row[col] is None for col in all_layers_cols if col in df_debug.columns):\n",
        "                debug_idx = idx\n",
        "                break\n",
        "\n",
        "        if debug_idx is not None:\n",
        "            print(f\"ãƒ‡ãƒãƒƒã‚°å¯¾è±¡: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {debug_idx}, ç”»åƒ: {df_debug.iloc[debug_idx]['image_basename']}\")\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=debug_idx, end_index=debug_idx+1,\n",
        "                         skip_processed=False, debug_mode=True)\n",
        "        else:\n",
        "            print(\"æœªå‡¦ç†ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "    else:\n",
        "        print(f\"ç„¡åŠ¹ãªãƒ¢ãƒ¼ãƒ‰: {MODE}\")\n",
        "        print(\"MODE ã¯ 'auto', 'manual', 'force', 'debug' ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    # å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)"
      ],
      "metadata": {
        "id": "XIq0IUZLdiz-",
        "outputId": "b966a4b8-e986-4985-c1f9-34073e6a1cd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\n",
            "å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: auto\n",
            "\n",
            "=== å‡¦ç†å‰ã®CSVçŠ¶æ…‹ ===\n",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "ç·è¡Œæ•°: 1039\n",
            "  AOI_0.5_layermodel_17_cv3_conv: 487/1039 (46.9%)\n",
            "  AOI_0.5_layermodel_20_cv3_conv: 487/1039 (46.9%)\n",
            "  AOI_0.5_layermodel_23_cv3_conv: 487/1039 (46.9%)\n",
            "  AOI_0.5_layer24_m_0: 487/1039 (46.9%)\n",
            "  AOI_0.5_layer24_m_1: 487/1039 (46.9%)\n",
            "  AOI_0.5_layer24_m_2: 487/1039 (46.9%)\n",
            "\n",
            "å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒ: 487/1039 (46.9%)\n",
            "\n",
            "æœ€åˆã®5è¡Œã®AOIå€¤:\n",
            "  è¡Œ0: 0.7660 0.2836 0.0995 0.0000 0.0000 0.0805 \n",
            "  è¡Œ1: 0.5208 0.1947 0.1076 0.0000 0.0000 0.0749 \n",
            "  è¡Œ2: 0.4451 0.4446 0.0835 0.0000 0.0000 0.0415 \n",
            "  è¡Œ3: 0.3175 0.1535 0.0170 0.0000 0.0000 0.0090 \n",
            "  è¡Œ4: 0.5564 0.1283 0.0797 0.0000 0.0000 0.0526 \n",
            "==================================================\n",
            "\n",
            "\n",
            "æœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™...\n",
            "\n",
            "ğŸš€ å‡¦ç†é–‹å§‹æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âœ… OK\n",
            "â”œâ”€ Usage: \u001b[92m[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.1%\u001b[0m\n",
            "â”œâ”€ Allocated: 0.03 GB\n",
            "â”œâ”€ Reserved:  0.06 GB\n",
            "â”œâ”€ Free:      39.50 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "æœªå‡¦ç†ç”»åƒæ•°: 552/1039\n",
            "æœ€åˆã®æœªå‡¦ç†ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 240\n",
            "å‡¦ç†å¯¾è±¡ç”»åƒæ•°: 552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   0%|          | 1/552 [00:04<39:31,  4.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image not found: [deposit]é¡†ç²’çŠ¶_iuh0408_01 \n",
            "Image not found: [deposit]é¡†ç²’çŠ¶_iuh0437_01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_17_cv3_conv: No masks or boxes detected\n",
            "  Layer model_20_cv3_conv: No masks or boxes detected\n",
            "  Layer model_23_cv3_conv: No masks or boxes detected\n",
            "  Layer model_24_m_0: No masks or boxes detected\n",
            "  Layer model_24_m_1: No masks or boxes detected\n",
            "  Layer model_24_m_2: No masks or boxes detected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 3/552 [00:07<20:36,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_17_cv3_conv: No masks or boxes detected\n",
            "  Layer model_20_cv3_conv: No masks or boxes detected\n",
            "  Layer model_23_cv3_conv: No masks or boxes detected\n",
            "  Layer model_24_m_0: No masks or boxes detected\n",
            "  Layer model_24_m_1: No masks or boxes detected\n",
            "  Layer model_24_m_2: No masks or boxes detected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 4/552 [00:10<23:48,  2.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_17_cv3_conv: AOI = 0.6999\n",
            "  Layer model_20_cv3_conv: AOI = 0.3270\n",
            "  Layer model_23_cv3_conv: AOI = 0.1253\n",
            "  Layer model_24_m_0: AOI = 0.0000\n",
            "  Layer model_24_m_1: AOI = 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 5/552 [00:14<26:49,  2.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_24_m_2: AOI = 0.0314\n",
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 1å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âœ… OK\n",
            "â”œâ”€ Usage: \u001b[92m[â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 6.9%\u001b[0m\n",
            "â”œâ”€ Allocated: 2.62 GB\n",
            "â”œâ”€ Reserved:  2.72 GB\n",
            "â”œâ”€ Free:      36.84 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 5/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "  Layer model_17_cv3_conv: AOI = 0.8232\n",
            "  Layer model_20_cv3_conv: AOI = 0.6214\n",
            "  Layer model_23_cv3_conv: AOI = 0.1745\n",
            "  Layer model_24_m_0: AOI = 0.0000\n",
            "  Layer model_24_m_1: AOI = 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 6/552 [00:17<26:51,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_24_m_2: AOI = 0.0422\n",
            "  Layer model_17_cv3_conv: AOI = 0.6866\n",
            "  Layer model_20_cv3_conv: AOI = 0.1576\n",
            "  Layer model_23_cv3_conv: AOI = 0.1842\n",
            "  Layer model_24_m_0: AOI = 0.0000\n",
            "  Layer model_24_m_1: AOI = 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|â–         | 7/552 [00:20<26:57,  2.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_24_m_2: AOI = 0.0513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   2%|â–         | 10/552 [00:29<27:35,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 6å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âœ… OK\n",
            "â”œâ”€ Usage: \u001b[92m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 18.3%\u001b[0m\n",
            "â”œâ”€ Allocated: 7.05 GB\n",
            "â”œâ”€ Reserved:  7.23 GB\n",
            "â”œâ”€ Free:      32.33 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 10/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   2%|â–         | 12/552 [00:35<27:28,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“ˆ é€²æ—: 10/552 ç”»åƒå‡¦ç†æ¸ˆã¿ (GPUä½¿ç”¨ç‡: 22.9%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   3%|â–         | 15/552 [00:44<27:21,  3.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 11å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âœ… OK\n",
            "â”œâ”€ Usage: \u001b[92m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 29.7%\u001b[0m\n",
            "â”œâ”€ Allocated: 11.48 GB\n",
            "â”œâ”€ Reserved:  11.77 GB\n",
            "â”œâ”€ Free:      27.79 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 15/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   4%|â–         | 20/552 [01:00<27:34,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 16å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âœ… OK\n",
            "â”œâ”€ Usage: \u001b[92m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 41.2%\u001b[0m\n",
            "â”œâ”€ Allocated: 15.91 GB\n",
            "â”œâ”€ Reserved:  16.30 GB\n",
            "â”œâ”€ Free:      23.26 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 20/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   4%|â–         | 22/552 [01:06<27:05,  3.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“ˆ é€²æ—: 20/552 ç”»åƒå‡¦ç†æ¸ˆã¿ (GPUä½¿ç”¨ç‡: 45.8%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   5%|â–         | 25/552 [01:15<27:24,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 21å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âœ… OK\n",
            "â”œâ”€ Usage: \u001b[92m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 52.7%\u001b[0m\n",
            "â”œâ”€ Allocated: 20.34 GB\n",
            "â”œâ”€ Reserved:  20.83 GB\n",
            "â”œâ”€ Free:      18.73 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 25/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   5%|â–Œ         | 30/552 [01:30<26:29,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 26å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âœ… OK\n",
            "â”œâ”€ Usage: \u001b[92m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 64.1%\u001b[0m\n",
            "â”œâ”€ Allocated: 24.76 GB\n",
            "â”œâ”€ Reserved:  25.36 GB\n",
            "â”œâ”€ Free:      14.20 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 30/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   6%|â–Œ         | 32/552 [01:37<27:00,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“ˆ é€²æ—: 30/552 ç”»åƒå‡¦ç†æ¸ˆã¿ (GPUä½¿ç”¨ç‡: 68.3%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   6%|â–‹         | 35/552 [01:46<27:09,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 31å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âœ… OK\n",
            "â”œâ”€ Usage: \u001b[92m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 74.6%\u001b[0m\n",
            "â”œâ”€ Allocated: 28.84 GB\n",
            "â”œâ”€ Reserved:  29.53 GB\n",
            "â”œâ”€ Free:      10.03 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 35/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   7%|â–‹         | 36/552 [01:49<26:57,  3.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš ï¸  ãƒ¡ãƒ¢ãƒªè­¦å‘ŠGPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  WARNING\n",
            "â”œâ”€ Usage: \u001b[93m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘] 76.9%\u001b[0m\n",
            "â”œâ”€ Allocated: 29.73 GB\n",
            "â”œâ”€ Reserved:  30.43 GB\n",
            "â”œâ”€ Free:      9.13 GB\n",
            "â””â”€ Total:     39.56 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   7%|â–‹         | 40/552 [02:07<33:03,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 36å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  CRITICAL\n",
            "â”œâ”€ Usage: \u001b[91m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘] 86.1%\u001b[0m\n",
            "â”œâ”€ Allocated: 33.27 GB\n",
            "â”œâ”€ Reserved:  34.06 GB\n",
            "â”œâ”€ Free:      5.50 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 40/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   8%|â–Š         | 42/552 [02:13<29:16,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“ˆ é€²æ—: 40/552 ç”»åƒå‡¦ç†æ¸ˆã¿ (GPUä½¿ç”¨ç‡: 90.7%)\n",
            "\n",
            "âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ« (90.7%) ã§ã™ï¼\n",
            "\n",
            "ğŸ“Š ç¾åœ¨GPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  CRITICAL\n",
            "â”œâ”€ Usage: \u001b[91m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 90.7%\u001b[0m\n",
            "â”œâ”€ Allocated: 35.04 GB\n",
            "â”œâ”€ Reserved:  35.87 GB\n",
            "â”œâ”€ Free:      3.69 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "\n",
            "ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œä¸­... (ä½¿ç”¨ç‡: 90.7%)\n",
            "âœ¨ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: 0.00 GB è§£æ”¾ã•ã‚Œã¾ã—ãŸ\n",
            "\n",
            "ğŸ”„ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾ŒGPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  CRITICAL\n",
            "â”œâ”€ Usage: \u001b[91m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 90.7%\u001b[0m\n",
            "â”œâ”€ Allocated: 35.04 GB\n",
            "â”œâ”€ Reserved:  35.87 GB\n",
            "â”œâ”€ Free:      3.69 GB\n",
            "â””â”€ Total:     39.56 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   8%|â–Š         | 45/552 [02:25<30:05,  3.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¾ ä¸­é–“ä¿å­˜: 41å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "\n",
            "ğŸ’¾ ä¿å­˜æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  CRITICAL\n",
            "â”œâ”€ Usage: \u001b[91m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 97.5%\u001b[0m\n",
            "â”œâ”€ Allocated: 37.70 GB\n",
            "â”œâ”€ Reserved:  38.59 GB\n",
            "â”œâ”€ Free:      0.97 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "âœ… ä¸­é–“ä¿å­˜å®Œäº†: 45/552 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "\n",
            "âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ« (97.5%) ã§ã™ï¼\n",
            "\n",
            "ğŸ“Š ç¾åœ¨GPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  CRITICAL\n",
            "â”œâ”€ Usage: \u001b[91m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 97.5%\u001b[0m\n",
            "â”œâ”€ Allocated: 37.70 GB\n",
            "â”œâ”€ Reserved:  38.59 GB\n",
            "â”œâ”€ Free:      0.97 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "\n",
            "ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œä¸­... (ä½¿ç”¨ç‡: 97.5%)\n",
            "âœ¨ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: 0.00 GB è§£æ”¾ã•ã‚Œã¾ã—ãŸ\n",
            "\n",
            "ğŸ”„ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾ŒGPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  CRITICAL\n",
            "â”œâ”€ Usage: \u001b[91m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 97.5%\u001b[0m\n",
            "â”œâ”€ Allocated: 37.70 GB\n",
            "â”œâ”€ Reserved:  38.59 GB\n",
            "â”œâ”€ Free:      0.97 GB\n",
            "â””â”€ Total:     39.56 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   8%|â–Š         | 45/552 [02:29<28:05,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "CUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\n",
            "ã‚¨ãƒ©ãƒ¼è©³ç´°: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB is free. Process 41447 has 39.55 GiB memory in use. Of the allocated memory 38.21 GiB is allocated by PyTorch, and 860.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ç¾åœ¨ã®ç”»åƒ: [normal]FKS_314_R_slit (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 532)\n",
            "ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ¬ã‚¤ãƒ¤ãƒ¼: model_23_cv3_conv\n",
            "\n",
            "ğŸ’¥ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  CRITICAL\n",
            "â”œâ”€ Usage: \u001b[91m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 98.7%\u001b[0m\n",
            "â”œâ”€ Allocated: 38.21 GB\n",
            "â”œâ”€ Reserved:  39.05 GB\n",
            "â”œâ”€ Free:      0.50 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "\n",
            "ğŸ å‡¦ç†çµ‚äº†æ™‚GPU Memory Status (NVIDIA A100-SXM4-40GB) âš ï¸  CRITICAL\n",
            "â”œâ”€ Usage: \u001b[91m[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 98.7%\u001b[0m\n",
            "â”œâ”€ Allocated: 38.11 GB\n",
            "â”œâ”€ Reserved:  39.05 GB\n",
            "â”œâ”€ Free:      0.50 GB\n",
            "â””â”€ Total:     39.56 GB\n",
            "\n",
            "\n",
            "========== CUDA OUT OF MEMORY ã‚¨ãƒ©ãƒ¼ã§å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ ==========\n",
            "æœ€å¾Œã«å‡¦ç†ã—ã‚ˆã†ã¨ã—ãŸç”»åƒ: [normal]FKS_314_R_slit (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 532)\n",
            "å‡¦ç†æ¸ˆã¿ç”»åƒæ•°: 43\n",
            "æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†: 248å€‹\n",
            "ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œå›æ•°: 2å›\n",
            "æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: 507\n",
            "çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "\n",
            "æ¬¡å›å®Ÿè¡Œæ™‚ã¯è‡ªå‹•çš„ã«ç¶šãã‹ã‚‰å‡¦ç†ã•ã‚Œã¾ã™ã€‚\n",
            "\n",
            "=== å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ ===\n",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "ç·è¡Œæ•°: 1039\n",
            "  AOI_0.5_layermodel_17_cv3_conv: 529/1039 (50.9%)\n",
            "  AOI_0.5_layermodel_20_cv3_conv: 529/1039 (50.9%)\n",
            "  AOI_0.5_layermodel_23_cv3_conv: 528/1039 (50.8%)\n",
            "  AOI_0.5_layer24_m_0: 528/1039 (50.8%)\n",
            "  AOI_0.5_layer24_m_1: 528/1039 (50.8%)\n",
            "  AOI_0.5_layer24_m_2: 528/1039 (50.8%)\n",
            "\n",
            "å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒ: 528/1039 (50.8%)\n",
            "\n",
            "æœ€åˆã®5è¡Œã®AOIå€¤:\n",
            "  è¡Œ0: 0.7660 0.2836 0.0995 0.0000 0.0000 0.0805 \n",
            "  è¡Œ1: 0.5208 0.1947 0.1076 0.0000 0.0000 0.0749 \n",
            "  è¡Œ2: 0.4451 0.4446 0.0835 0.0000 0.0000 0.0415 \n",
            "  è¡Œ3: 0.3175 0.1535 0.0170 0.0000 0.0000 0.0090 \n",
            "  è¡Œ4: 0.5564 0.1283 0.0797 0.0000 0.0000 0.0526 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Colabç’°å¢ƒã§cv2_imshowã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã«ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "# FutureWarningã‚’ä¸€æ™‚çš„ã«æŠ‘åˆ¶ï¼ˆregister_backward_hookä½¿ç”¨æ™‚ï¼‰\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5ã®å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ - attempt_loadã®å ´æ‰€ã‚’ä¿®æ­£\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # ä¿®æ­£: models.experimentalã‹ã‚‰\n",
        "\n",
        "# YOLOv5ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# ã“ã‚Œã¾ã§ã«ç™»å ´ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# === æœ€é©åŒ–: PYTORCH_CUDA_ALLOC_CONFã®è©³ç´°è¨­å®š ===\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
        "\n",
        "# === æœ€é©åŒ–: PyTorch 2.0ä»¥ä¸Šã®å ´åˆã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªã‚»ãƒƒãƒˆ ===\n",
        "if hasattr(torch._dynamo, 'reset'):\n",
        "    torch._dynamo.reset()\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# === æœ€é©åŒ–: CUDAãƒ†ãƒ³ã‚½ãƒ«ã®æœªè§£æ”¾ã‚’æ¤œå‡ºã™ã‚‹é–¢æ•° ===\n",
        "def find_cuda_tensors():\n",
        "    \"\"\"ãƒ¡ãƒ¢ãƒªã«æ®‹ã£ã¦ã„ã‚‹CUDAãƒ†ãƒ³ã‚½ãƒ«ã‚’æ¤œå‡º\"\"\"\n",
        "    cuda_tensors = []\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if torch.is_tensor(obj) and obj.is_cuda:\n",
        "                cuda_tensors.append((type(obj), obj.size(), obj.element_size() * obj.nelement()))\n",
        "        except:\n",
        "            pass\n",
        "    return cuda_tensors\n",
        "\n",
        "def log_cuda_tensors(message=\"\"):\n",
        "    \"\"\"CUDAãƒ†ãƒ³ã‚½ãƒ«ã®çŠ¶æ…‹ã‚’ãƒ­ã‚°å‡ºåŠ›\"\"\"\n",
        "    cuda_tensors = find_cuda_tensors()\n",
        "    if cuda_tensors:\n",
        "        print(f\"{message} æ¤œå‡ºã•ã‚ŒãŸCUDAãƒ†ãƒ³ã‚½ãƒ«æ•°: {len(cuda_tensors)}\")\n",
        "        # å¤§ãã„ãƒ†ãƒ³ã‚½ãƒ«ã®ã¿è¡¨ç¤º\n",
        "        large_tensors = [t for t in cuda_tensors if t[2] > 1024*1024]  # 1MBä»¥ä¸Š\n",
        "        if large_tensors:\n",
        "            print(f\"  å¤§ããªãƒ†ãƒ³ã‚½ãƒ« (>1MB): {len(large_tensors)}å€‹\")\n",
        "            for i, (tensor_type, size, bytes_size) in enumerate(large_tensors[:5]):\n",
        "                print(f\"    {i+1}. Size: {size}, Memory: {bytes_size/1024**2:.2f}MB\")\n",
        "\n",
        "# === ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–é–¢æ•° ===\n",
        "def log_memory_usage(message=\"\"):\n",
        "    \"\"\"GPU/CPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°å‡ºåŠ›\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3  # GB\n",
        "        print(f\"{message} GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB\")\n",
        "\n",
        "# === æœ€é©åŒ–: ã‚ˆã‚Šç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•° ===\n",
        "def aggressive_memory_cleanup():\n",
        "    \"\"\"ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ\"\"\"\n",
        "    # Pythonã®ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³\n",
        "    gc.collect()\n",
        "\n",
        "    # PyTorchã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # CUDAã®ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆPyTorch 1.10ä»¥é™ï¼‰\n",
        "        if hasattr(torch.cuda, 'reset_peak_memory_stats'):\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        if hasattr(torch.cuda, 'reset_accumulated_memory_stats'):\n",
        "            torch.cuda.reset_accumulated_memory_stats()\n",
        "\n",
        "    # PyTorch 2.0ä»¥ä¸Šã®å ´åˆã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚ã‚¯ãƒªã‚¢\n",
        "    if hasattr(torch._dynamo, 'reset'):\n",
        "        torch._dynamo.reset()\n",
        "\n",
        "# === æœ€é©åŒ–: å‹¾é…è¨ˆç®—ã®é¸æŠçš„æœ‰åŠ¹åŒ– ===\n",
        "def set_model_gradients(model, layer_name, enable=True):\n",
        "    \"\"\"ç‰¹å®šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ–\"\"\"\n",
        "    # ã¾ãšå…¨ä½“ã®å‹¾é…ã‚’ç„¡åŠ¹åŒ–\n",
        "    for param in model.model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # ç‰¹å®šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿æœ‰åŠ¹åŒ–\n",
        "    if enable:\n",
        "        target_layer = find_yolo_layer(model, layer_name)\n",
        "        for param in target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®YOLOV5TorchObjectDetectorã‚¯ãƒ©ã‚¹ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # === æœ€é©åŒ–: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ– ===\n",
        "        # Grad-CAMä½¿ç”¨æ™‚ã®ã¿å¿…è¦ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æœ‰åŠ¹åŒ–\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            print(\"[INFO] fetching names from coco file\")\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\",\n",
        "                          \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\",\n",
        "                          \"cow\",\n",
        "                          \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "                          \"frisbee\",\n",
        "                          \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "                          \"surfboard\",\n",
        "                          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "                          \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "                          \"couch\",\n",
        "                          \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "                          \"keyboard\", \"cell phone\",\n",
        "                          \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "                          \"teddy bear\",\n",
        "                          \"hair drier\", \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # === å¤‰æ›´ç‚¹: æ¨è«–éƒ¨åˆ†ã®with torch.no_grad()ã‚’å‰Šé™¤ ===\n",
        "        # Grad-CAMã®ãŸã‚ã«å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "\n",
        "        # === æ”¹å–„ç‚¹: NMSå¾Œã®å‡¦ç†ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’ä½¿ç”¨ ===\n",
        "        with torch.no_grad():\n",
        "            # ä»¥ä¸‹ã®å‡¦ç†ã¯CPUã§è¡Œã†ã“ã¨ã§GPUãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„\n",
        "            self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                            range(4)]\n",
        "            for i, det in enumerate(prediction):  # detections per image\n",
        "                if len(det):\n",
        "                    # detã‚’CPUã«ç§»å‹•ã—ã€å…ƒã®GPUãƒ†ãƒ³ã‚½ãƒ«ã¯å³åº§ã«å‰Šé™¤\n",
        "                    det_cpu = det.cpu()\n",
        "                    del det  # GPUãƒ¡ãƒ¢ãƒªã‚’å³åº§ã«è§£æ”¾\n",
        "\n",
        "                    for *xyxy, conf, cls in det_cpu:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(img_size, xyxy[2])\n",
        "                        xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(round(conf.item(), 2))\n",
        "                        cls = int(cls.item())\n",
        "                        self.classes[i].append(cls)\n",
        "                        if self.names is not None:\n",
        "                            self.class_names[i].append(self.names[cls])\n",
        "                        else:\n",
        "                            self.class_names[i].append(cls)\n",
        "\n",
        "            # predictionã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "            del prediction\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        del img  # å…ƒã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\n",
        "\n",
        "        # ãƒªã‚µã‚¤ã‚ºå‡¦ç†ã‚’åŠ¹ç‡åŒ–\n",
        "        resized_imgs = []\n",
        "        for im in im0:\n",
        "            resized_img = self.yolo_resize(im, new_shape=self.img_size)[0]\n",
        "            resized_imgs.append(resized_img)\n",
        "\n",
        "        img = np.array(resized_imgs)\n",
        "        del resized_imgs, im0  # ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\n",
        "\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# YOLOV5GradCAMã‚¯ãƒ©ã‚¹ï¼ˆæœ€é©åŒ–ç‰ˆï¼šãƒ•ãƒƒã‚¯ã®æ˜ç¤ºçš„ãªå‰Šé™¤ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’å®Ÿè£…ï¼‰\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\", debug=False):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.debug = debug  # ãƒ‡ãƒãƒƒã‚°ãƒ•ãƒ©ã‚°\n",
        "        self.layer_name = layer_name  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¿å­˜\n",
        "\n",
        "        # === æ”¹å–„ç‚¹1: ãƒ•ãƒƒã‚¯ãƒãƒ³ãƒ‰ãƒ«ã‚’ä¿å­˜ ===\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            # === å¤‰æ›´ç‚¹: detach()ã—ã¦ã‹ã‚‰clone()ã§ã‚ˆã‚Šå®‰å…¨ã« ===\n",
        "            try:\n",
        "                # detach()ã§è¨ˆç®—ã‚°ãƒ©ãƒ•ã‹ã‚‰åˆ‡ã‚Šé›¢ã—ã¦ã‹ã‚‰clone()\n",
        "                self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "                if self.debug:\n",
        "                    print(f\"[DEBUG] Successfully detached and cloned grad_output for {layer_name}\")\n",
        "            except Exception as e:\n",
        "                if self.debug:\n",
        "                    print(f\"[DEBUG] Error in backward_hook: {e}\")\n",
        "                raise\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            # === å¤‰æ›´ç‚¹: outputã‚‚detach().clone()ã—ã¦ã‚ˆã‚Šå®‰å…¨ã« ===\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # === æœ€é©åŒ–: å¿…è¦ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ– ===\n",
        "        set_model_gradients(self.model, layer_name, enable=True)\n",
        "\n",
        "        # === æ”¹å–„ç‚¹2: ãƒ•ãƒƒã‚¯ãƒãƒ³ãƒ‰ãƒ«ã‚’ä¿å­˜ ===\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"ãƒ•ãƒƒã‚¯ã¨ãƒ¡ãƒ¢ãƒªã‚’æ˜ç¤ºçš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹\"\"\"\n",
        "        # === æ”¹å–„ç‚¹3: ãƒ•ãƒƒã‚¯ã®å‰Šé™¤ ===\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "            if self.debug:\n",
        "                print(f\"[DEBUG] Forward hook removed for {self.layer_name}\")\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "            if self.debug:\n",
        "                print(f\"[DEBUG] Backward hook removed for {self.layer_name}\")\n",
        "\n",
        "        # === æ”¹å–„ç‚¹4: è¾æ›¸ã®ã‚¯ãƒªã‚¢ ===\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "\n",
        "        # === æœ€é©åŒ–: ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ– ===\n",
        "        set_model_gradients(self.model, self.layer_name, enable=False)\n",
        "\n",
        "        # === æ”¹å–„ç‚¹5: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã¸ã®å‚ç…§ã‚’å‰Šé™¤ ===\n",
        "        self.target_layer = None\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] GradCAM cleanup completed for {self.layer_name}\")\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ\"\"\"\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] Entering GradCAM context for {self.layer_name}\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†ãƒã‚¤ãƒ³ãƒˆï¼ˆè‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰\"\"\"\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] Exiting GradCAM context for {self.layer_name}\")\n",
        "            if exc_type is not None:\n",
        "                print(f\"[DEBUG] Exception occurred: {exc_type.__name__}: {exc_val}\")\n",
        "        self.cleanup()\n",
        "        # Falseã‚’è¿”ã™ã“ã¨ã§ä¾‹å¤–ã‚’å†ç™ºç”Ÿã•ã›ã‚‹\n",
        "        return False\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ã‚‚å¿µã®ãŸã‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\"\"\"\n",
        "        try:\n",
        "            self.cleanup()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # === æ”¹å–„ç‚¹: å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«é †ä¼æ’­ã‚’å†å®Ÿè¡Œ ===\n",
        "        # ã¾ãšã€ä¸€åº¦ã ã‘ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¦top3_indicesã‚’å–å¾—\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            # top3_indicesã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "            del top3_indices\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            # EigenCAMã®å ´åˆã¯ä¸€åº¦ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§è¨ˆç®—\n",
        "            with torch.no_grad():  # EigenCAMã¯å‹¾é…ä¸è¦\n",
        "                saliency_map = self._eigencam()\n",
        "                saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            # GradCAMã¾ãŸã¯GradCAM++ã®å ´åˆã€å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«å‡¦ç†\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                # === é‡è¦ãªå¤‰æ›´: å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œ ===\n",
        "                # ãƒ¢ãƒ‡ãƒ«ã®å‹¾é…ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
        "                self.model.zero_grad()\n",
        "\n",
        "                # === è¿½åŠ : ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã‚’é¿ã‘ã‚‹ãŸã‚ã«input_imgã‚’clone ===\n",
        "                input_img_clone = input_img.clone()\n",
        "\n",
        "                # æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
        "                _, new_logits = self.model(input_img_clone)\n",
        "\n",
        "                # å³åº§ã«ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤\n",
        "                del input_img_clone\n",
        "\n",
        "                if class_idx:\n",
        "                    score = new_logits[0][0][cls]\n",
        "                else:\n",
        "                    score = new_logits[0][0].max()\n",
        "\n",
        "                # === å¤‰æ›´ç‚¹: retain_graph=Trueã‚’å‰Šé™¤ ===\n",
        "                score.backward()  # retain_graph=Trueã‚’å‰Šé™¤\n",
        "\n",
        "                # === è¿½åŠ : gradientsã¨activationsãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª ===\n",
        "                if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                    print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                    del score, new_logits  # å­˜åœ¨ã—ãªã„å ´åˆã¯ã“ã“ã§å‰Šé™¤\n",
        "                    continue\n",
        "\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # scoreã¨new_logitsã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "                del score\n",
        "                del new_logits\n",
        "\n",
        "                # saliency_mapã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    del weights  # weightsã‚’å³åº§ã«å‰Šé™¤\n",
        "\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«epsilonã‚’è¿½åŠ \n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                    # min/maxã¯ä¸è¦ãªã®ã§å‰Šé™¤\n",
        "                    del saliency_map_min, saliency_map_max\n",
        "\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "                # === è¿½åŠ : å„ã‚¯ãƒ©ã‚¹ã®å‡¦ç†å¾Œã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "                # å‹¾é…ã¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å³åº§ã«å‰Šé™¤\n",
        "                if \"value\" in self.gradients:\n",
        "                    del self.gradients[\"value\"]\n",
        "                if \"value\" in self.activations:\n",
        "                    del self.activations[\"value\"]\n",
        "\n",
        "                # ä¸è¦ãªä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "                del gradients, activations\n",
        "\n",
        "                # CUDAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        # viewæ“ä½œã¯æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆã—ãªã„ã®ã§åŠ¹ç‡çš„\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha  # alphaã‚’å‰Šé™¤\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        del cov  # å…±åˆ†æ•£è¡Œåˆ—ã‚’å‰Šé™¤\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        del eigenvalues, eigenvectors  # ä¸è¦ãªå›ºæœ‰å€¤ãƒ»å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        del activations_reshaped, leading_eigenvector  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        del eigen_cam_min, eigen_cam_max  # min/maxã‚’å‰Šé™¤\n",
        "\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# find_yolo_layeré–¢æ•°\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "# get_aoié–¢æ•°ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ç‰ˆï¼‰\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "    total_intersect_pixels = 0\n",
        "\n",
        "    with torch.no_grad():  # å…¨ä½“ã‚’å‹¾é…è¨ˆç®—ä¸è¦ã§å›²ã‚€\n",
        "        for mask in masks:\n",
        "            # ãƒã‚¹ã‚¯ã‚’CPUã§å‡¦ç†ï¼ˆGPUãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
        "            mask_cpu = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "            del mask  # å…ƒã®ãƒã‚¹ã‚¯ã¯å³åº§ã«å‰Šé™¤\n",
        "\n",
        "            mask_cpu = np.nan_to_num(mask_cpu, nan=0.0).astype(np.uint8)\n",
        "\n",
        "            # é–¾å€¤å‡¦ç†ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ–¹æ³•ï¼‰\n",
        "            binary_mask = mask_cpu >= (threshold * 255)\n",
        "            del mask_cpu  # å‡¦ç†æ¸ˆã¿ã®ãƒã‚¹ã‚¯ã¯å‰Šé™¤\n",
        "\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "            intersect_pixels = np.sum(mask_bbox)\n",
        "            total_intersect_pixels += intersect_pixels\n",
        "\n",
        "            del mask_bbox, binary_mask  # ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "        AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "    return AOI\n",
        "\n",
        "# calculate_aoié–¢æ•°ï¼ˆæœ€é©åŒ–ç‰ˆï¼šå®Œå…¨ãªãƒ¡ãƒ¢ãƒªç®¡ç†ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                  start_index=None, end_index=None, skip_processed=True, debug_mode=False):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # AOIã‚«ãƒ©ãƒ ã®å®šç¾©\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ \n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    # å‡¦ç†æ¸ˆã¿ç”»åƒã®åˆ¤å®šé–¢æ•°\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š\n",
        "    if skip_processed and start_index is None and end_index is None:\n",
        "        # æœªå‡¦ç†ã®ç”»åƒã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹\n",
        "        unprocessed_indices = []\n",
        "        for idx, row in df.iterrows():\n",
        "            if not is_processed(row):\n",
        "                unprocessed_indices.append(idx)\n",
        "\n",
        "        if not unprocessed_indices:\n",
        "            print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "            return\n",
        "\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}/{len(df)}\")\n",
        "        print(f\"æœ€åˆã®æœªå‡¦ç†ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {unprocessed_indices[0]}\")\n",
        "\n",
        "        # å‡¦ç†å¯¾è±¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
        "        target_indices = unprocessed_indices\n",
        "\n",
        "    else:\n",
        "        # æ‰‹å‹•ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’æŒ‡å®šã—ãŸå ´åˆ\n",
        "        if start_index is None:\n",
        "            start_index = 0\n",
        "        if end_index is None:\n",
        "            end_index = len(df)\n",
        "\n",
        "        target_indices = list(range(start_index, end_index))\n",
        "\n",
        "        # skip_processedãŒTrueã®å ´åˆã€ç¯„å›²å†…ã§ã‚‚å‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
        "        if skip_processed:\n",
        "            target_indices = [idx for idx in target_indices\n",
        "                            if idx < len(df) and not is_processed(df.iloc[idx])]\n",
        "\n",
        "    print(f\"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(target_indices)}\")\n",
        "\n",
        "    # é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
        "    if device == \"cuda\":\n",
        "        log_memory_usage(\"å‡¦ç†é–‹å§‹æ™‚:\")\n",
        "        if debug_mode:\n",
        "            log_cuda_tensors(\"å‡¦ç†é–‹å§‹æ™‚:\")\n",
        "\n",
        "    # é€²æ—çŠ¶æ³ã®è¡¨ç¤º\n",
        "    processed_count = 0\n",
        "    skipped_count = len(df) - len(target_indices) if skip_processed else 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "    cuda_error_occurred = False\n",
        "\n",
        "    # ã‚¨ãƒ©ãƒ¼è©³ç´°ã®è¨˜éŒ²ç”¨\n",
        "    error_details = {}\n",
        "    # æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "    successful_layers = 0\n",
        "\n",
        "    # å®šæœŸçš„ãªä¿å­˜ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
        "    save_interval = 5  # 5ç”»åƒã”ã¨ã«ä¿å­˜ï¼ˆã‚ˆã‚Šé »ç¹ã«ï¼‰\n",
        "\n",
        "    # ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ãƒ«ãƒ¼ãƒ—\n",
        "    for i, index in enumerate(tqdm(target_indices, desc=\"Processing images\")):\n",
        "        row = df.iloc[index]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # === æ”¹å–„ç‚¹: å‰å‡¦ç†å¾Œã€å…ƒã®ç”»åƒã¯å³åº§ã«å‰Šé™¤ ===\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "                del img  # å…ƒã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯debug=Trueã‚’æ¸¡ã™\n",
        "                        debug_flag = debug_mode and processed_count < 5\n",
        "\n",
        "                        # === æ”¹å–„ç‚¹: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨ ===\n",
        "                        with YOLOV5GradCAM(model=model, layer_name=layer_name,\n",
        "                                          img_size=input_size, method=\"gradcampp\",\n",
        "                                          debug=debug_flag) as saliency_method:\n",
        "\n",
        "                            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                            if len(masks) > 0 and len(boxes) > 0:\n",
        "                                mask = masks[0][0]\n",
        "                                bbox = boxes[0][0]\n",
        "\n",
        "                                aoi = get_aoi(bbox, [mask], threshold)\n",
        "                                df.at[index, col_name] = aoi\n",
        "                                successful_layers += 1\n",
        "\n",
        "                                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šAOIå€¤ãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
        "                                if processed_count < 5:  # æœ€åˆã®5ç”»åƒã®ã¿ãƒ­ã‚°å‡ºåŠ›\n",
        "                                    print(f\"  Layer {layer_name}: AOI = {aoi:.4f}\")\n",
        "\n",
        "                                # === æ”¹å–„ç‚¹: ãƒã‚¹ã‚¯ã¨ãƒœãƒƒã‚¯ã‚¹ã‚’å³åº§ã«å‰Šé™¤ ===\n",
        "                                del mask, bbox\n",
        "                            else:\n",
        "                                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒã‚¹ã‚¯ã¾ãŸã¯ãƒœãƒƒã‚¯ã‚¹ãŒç©ºã®å ´åˆ\n",
        "                                if processed_count < 5:\n",
        "                                    print(f\"  Layer {layer_name}: No masks or boxes detected\")\n",
        "\n",
        "                            # === æ”¹å–„ç‚¹: çµæœã‚’å³åº§ã«å‰Šé™¤ ===\n",
        "                            del masks, logits, boxes, cls_names\n",
        "\n",
        "                        # withæ–‡ã‚’æŠœã‘ãŸæ™‚ç‚¹ã§è‡ªå‹•çš„ã«cleanup()ãŒå‘¼ã°ã‚Œã‚‹\n",
        "                        # è¿½åŠ ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå„ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†å¾Œï¼‰\n",
        "                        aggressive_memory_cleanup()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # CUDA out of memoryã‚¨ãƒ©ãƒ¼ã®ãƒã‚§ãƒƒã‚¯\n",
        "                        if \"CUDA out of memory\" in str(e):\n",
        "                            print(f\"\\n\\nCUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\")\n",
        "                            print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}\")\n",
        "                            print(f\"ç¾åœ¨ã®ç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "                            print(f\"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ¬ã‚¤ãƒ¤ãƒ¼: {layer_name}\")\n",
        "                            log_memory_usage(\"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚:\")\n",
        "                            if debug_mode:\n",
        "                                log_cuda_tensors(\"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚:\")\n",
        "                            cuda_error_occurred = True\n",
        "                            break\n",
        "                        else:\n",
        "                            error_count += 1\n",
        "                            # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¨˜éŒ²\n",
        "                            error_key = f\"{img_basename}_{layer_name}\"\n",
        "                            if error_key not in error_details:\n",
        "                                error_details[error_key] = str(e)\n",
        "\n",
        "                            if error_count <= 5:\n",
        "                                print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                # CUDA out of memoryã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
        "                if cuda_error_occurred:\n",
        "                    break\n",
        "\n",
        "                # === æ”¹å–„ç‚¹: å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†å¾Œã«torch_imgã‚’å‰Šé™¤ ===\n",
        "                del torch_img\n",
        "\n",
        "                # === æœ€é©åŒ–: å„ç”»åƒå‡¦ç†å¾Œã«å®Œå…¨ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "                aggressive_memory_cleanup()\n",
        "\n",
        "                processed_count += 1\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "                    log_memory_usage(f\"  {processed_count}æšå‡¦ç†å¾Œ:\")\n",
        "\n",
        "                # å®šæœŸçš„ãªä¸­é–“ä¿å­˜\n",
        "                if (i + 1) % save_interval == 0:\n",
        "                    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šä¿å­˜å‰ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
        "                    saved_count = sum(1 for idx in target_indices[:i+1]\n",
        "                                    if idx < len(df) and is_processed(df.iloc[idx]))\n",
        "                    print(f\"ä¸­é–“ä¿å­˜å‰: {saved_count}å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\")\n",
        "\n",
        "                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ­ã‚°\n",
        "                    log_memory_usage(\"ä¸­é–“ä¿å­˜æ™‚:\")\n",
        "\n",
        "                    df.to_csv(csv_path, index=False)\n",
        "                    print(f\"ä¸­é–“ä¿å­˜å®Œäº†: {i + 1}/{len(target_indices)} ç”»åƒå‡¦ç†æ¸ˆã¿\")\n",
        "\n",
        "                    # === æœ€é©åŒ–: ã‚ˆã‚Šç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "                    aggressive_memory_cleanup()\n",
        "\n",
        "                    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
        "                    log_memory_usage(\"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "                    if debug_mode:\n",
        "                        log_cuda_tensors(\"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # CUDA out of memoryã‚¨ãƒ©ãƒ¼ã®ãƒã‚§ãƒƒã‚¯\n",
        "                if \"CUDA out of memory\" in str(e):\n",
        "                    print(f\"\\n\\nCUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\")\n",
        "                    print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}\")\n",
        "                    print(f\"ç¾åœ¨ã®ç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "                    log_memory_usage(\"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚:\")\n",
        "                    cuda_error_occurred = True\n",
        "                    break\n",
        "                else:\n",
        "                    error_count += 1\n",
        "                    if error_count <= 5:\n",
        "                        print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "        # CUDA out of memoryã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
        "        if cuda_error_occurred:\n",
        "            break\n",
        "\n",
        "    # æœ€çµ‚ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚å«ã‚€ï¼‰\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
        "    if device == \"cuda\":\n",
        "        log_memory_usage(\"å‡¦ç†çµ‚äº†æ™‚:\")\n",
        "        if debug_mode:\n",
        "            log_cuda_tensors(\"å‡¦ç†çµ‚äº†æ™‚:\")\n",
        "\n",
        "    if cuda_error_occurred:\n",
        "        print(f\"\\n\\n========== CUDA OUT OF MEMORY ã‚¨ãƒ©ãƒ¼ã§å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ ==========\")\n",
        "        print(f\"æœ€å¾Œã«å‡¦ç†ã—ã‚ˆã†ã¨ã—ãŸç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "        print(f\"å‡¦ç†æ¸ˆã¿ç”»åƒæ•°: {processed_count}\")\n",
        "        print(f\"æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†: {successful_layers}å€‹\")\n",
        "        print(f\"æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: {len(target_indices) - i}\")\n",
        "        print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "        print(f\"\\næ¬¡å›å®Ÿè¡Œæ™‚ã¯è‡ªå‹•çš„ã«ç¶šãã‹ã‚‰å‡¦ç†ã•ã‚Œã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"\\nå‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼:\")\n",
        "        print(f\"- å‡¦ç†ã—ãŸç”»åƒ: {processed_count}å€‹\")\n",
        "        print(f\"- æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†: {successful_layers}å€‹\")\n",
        "        print(f\"- ã‚¹ã‚­ãƒƒãƒ—ã—ãŸç”»åƒ: {skipped_count}å€‹\")\n",
        "        print(f\"- è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "        print(f\"- ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "\n",
        "        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º\n",
        "        if error_details:\n",
        "            print(f\"\\nã‚¨ãƒ©ãƒ¼ã®è©³ç´°ï¼ˆæœ€åˆã®10ä»¶ï¼‰:\")\n",
        "            for i, (key, error_msg) in enumerate(list(error_details.items())[:10]):\n",
        "                print(f\"  {i+1}. {key}: {error_msg[:100]}...\")\n",
        "\n",
        "        # æœ€çµ‚çš„ãªå®Œå…¨å‡¦ç†æ¸ˆã¿ç”»åƒæ•°ã‚’ç¢ºèª\n",
        "        fully_processed = sum(1 for idx, row in df.iterrows() if is_processed(row))\n",
        "        print(f\"\\nå®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒæ•°: {fully_processed}/{len(df)}\")\n",
        "\n",
        "        print(f\"\\nçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ========== å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰è¨­å®š ==========\n",
        "# ä»¥ä¸‹ã®å¤‰æ•°ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã§ãã¾ã™\n",
        "\n",
        "# MODE = \"auto\"ï¼šæœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†\n",
        "# MODE = \"manual\"ï¼šstart_indexã¨end_indexã§ç¯„å›²ã‚’æŒ‡å®šï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "# MODE = \"force\"ï¼šstart_indexã¨end_indexã§ç¯„å›²ã‚’æŒ‡å®šï¼ˆå‡¦ç†æ¸ˆã¿ã‚‚å†å‡¦ç†ï¼‰\n",
        "# MODE = \"debug\"ï¼šæœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "MODE = \"auto\"  # \"auto\", \"manual\", \"force\", \"debug\" ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®š\n",
        "\n",
        "# manualãƒ¢ãƒ¼ãƒ‰ã¾ãŸã¯forceãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ç¯„å›²æŒ‡å®š\n",
        "MANUAL_START_INDEX = 0\n",
        "MANUAL_END_INDEX = 100\n",
        "\n",
        "# ========== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ† ==========\n",
        "# ãƒ‡ãƒãƒƒã‚°é–¢æ•°ï¼šCSVãƒ•ã‚¡ã‚¤ãƒ«ã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
        "def check_csv_status(csv_path, threshold=0.5):\n",
        "    \"\"\"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†çŠ¶æ³ã‚’ç¢ºèªã™ã‚‹é–¢æ•°\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    all_layers_cols = [\n",
        "        f\"AOI_{threshold}_layermodel_17_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layermodel_20_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layermodel_23_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layer24_m_0\",\n",
        "        f\"AOI_{threshold}_layer24_m_1\",\n",
        "        f\"AOI_{threshold}_layer24_m_2\"\n",
        "    ]\n",
        "\n",
        "    print(f\"CSVãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}\")\n",
        "    print(f\"ç·è¡Œæ•°: {len(df)}\")\n",
        "\n",
        "    # å„ã‚«ãƒ©ãƒ ã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    for col in all_layers_cols:\n",
        "        if col in df.columns:\n",
        "            non_null_count = df[col].notna().sum()\n",
        "            print(f\"  {col}: {non_null_count}/{len(df)} ({non_null_count/len(df)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"  {col}: ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n",
        "\n",
        "    # å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®è¡Œæ•°\n",
        "    fully_processed = 0\n",
        "    for idx, row in df.iterrows():\n",
        "        if all(pd.notna(row[col]) and row[col] is not None for col in all_layers_cols if col in df.columns):\n",
        "            fully_processed += 1\n",
        "\n",
        "    print(f\"\\nå®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒ: {fully_processed}/{len(df)} ({fully_processed/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # æœ€åˆã®5è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
        "    print(\"\\næœ€åˆã®5è¡Œã®AOIå€¤:\")\n",
        "    for idx in range(min(5, len(df))):\n",
        "        print(f\"  è¡Œ{idx}: \", end=\"\")\n",
        "        for col in all_layers_cols:\n",
        "            if col in df.columns:\n",
        "                val = df.iloc[idx][col]\n",
        "                if pd.notna(val):\n",
        "                    print(f\"{val:.4f} \", end=\"\")\n",
        "                else:\n",
        "                    print(\"NaN \", end=\"\")\n",
        "        print()\n",
        "\n",
        "# === æœ€é©åŒ–: ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º ===\n",
        "print(\"=== ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± ===\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"    Total memory: {props.total_memory / 1024**3:.2f} GB\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",\n",
        "    \"model_20_cv3_conv\",\n",
        "    \"model_23_cv3_conv\",\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š\n",
        "USE_BATCH_MODE = False\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # å‡¦ç†å‰ã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å‰ã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # is_processedé–¢æ•°ã‚’å®šç¾©ï¼ˆcalculate_aoié–¢æ•°å¤–ã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ï¼‰\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        all_layers_cols = [\n",
        "            (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "            (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "            (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "            (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "            (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "            (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "        ]\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # æœªå‡¦ç†ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    unprocessed_indices = []\n",
        "\n",
        "    for idx, row in df_check.iterrows():\n",
        "        if not is_processed(row):\n",
        "            unprocessed_indices.append(idx)\n",
        "\n",
        "    if not unprocessed_indices:\n",
        "        print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}\")\n",
        "\n",
        "        # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†\n",
        "        for i in range(0, len(unprocessed_indices), BATCH_SIZE):\n",
        "            batch_indices = unprocessed_indices[i:i+BATCH_SIZE]\n",
        "            batch_start = batch_indices[0] if batch_indices else 0\n",
        "            batch_end = batch_indices[-1] + 1 if batch_indices else 0\n",
        "\n",
        "            print(f\"\\nãƒãƒƒãƒ {i//BATCH_SIZE + 1}/{(len(unprocessed_indices)-1)//BATCH_SIZE + 1}: \"\n",
        "                  f\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {batch_indices}\")\n",
        "\n",
        "            # ãƒãƒƒãƒå†…ã®ç”»åƒã‚’ã¾ã¨ã‚ã¦å‡¦ç†\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=None, end_index=None, skip_processed=True, debug_mode=False)\n",
        "\n",
        "            # å‡¦ç†å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "            aggressive_memory_cleanup()\n",
        "\n",
        "            print(f\"ãƒãƒƒãƒ {i//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "            # ãƒãƒƒãƒãŒå®Œäº†ã—ãŸã‚‰ã€å†åº¦æœªå‡¦ç†ç”»åƒã‚’ç¢ºèªï¼ˆä¸­æ–­ã•ã‚ŒãŸå ´åˆã®å¯¾ç­–ï¼‰\n",
        "            df_check = pd.read_csv(csv_path)\n",
        "            remaining_unprocessed = sum(1 for idx, row in df_check.iterrows() if not is_processed(row))\n",
        "            print(f\"æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: {remaining_unprocessed}\")\n",
        "\n",
        "            if remaining_unprocessed == 0:\n",
        "                break\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "    # å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "    print(f\"å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {MODE}\")\n",
        "\n",
        "    # å‡¦ç†å‰ã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å‰ã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    if MODE == \"auto\":\n",
        "        # è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆæœªå‡¦ç†ç”»åƒã‚’æ¤œå‡ºï¼‰\n",
        "        print(\"\\næœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     skip_processed=True, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"manual\":\n",
        "        # æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆç¯„å›²æŒ‡å®šã€å‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {MANUAL_START_INDEX} ã‹ã‚‰ {MANUAL_END_INDEX-1} ã¾ã§å‡¦ç†ã—ã¾ã™ï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=MANUAL_START_INDEX, end_index=MANUAL_END_INDEX,\n",
        "                     skip_processed=True, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"force\":\n",
        "        # å¼·åˆ¶å†å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆç¯„å›²æŒ‡å®šã€å‡¦ç†æ¸ˆã¿ã‚‚å†å‡¦ç†ï¼‰\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {MANUAL_START_INDEX} ã‹ã‚‰ {MANUAL_END_INDEX-1} ã¾ã§å¼·åˆ¶çš„ã«å†å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=MANUAL_START_INDEX, end_index=MANUAL_END_INDEX,\n",
        "                     skip_processed=False, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"debug\":\n",
        "        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ï¼‰\n",
        "        print(\"\\nãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ã—ã¾ã™...\")\n",
        "        # æœªå‡¦ç†ç”»åƒã‚’æ¢ã™\n",
        "        df_debug = pd.read_csv(csv_path)\n",
        "        debug_idx = None\n",
        "        for idx, row in df_debug.iterrows():\n",
        "            all_layers_cols = [\n",
        "                f\"AOI_{threshold}_layermodel_17_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layermodel_20_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layermodel_23_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layer24_m_0\",\n",
        "                f\"AOI_{threshold}_layer24_m_1\",\n",
        "                f\"AOI_{threshold}_layer24_m_2\"\n",
        "            ]\n",
        "            if any(pd.isna(row[col]) or row[col] is None for col in all_layers_cols if col in df_debug.columns):\n",
        "                debug_idx = idx\n",
        "                break\n",
        "\n",
        "        if debug_idx is not None:\n",
        "            print(f\"ãƒ‡ãƒãƒƒã‚°å¯¾è±¡: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {debug_idx}, ç”»åƒ: {df_debug.iloc[debug_idx]['image_basename']}\")\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=debug_idx, end_index=debug_idx+1,\n",
        "                         skip_processed=False, debug_mode=True)\n",
        "        else:\n",
        "            print(\"æœªå‡¦ç†ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "    else:\n",
        "        print(f\"ç„¡åŠ¹ãªãƒ¢ãƒ¼ãƒ‰: {MODE}\")\n",
        "        print(\"MODE ã¯ 'auto', 'manual', 'force', 'debug' ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    # å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "\n",
        "# === æœ€é©åŒ–: æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "print(\"\\n=== æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\")\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    log_memory_usage(\"æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "    if MODE == \"debug\":\n",
        "        log_cuda_tensors(\"æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®å‰Šé™¤\n",
        "del model\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "EhLYmDwTjeC4",
        "outputId": "2f52a2ec-8f35-4aed-f882-842a433e6724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± ===\n",
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA version: 12.4\n",
            "GPU count: 1\n",
            "  GPU 0: NVIDIA A100-SXM4-40GB\n",
            "    Total memory: 39.56 GB\n",
            "==================================================\n",
            "\n",
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\n",
            "å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: auto\n",
            "\n",
            "=== å‡¦ç†å‰ã®CSVçŠ¶æ…‹ ===\n",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "ç·è¡Œæ•°: 1039\n",
            "  AOI_0.5_layermodel_17_cv3_conv: 529/1039 (50.9%)\n",
            "  AOI_0.5_layermodel_20_cv3_conv: 529/1039 (50.9%)\n",
            "  AOI_0.5_layermodel_23_cv3_conv: 528/1039 (50.8%)\n",
            "  AOI_0.5_layer24_m_0: 528/1039 (50.8%)\n",
            "  AOI_0.5_layer24_m_1: 528/1039 (50.8%)\n",
            "  AOI_0.5_layer24_m_2: 528/1039 (50.8%)\n",
            "\n",
            "å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒ: 528/1039 (50.8%)\n",
            "\n",
            "æœ€åˆã®5è¡Œã®AOIå€¤:\n",
            "  è¡Œ0: 0.7660 0.2836 0.0995 0.0000 0.0000 0.0805 \n",
            "  è¡Œ1: 0.5208 0.1947 0.1076 0.0000 0.0000 0.0749 \n",
            "  è¡Œ2: 0.4451 0.4446 0.0835 0.0000 0.0000 0.0415 \n",
            "  è¡Œ3: 0.3175 0.1535 0.0170 0.0000 0.0000 0.0090 \n",
            "  è¡Œ4: 0.5564 0.1283 0.0797 0.0000 0.0000 0.0526 \n",
            "==================================================\n",
            "\n",
            "\n",
            "æœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™...\n",
            "æœªå‡¦ç†ç”»åƒæ•°: 511/1039\n",
            "æœ€åˆã®æœªå‡¦ç†ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 240\n",
            "å‡¦ç†å¯¾è±¡ç”»åƒæ•°: 511\n",
            "å‡¦ç†é–‹å§‹æ™‚: GPU Memory: Allocated=0.03GB, Reserved=0.06GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   0%|          | 1/511 [00:03<26:21,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image not found: [deposit]é¡†ç²’çŠ¶_iuh0408_01 \n",
            "Image not found: [deposit]é¡†ç²’çŠ¶_iuh0437_01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_17_cv3_conv: No masks or boxes detected\n",
            "  Layer model_20_cv3_conv: No masks or boxes detected\n",
            "  Layer model_23_cv3_conv: No masks or boxes detected\n",
            "  Layer model_24_m_0: No masks or boxes detected\n",
            "  Layer model_24_m_1: No masks or boxes detected\n",
            "  Layer model_24_m_2: No masks or boxes detected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 3/511 [00:05<13:21,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_17_cv3_conv: No masks or boxes detected\n",
            "  Layer model_20_cv3_conv: No masks or boxes detected\n",
            "  Layer model_23_cv3_conv: No masks or boxes detected\n",
            "  Layer model_24_m_0: No masks or boxes detected\n",
            "  Layer model_24_m_1: No masks or boxes detected\n",
            "  Layer model_24_m_2: No masks or boxes detected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 4/511 [00:07<14:08,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_17_cv3_conv: AOI = 0.6090\n",
            "  Layer model_20_cv3_conv: AOI = 0.5421\n",
            "  Layer model_23_cv3_conv: AOI = 0.1917\n",
            "  Layer model_24_m_0: AOI = 0.0000\n",
            "  Layer model_24_m_1: AOI = 0.0000\n",
            "  Layer model_24_m_2: AOI = 0.0460\n",
            "ä¸­é–“ä¿å­˜å‰: 1å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=2.62GB, Reserved=2.72GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 5/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 5/511 [00:10<17:48,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=2.62GB, Reserved=2.72GB\n",
            "  Layer model_17_cv3_conv: AOI = 0.8070\n",
            "  Layer model_20_cv3_conv: AOI = 0.5397\n",
            "  Layer model_23_cv3_conv: AOI = 0.1266\n",
            "  Layer model_24_m_0: AOI = 0.0000\n",
            "  Layer model_24_m_1: AOI = 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 6/511 [00:12<17:44,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_24_m_2: AOI = 0.0395\n",
            "  Layer model_17_cv3_conv: AOI = 0.6909\n",
            "  Layer model_20_cv3_conv: AOI = 0.2888\n",
            "  Layer model_23_cv3_conv: AOI = 0.0094\n",
            "  Layer model_24_m_0: AOI = 0.0000\n",
            "  Layer model_24_m_1: AOI = 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|â–         | 7/511 [00:14<17:55,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Layer model_24_m_2: AOI = 0.0022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   2%|â–         | 10/511 [00:21<18:46,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å‰: 6å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=7.05GB, Reserved=7.24GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 10/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=7.05GB, Reserved=7.24GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   3%|â–         | 14/511 [00:30<18:31,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å‰: 11å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=11.30GB, Reserved=11.61GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 15/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   3%|â–         | 15/511 [00:33<20:29,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=11.30GB, Reserved=11.61GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   4%|â–         | 20/511 [00:45<19:46,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å‰: 16å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=15.55GB, Reserved=15.93GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 20/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=15.55GB, Reserved=15.93GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   5%|â–         | 25/511 [00:57<19:22,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å‰: 21å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=19.98GB, Reserved=20.47GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 25/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=19.98GB, Reserved=20.47GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   6%|â–Œ         | 30/511 [01:08<18:59,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å‰: 26å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=24.41GB, Reserved=24.99GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 30/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=24.41GB, Reserved=24.99GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   7%|â–‹         | 35/511 [01:21<18:48,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å‰: 31å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=28.84GB, Reserved=29.53GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 35/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=28.84GB, Reserved=29.53GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   8%|â–Š         | 40/511 [01:32<18:25,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å‰: 36å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=33.27GB, Reserved=34.06GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 40/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=33.27GB, Reserved=34.06GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   9%|â–‰         | 45/511 [01:46<21:18,  2.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å‰: 41å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\n",
            "ä¸­é–“ä¿å­˜æ™‚: GPU Memory: Allocated=37.70GB, Reserved=38.59GB\n",
            "ä¸­é–“ä¿å­˜å®Œäº†: 45/511 ç”»åƒå‡¦ç†æ¸ˆã¿\n",
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: GPU Memory: Allocated=37.70GB, Reserved=38.59GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   9%|â–‰         | 45/511 [01:48<18:40,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "CUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\n",
            "ã‚¨ãƒ©ãƒ¼è©³ç´°: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB is free. Process 25564 has 39.55 GiB memory in use. Of the allocated memory 38.21 GiB is allocated by PyTorch, and 861.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ç¾åœ¨ã®ç”»åƒ: [normal]FKS_356_R_slit (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 573)\n",
            "ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ¬ã‚¤ãƒ¤ãƒ¼: model_23_cv3_conv\n",
            "ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚: GPU Memory: Allocated=38.21GB, Reserved=39.05GB\n",
            "å‡¦ç†çµ‚äº†æ™‚: GPU Memory: Allocated=38.11GB, Reserved=39.05GB\n",
            "\n",
            "\n",
            "========== CUDA OUT OF MEMORY ã‚¨ãƒ©ãƒ¼ã§å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ ==========\n",
            "æœ€å¾Œã«å‡¦ç†ã—ã‚ˆã†ã¨ã—ãŸç”»åƒ: [normal]FKS_356_R_slit (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 573)\n",
            "å‡¦ç†æ¸ˆã¿ç”»åƒæ•°: 43\n",
            "æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†: 248å€‹\n",
            "æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: 466\n",
            "çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "\n",
            "æ¬¡å›å®Ÿè¡Œæ™‚ã¯è‡ªå‹•çš„ã«ç¶šãã‹ã‚‰å‡¦ç†ã•ã‚Œã¾ã™ã€‚\n",
            "\n",
            "=== å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ ===\n",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "ç·è¡Œæ•°: 1039\n",
            "  AOI_0.5_layermodel_17_cv3_conv: 570/1039 (54.9%)\n",
            "  AOI_0.5_layermodel_20_cv3_conv: 570/1039 (54.9%)\n",
            "  AOI_0.5_layermodel_23_cv3_conv: 569/1039 (54.8%)\n",
            "  AOI_0.5_layer24_m_0: 569/1039 (54.8%)\n",
            "  AOI_0.5_layer24_m_1: 569/1039 (54.8%)\n",
            "  AOI_0.5_layer24_m_2: 569/1039 (54.8%)\n",
            "\n",
            "å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒ: 569/1039 (54.8%)\n",
            "\n",
            "æœ€åˆã®5è¡Œã®AOIå€¤:\n",
            "  è¡Œ0: 0.7660 0.2836 0.0995 0.0000 0.0000 0.0805 \n",
            "  è¡Œ1: 0.5208 0.1947 0.1076 0.0000 0.0000 0.0749 \n",
            "  è¡Œ2: 0.4451 0.4446 0.0835 0.0000 0.0000 0.0415 \n",
            "  è¡Œ3: 0.3175 0.1535 0.0170 0.0000 0.0000 0.0090 \n",
            "  è¡Œ4: 0.5564 0.1283 0.0797 0.0000 0.0000 0.0526 \n",
            "\n",
            "=== æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'log_memory_usage' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2328283413.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1094\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m     \u001b[0mlog_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mMODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"debug\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mlog_cuda_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'log_memory_usage' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ä¸Šè¨˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãŸã‚ã®ä¿å­˜ç”¨csvãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ**"
      ],
      "metadata": {
        "id": "y52nNVm4SrDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up7ykhgI5l3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5ee0a1-5300-42a0-fac2-a77a427f8b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è­¦å‘Š: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "ä¸Šæ›¸ãã—ã¾ã™ã‹ï¼Ÿ (y/n): y\n",
            "æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚\n",
            "\n",
            "ä½œæˆã•ã‚ŒãŸDataFrame:\n",
            "                     AOI_0.5_layermodel_17_cv3_conv  \\\n",
            "image_basename                                        \n",
            "[apac]FKS_130_R_slit                           None   \n",
            "[apac]FKS_238_L_slit                           None   \n",
            "[apac]FKS_382_L_slit                           None   \n",
            "[apac]TKB_001_R_slit                           None   \n",
            "[apac]fko0074                                  None   \n",
            "\n",
            "                     AOI_0.5_layermodel_20_cv3_conv  \\\n",
            "image_basename                                        \n",
            "[apac]FKS_130_R_slit                           None   \n",
            "[apac]FKS_238_L_slit                           None   \n",
            "[apac]FKS_382_L_slit                           None   \n",
            "[apac]TKB_001_R_slit                           None   \n",
            "[apac]fko0074                                  None   \n",
            "\n",
            "                     AOI_0.5_layermodel_23_cv3_conv AOI_0.5_layer24_m_0  \\\n",
            "image_basename                                                            \n",
            "[apac]FKS_130_R_slit                           None                None   \n",
            "[apac]FKS_238_L_slit                           None                None   \n",
            "[apac]FKS_382_L_slit                           None                None   \n",
            "[apac]TKB_001_R_slit                           None                None   \n",
            "[apac]fko0074                                  None                None   \n",
            "\n",
            "                     AOI_0.5_layer24_m_1 AOI_0.5_layer24_m_2  \n",
            "image_basename                                                \n",
            "[apac]FKS_130_R_slit                None                None  \n",
            "[apac]FKS_238_L_slit                None                None  \n",
            "[apac]FKS_382_L_slit                None                None  \n",
            "[apac]TKB_001_R_slit                None                None  \n",
            "[apac]fko0074                       None                None  \n",
            "\n",
            "ç·ç”»åƒæ•°: 1039\n",
            "\n",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "###ã“ã‚Œã‚’æŠ¼ã™ã¨csvãŒæ›´æ–°ã•ã‚Œã¦ã—ã¾ã†ã®ã§æ³¨æ„ï¼ï¼ï¼\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
        "image_dir = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "output_file_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "if os.path.exists(output_file_path):\n",
        "    print(f\"è­¦å‘Š: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™: {output_file_path}\")\n",
        "    response = input(\"ä¸Šæ›¸ãã—ã¾ã™ã‹ï¼Ÿ (y/n): \").lower()\n",
        "\n",
        "    if response != 'y':\n",
        "        print(\"å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚\")\n",
        "\n",
        "# ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "if not os.path.exists(image_dir):\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_dir}\")\n",
        "    exit()\n",
        "\n",
        "# ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆä¸€èˆ¬çš„ãªç”»åƒæ‹¡å¼µå­ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
        "image_files = []\n",
        "\n",
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯\n",
        "try:\n",
        "    for file in os.listdir(image_dir):\n",
        "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
        "            # basenameã‚’å–å¾—ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰\n",
        "            basename = os.path.splitext(file)[0]\n",
        "            image_files.append(basename)\n",
        "except Exception as e:\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®å‡¦ç†\n",
        "if not image_files:\n",
        "    print(f\"è­¦å‘Š: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "    print(f\"å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {image_dir}\")\n",
        "    print(f\"å¯¾è±¡æ‹¡å¼µå­: {', '.join(image_extensions)}\")\n",
        "    exit()\n",
        "\n",
        "# ã‚½ãƒ¼ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
        "image_files.sort()\n",
        "\n",
        "# DataFrameã‚’ä½œæˆï¼ˆbasenameã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ï¼‰\n",
        "df = pd.DataFrame(index=image_files)\n",
        "df.index.name = 'image_basename'  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã‚’è¨­å®š\n",
        "\n",
        "# ä¾‹ã¨ã—ã¦thresholdã®å€¤ã‚’è¨­å®š\n",
        "threshold = 0.5\n",
        "\n",
        "# æŒ‡å®šã•ã‚ŒãŸãƒ¬ã‚¤ãƒ¤ãƒ¼ (originalã¯convã§ãªãact??)\n",
        "layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv', '24_m_0', '24_m_1', '24_m_2']\n",
        "\n",
        "# å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«å¯¾ã—ã¦æ–°ã—ã„åˆ—ã‚’ä½œæˆ\n",
        "for layer in layers:\n",
        "    df[f'AOI_{threshold}_layer{layer}'] = None  # åˆæœŸå€¤ã‚’Noneã«è¨­å®š\n",
        "\n",
        "# DataFrameã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º\n",
        "print(\"\\nä½œæˆã•ã‚ŒãŸDataFrame:\")\n",
        "print(df.head())\n",
        "print(f\"\\nç·ç”»åƒæ•°: {len(df)}\")\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
        "try:\n",
        "    df.to_csv(output_file_path, index=True)\n",
        "    print(f\"\\nCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {output_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tT3myDD_dWHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPKZg9H8RXow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6ACrQR_RXrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Colabç’°å¢ƒã§cv2_imshowã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã«ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "\n",
        "# YOLOv5ã®å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ - attempt_loadã®å ´æ‰€ã‚’ä¿®æ­£\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # ä¿®æ­£: models.experimentalã‹ã‚‰\n",
        "\n",
        "# YOLOv5ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# ã“ã‚Œã¾ã§ã«ç™»å ´ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFã‚’è¨­å®š\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFã‚’è¨­å®š\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®YOLOV5TorchObjectDetectorã‚¯ãƒ©ã‚¹\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # === å¤‰æ›´ç‚¹: ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®requires_grad_(True)ã‚’æ˜ç¤ºçš„ã«è¨­å®š ===\n",
        "        # Grad-CAMã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            print(\"[INFO] fetching names from coco file\")\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\",\n",
        "                          \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\",\n",
        "                          \"cow\",\n",
        "                          \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "                          \"frisbee\",\n",
        "                          \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "                          \"surfboard\",\n",
        "                          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "                          \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "                          \"couch\",\n",
        "                          \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "                          \"keyboard\", \"cell phone\",\n",
        "                          \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "                          \"teddy bear\",\n",
        "                          \"hair drier\", \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        # ãƒ€ãƒŸãƒ¼ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã¯ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã™ã‚‹ãŸã‚ã€ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ\n",
        "        # img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        # self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # === å¤‰æ›´ç‚¹: æ¨è«–éƒ¨åˆ†ã®with torch.no_grad()ã‚’å‰Šé™¤ ===\n",
        "        # Grad-CAMã®ãŸã‚ã«å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "\n",
        "        # ä»¥ä¸‹ã®å‡¦ç†ã¯CPUã§è¡Œã†ã“ã¨ã§GPUãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                # detã‚’CPUã«ç§»å‹•\n",
        "                det_cpu = det.cpu()\n",
        "                for *xyxy, conf, cls in det_cpu:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# YOLOV5GradCAMã‚¯ãƒ©ã‚¹\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        # === å¤‰æ›´ç‚¹: target_layerã®requires_gradã‚’Trueã«è¨­å®š ===\n",
        "        # ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®requires_grad_(True)ã‚’å‰Šé™¤ã—ãŸãŸã‚ã€ã“ã“ã§Grad-CAMã«å¿…è¦ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿Trueã«ã™ã‚‹\n",
        "        for param in target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        # åˆæœŸåŒ–æ™‚ã®ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å‰Šé™¤\n",
        "        # device = \"cuda\" if next(self.model.model.parameters()).is_cuda else \"cpu\"\n",
        "        # self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã¯å‹¾é…è¨ˆç®—ãŒå¿…è¦ãªãŸã‚ã€no_grad()ã‚’é©ç”¨ã—ãªã„\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # saliency_mapã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«epsilonã‚’è¿½åŠ \n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# find_yolo_layeré–¢æ•°\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "# get_aoié–¢æ•°\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0\n",
        "\n",
        "   for mask in masks:\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask = np.nan_to_num(mask, nan=0.0).astype(np.uint8)\n",
        "\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255\n",
        "\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "# calculate_aoié–¢æ•°ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ä»˜ãï¼‰\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                  start_index=None, end_index=None, skip_processed=True):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # AOIã‚«ãƒ©ãƒ ã®å®šç¾©\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ \n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    # å‡¦ç†æ¸ˆã¿ç”»åƒã®åˆ¤å®šé–¢æ•°\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š\n",
        "    if skip_processed and start_index is None and end_index is None:\n",
        "        # æœªå‡¦ç†ã®ç”»åƒã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹\n",
        "        unprocessed_indices = []\n",
        "        for idx, row in df.iterrows():\n",
        "            if not is_processed(row):\n",
        "                unprocessed_indices.append(idx)\n",
        "\n",
        "        if not unprocessed_indices:\n",
        "            print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "            return\n",
        "\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}/{len(df)}\")\n",
        "        print(f\"æœ€åˆã®æœªå‡¦ç†ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {unprocessed_indices[0]}\")\n",
        "\n",
        "        # å‡¦ç†å¯¾è±¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
        "        target_indices = unprocessed_indices\n",
        "\n",
        "    else:\n",
        "        # æ‰‹å‹•ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’æŒ‡å®šã—ãŸå ´åˆ\n",
        "        if start_index is None:\n",
        "            start_index = 0\n",
        "        if end_index is None:\n",
        "            end_index = len(df)\n",
        "\n",
        "        target_indices = list(range(start_index, end_index))\n",
        "\n",
        "        # skip_processedãŒTrueã®å ´åˆã€ç¯„å›²å†…ã§ã‚‚å‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
        "        if skip_processed:\n",
        "            target_indices = [idx for idx in target_indices\n",
        "                            if idx < len(df) and not is_processed(df.iloc[idx])]\n",
        "\n",
        "    print(f\"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(target_indices)}\")\n",
        "\n",
        "    # é€²æ—çŠ¶æ³ã®è¡¨ç¤º\n",
        "    processed_count = 0\n",
        "    skipped_count = len(df) - len(target_indices) if skip_processed else 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    # å®šæœŸçš„ãªä¿å­˜ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
        "    save_interval = 10  # 10ç”»åƒã”ã¨ã«ä¿å­˜\n",
        "\n",
        "    # ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ãƒ«ãƒ¼ãƒ—\n",
        "    for i, index in enumerate(tqdm(target_indices, desc=\"Processing images\")):\n",
        "        row = df.iloc[index]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=\"gradcampp\")\n",
        "                        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                        if len(masks) > 0 and len(boxes) > 0:\n",
        "                            mask = masks[0][0]\n",
        "                            bbox = boxes[0][0]\n",
        "\n",
        "                            aoi = get_aoi(bbox, [mask], threshold)\n",
        "                            df.at[index, col_name] = aoi\n",
        "\n",
        "                        del saliency_method\n",
        "                        if device == \"cuda\":\n",
        "                            torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_count += 1\n",
        "                        if error_count <= 5:\n",
        "                            print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "                # å®šæœŸçš„ãªä¸­é–“ä¿å­˜\n",
        "                if (i + 1) % save_interval == 0:\n",
        "                    df.to_csv(csv_path, index=False)\n",
        "                    print(f\"ä¸­é–“ä¿å­˜å®Œäº†: {i + 1}/{len(target_indices)} ç”»åƒå‡¦ç†æ¸ˆã¿\")\n",
        "\n",
        "                    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "                    if device == \"cuda\":\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    # æœ€çµ‚ä¿å­˜\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\nå‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼:\")\n",
        "    print(f\"- å‡¦ç†ã—ãŸç”»åƒ: {processed_count}å€‹\")\n",
        "    print(f\"- ã‚¹ã‚­ãƒƒãƒ—ã—ãŸç”»åƒ: {skipped_count}å€‹\")\n",
        "    print(f\"- è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "    print(f\"- ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "    print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",\n",
        "    \"model_20_cv3_conv\",\n",
        "    \"model_23_cv3_conv\",\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š\n",
        "USE_BATCH_MODE = False\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # is_processedé–¢æ•°ã‚’å®šç¾©ï¼ˆcalculate_aoié–¢æ•°å¤–ã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ï¼‰\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        all_layers_cols = [\n",
        "            (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "            (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "            (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "            (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "            (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "            (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "        ]\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # æœªå‡¦ç†ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    unprocessed_indices = []\n",
        "\n",
        "    for idx, row in df_check.iterrows():\n",
        "        if not is_processed(row):\n",
        "            unprocessed_indices.append(idx)\n",
        "\n",
        "    if not unprocessed_indices:\n",
        "        print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}\")\n",
        "\n",
        "        # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†\n",
        "        for i in range(0, len(unprocessed_indices), BATCH_SIZE):\n",
        "            batch_indices = unprocessed_indices[i:i+BATCH_SIZE]\n",
        "            batch_start = batch_indices[0] if batch_indices else 0\n",
        "            batch_end = batch_indices[-1] + 1 if batch_indices else 0\n",
        "\n",
        "            print(f\"\\nãƒãƒƒãƒ {i//BATCH_SIZE + 1}/{(len(unprocessed_indices)-1)//BATCH_SIZE + 1}: \"\n",
        "                  f\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {batch_indices}\")\n",
        "\n",
        "            # ãƒãƒƒãƒå†…ã®ç”»åƒã‚’ã¾ã¨ã‚ã¦å‡¦ç†\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=None, end_index=None, skip_processed=True)\n",
        "\n",
        "            # å‡¦ç†å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(f\"ãƒãƒƒãƒ {i//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "            # ãƒãƒƒãƒãŒå®Œäº†ã—ãŸã‚‰ã€å†åº¦æœªå‡¦ç†ç”»åƒã‚’ç¢ºèªï¼ˆä¸­æ–­ã•ã‚ŒãŸå ´åˆã®å¯¾ç­–ï¼‰\n",
        "            df_check = pd.read_csv(csv_path)\n",
        "            remaining_unprocessed = sum(1 for idx, row in df_check.iterrows() if not is_processed(row))\n",
        "            print(f\"æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: {remaining_unprocessed}\")\n",
        "\n",
        "            if remaining_unprocessed == 0:\n",
        "                break\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # ãƒ¢ãƒ¼ãƒ‰é¸æŠ\n",
        "    mode = input(\"å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„:\\n1: è‡ªå‹•ï¼ˆæœªå‡¦ç†ç”»åƒã‚’æ¤œå‡ºï¼‰\\n2: æ‰‹å‹•ï¼ˆç¯„å›²ã‚’æŒ‡å®šï¼‰\\n3: å¼·åˆ¶å†å‡¦ç†ï¼ˆç¯„å›²ã‚’æŒ‡å®šã€å‡¦ç†æ¸ˆã¿ã‚‚å†å‡¦ç†ï¼‰\\né¸æŠ (1/2/3): \")\n",
        "\n",
        "    if mode == \"1\":\n",
        "        # è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰\n",
        "        print(\"\\næœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     skip_processed=True)\n",
        "\n",
        "    elif mode == \"2\":\n",
        "        # æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "        start_index = int(input(\"é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \"))\n",
        "        end_index = int(input(\"çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \"))\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {start_index} ã‹ã‚‰ {end_index-1} ã¾ã§å‡¦ç†ã—ã¾ã™ï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=start_index, end_index=end_index, skip_processed=True)\n",
        "\n",
        "    elif mode == \"3\":\n",
        "        # å¼·åˆ¶å†å‡¦ç†ãƒ¢ãƒ¼ãƒ‰\n",
        "        start_index = int(input(\"é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \"))\n",
        "        end_index = int(input(\"çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \"))\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {start_index} ã‹ã‚‰ {end_index-1} ã¾ã§å¼·åˆ¶çš„ã«å†å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=start_index, end_index=end_index, skip_processed=False)\n",
        "\n",
        "    else:\n",
        "        print(\"ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\")"
      ],
      "metadata": {
        "id": "aHMNNdM1dWJd",
        "outputId": "b442f861-cd81-412d-bc08-11fb93068b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\n",
            "å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: auto\n",
            "\n",
            "æœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™...\n",
            "æœªå‡¦ç†ç”»åƒæ•°: 994/1039\n",
            "æœ€åˆã®æœªå‡¦ç†ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 45\n",
            "å‡¦ç†å¯¾è±¡ç”»åƒæ•°: 994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   0%|          | 0/994 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 1/994 [00:06<1:51:40,  6.75s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 2/994 [00:09<1:11:57,  4.35s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 3/994 [00:13<1:06:54,  4.05s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 4/994 [00:15<56:43,  3.44s/it]  /usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|          | 5/994 [00:19<59:07,  3.59s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|          | 6/994 [00:22<54:03,  3.28s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|          | 7/994 [00:24<51:37,  3.14s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|          | 8/994 [00:27<49:33,  3.02s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|          | 9/994 [00:30<49:39,  3.03s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|          | 10/994 [00:34<51:28,  3.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸­é–“ä¿å­˜å®Œäº†: 10/994 ç”»åƒå‡¦ç†æ¸ˆã¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|          | 11/994 [00:36<49:00,  2.99s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|          | 12/994 [00:39<47:54,  2.93s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|â–         | 13/994 [00:42<48:28,  2.96s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   1%|â–         | 14/994 [00:45<48:21,  2.96s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   2%|â–         | 15/994 [00:48<49:32,  3.04s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   2%|â–         | 16/994 [00:51<47:44,  2.93s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   2%|â–         | 17/994 [00:54<46:20,  2.85s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   2%|â–         | 17/994 [00:56<53:57,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "CUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\n",
            "ã‚¨ãƒ©ãƒ¼è©³ç´°: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 17090 has 14.73 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 79.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ç¾åœ¨ã®ç”»åƒ: [bullous]knk0249 (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 62)\n",
            "ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ¬ã‚¤ãƒ¤ãƒ¼: model_24_m_1\n",
            "\n",
            "\n",
            "========== CUDA OUT OF MEMORY ã‚¨ãƒ©ãƒ¼ã§å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ ==========\n",
            "æœ€å¾Œã«å‡¦ç†ã—ã‚ˆã†ã¨ã—ãŸç”»åƒ: [bullous]knk0249 (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: 62)\n",
            "å‡¦ç†æ¸ˆã¿ç”»åƒæ•°: 17\n",
            "æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: 977\n",
            "çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "\n",
            "æ¬¡å›å®Ÿè¡Œæ™‚ã¯è‡ªå‹•çš„ã«ç¶šãã‹ã‚‰å‡¦ç†ã•ã‚Œã¾ã™ã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbMtDZ6MdWNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IXxnkbradWPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj6fl4uVCCji"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "#################\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "\n",
        "# YOLOv5ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# ã“ã‚Œã¾ã§ã«ç™»å ´ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyã®å€¤ã‚’ä¿®æ­£\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS3iA_8QKWZC",
        "outputId": "c6ac0697-2f07-4539-90a4-ee8bb1672824"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrjCYpUtJlyR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "419148f1-dbcb-4cad-8b6a-1c5e05072787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\n",
            "GradCAMãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆæœŸåŒ–ä¸­...\n",
            "  Layer 1/6: model_17_cv3_conv\n",
            "  Layer 2/6: model_20_cv3_conv\n",
            "  Layer 3/6: model_23_cv3_conv\n",
            "  Layer 4/6: model_24_m_0\n",
            "  Layer 5/6: model_24_m_1\n",
            "  Layer 6/6: model_24_m_2\n",
            "\n",
            "åˆæœŸåŒ–å®Œäº†: 6å€‹ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼\n",
            "\n",
            "è§£æã‚’é–‹å§‹ã—ã¾ã™...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   0%|          | 0/1039 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 1/1039 [00:00<06:37,  2.61it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 2/1039 [00:00<06:09,  2.81it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 3/1039 [00:01<06:13,  2.78it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 4/1039 [00:01<06:14,  2.76it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/tmp/ipython-input-13-1326724775.py:155: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1039/1039 [00:01<00:00, 560.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "å‡¦ç†å®Œäº†: 5å€‹ã®ç”»åƒã‚’å‡¦ç†\n",
            "è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: 0å€‹\n",
            "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: 0å€‹\n",
            "çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"=== ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®å€‹åˆ¥å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ===\")\\nfor layer_idx, layer_name in enumerate(target_layers):\\n    print(f\"\\n--- ãƒ¬ã‚¤ãƒ¤ãƒ¼ {layer_idx+1}/6: {layer_name} ---\")\\n    \\n    try:\\n        # å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®GradCAMã‚’ä½œæˆ\\n        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, \\n                                       img_size=input_size, method=\"gradcampp\")\\n        \\n        # ã“ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ã§å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†\\n        calculate_aoi(folder_path, csv_path, threshold, model, \\n                     [saliency_method], start_index=0, end_index=None)\\n        \\n        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\\n        del saliency_method\\n        if device == \\'cuda\\':\\n            torch.cuda.empty_cache()\\n        gc.collect()\\n        \\n    except Exception as e:\\n        print(f\"  ãƒ¬ã‚¤ãƒ¤ãƒ¼ {layer_name} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}\")\\n        continue\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "##################\n",
        "##################\n",
        "## 6-layerã§è§£æ##\n",
        "##################\n",
        "##################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # å…±é€šéƒ¨åˆ†ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–\n",
        "\n",
        "   for mask in masks:\n",
        "       # ãƒã‚¹ã‚¯ã®å‰å‡¦ç†\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # é–¾å€¤ã‚’è¶…ãˆã‚‹éƒ¨åˆ†ã‚’äºŒå€¤åŒ–\n",
        "\n",
        "       # bboxã®ç¯„å›²å†…ã®ãƒã‚¹ã‚¯éƒ¨åˆ†ã‚’å–å¾—\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # é–¾å€¤ã‚’è¶…ãˆã‚‹å…±é€šéƒ¨åˆ†ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’å–å¾—\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI ã‚’è¨ˆç®—\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨­å®š\n",
        "    all_layers = [\n",
        "        ('model_17_cv3_act', f'AOI_{threshold}_layermodel_17_cv3_act'),\n",
        "        ('model_20_cv3_act', f'AOI_{threshold}_layermodel_20_cv3_act'),\n",
        "        ('model_23_cv3_act', f'AOI_{threshold}_layermodel_23_cv3_act'),\n",
        "        ('model_24_m_0', f'AOI_{threshold}_layer24_m_0'),\n",
        "        ('model_24_m_1', f'AOI_{threshold}_layer24_m_1'),\n",
        "        ('model_24_m_2', f'AOI_{threshold}_layer24_m_2')\n",
        "    ]\n",
        "\n",
        "    # åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
        "    for layer_name, col_name in all_layers:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    if end_index is None:\n",
        "        end_index = len(df)\n",
        "\n",
        "    # å®Ÿéš›ã«å‡¦ç†ã•ã‚ŒãŸç”»åƒã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "    processed_count = 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "        if index < start_index or index >= end_index:\n",
        "            continue\n",
        "\n",
        "        img_basename = row['image_basename']\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        # æ‹¡å¼µå­ã‚’è©¦ã™\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG', '']:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‡¦ç†\n",
        "                for i, (layer_name, col_name) in enumerate(all_layers):\n",
        "                    if i < len(saliency_methods):\n",
        "                        try:\n",
        "                            masks, logits, [boxes, _, _, _], cls_names = saliency_methods[i](torch_img)\n",
        "\n",
        "                            if len(masks) > 0 and len(boxes) > 0:\n",
        "                                mask = masks[0][0]\n",
        "                                bbox = boxes[0][0]\n",
        "\n",
        "                                aoi = get_aoi(bbox, [mask], threshold)\n",
        "                                df.at[index, col_name] = aoi\n",
        "\n",
        "                        except Exception as e:\n",
        "                            error_count += 1\n",
        "                            if error_count <= 5:\n",
        "                                print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                # é€²æ—è¡¨ç¤º\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    print(f\"\\nå‡¦ç†å®Œäº†: {processed_count}å€‹ã®ç”»åƒã‚’å‡¦ç†\")\n",
        "    print(f\"è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã¨saliency_methodã®å®šç¾©\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "# å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®target_layersï¼ˆconvã«ä¿®æ­£ãŒå¿…è¦ãªå ´åˆï¼‰\n",
        "target_layers = [\n",
        "    'model_17_cv3_conv',  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    'model_20_cv3_conv',  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    'model_23_cv3_conv',  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    'model_24_m_0',\n",
        "    'model_24_m_1',\n",
        "    'model_24_m_2'\n",
        "]\n",
        "\n",
        "# å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ\n",
        "USE_BATCH_MODE = False  # ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
        "BATCH_SIZE = 20       # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’ç¢ºèª\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    total_images = len(df_check)\n",
        "    print(f\"ç·ç”»åƒæ•°: {total_images}\")\n",
        "\n",
        "    # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®saliency_methods ã‚’ä½œæˆ\n",
        "    print(\"\\nGradCAMãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆæœŸåŒ–ä¸­...\")\n",
        "    saliency_methods = []\n",
        "    for i, layer in enumerate(target_layers):\n",
        "        print(f\"  Layer {i+1}/{len(target_layers)}: {layer}\")\n",
        "        try:\n",
        "            saliency_methods.append(\n",
        "                YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\")\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  ã‚¨ãƒ©ãƒ¼: {layer} - {str(e)}\")\n",
        "\n",
        "    print(f\"\\nåˆæœŸåŒ–å®Œäº†: {len(saliency_methods)}å€‹ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼\")\n",
        "\n",
        "    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ\n",
        "    for batch_start in range(0, total_images, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, total_images)\n",
        "        print(f\"\\n=== ãƒãƒƒãƒ {batch_start//BATCH_SIZE + 1}/{(total_images-1)//BATCH_SIZE + 1}: \"\n",
        "              f\"ç”»åƒ {batch_start+1}-{batch_end}/{total_images} ===\")\n",
        "\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods,\n",
        "                     start_index=batch_start, end_index=batch_end)\n",
        "\n",
        "        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"ãƒãƒƒãƒ {batch_start//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # å‡¦ç†ç¯„å›²ã®è¨­å®š\n",
        "    start_index = 0\n",
        "    end_index = 5  # Noneã§å…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
        "\n",
        "    # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®saliency_methods ã‚’ä½œæˆ\n",
        "    print(\"GradCAMãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆæœŸåŒ–ä¸­...\")\n",
        "    saliency_methods = []\n",
        "    for i, layer in enumerate(target_layers):\n",
        "        print(f\"  Layer {i+1}/{len(target_layers)}: {layer}\")\n",
        "        try:\n",
        "            saliency_methods.append(\n",
        "                YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\")\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  ã‚¨ãƒ©ãƒ¼: {layer} - {str(e)}\")\n",
        "\n",
        "    print(f\"\\nåˆæœŸåŒ–å®Œäº†: {len(saliency_methods)}å€‹ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼\")\n",
        "    print(\"\\nè§£æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "    calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®å€‹åˆ¥å‡¦ç†ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆã®ä»£æ›¿æ¡ˆï¼‰\n",
        "\"\"\"\n",
        "print(\"=== ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®å€‹åˆ¥å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ===\")\n",
        "for layer_idx, layer_name in enumerate(target_layers):\n",
        "    print(f\"\\n--- ãƒ¬ã‚¤ãƒ¤ãƒ¼ {layer_idx+1}/6: {layer_name} ---\")\n",
        "\n",
        "    try:\n",
        "        # å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®GradCAMã‚’ä½œæˆ\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name,\n",
        "                                       img_size=input_size, method=\"gradcampp\")\n",
        "\n",
        "        # ã“ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ã§å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model,\n",
        "                     [saliency_method], start_index=0, end_index=None)\n",
        "\n",
        "        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "        del saliency_method\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ãƒ¬ã‚¤ãƒ¤ãƒ¼ {layer_name} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
        "        continue\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ãƒ¡ãƒ¢ãƒªç¯€ç´„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ ###\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFã‚’è¨­å®š\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        # åˆæœŸåŒ–æ™‚ã®ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å‰Šé™¤\n",
        "        # device = \"cuda\" if next(self.model.model.parameters()).is_cuda else \"cpu\"\n",
        "        # self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # saliency_mapã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«epsilonã‚’è¿½åŠ \n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # å…±é€šéƒ¨åˆ†ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–\n",
        "\n",
        "   for mask in masks:\n",
        "       # ãƒã‚¹ã‚¯ã®å‰å‡¦ç†\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       # RuntimeWarning: invalid value encountered in cast å¯¾ç­–\n",
        "       mask = np.nan_to_num(mask, nan=0.0).astype(np.uint8)\n",
        "\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # é–¾å€¤ã‚’è¶…ãˆã‚‹éƒ¨åˆ†ã‚’äºŒå€¤åŒ–\n",
        "\n",
        "       # bboxã®ç¯„å›²å†…ã®ãƒã‚¹ã‚¯éƒ¨åˆ†ã‚’å–å¾—\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # é–¾å€¤ã‚’è¶…ãˆã‚‹å…±é€šéƒ¨åˆ†ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’å–å¾—\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI ã‚’è¨ˆç®—\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers, start_index=0, end_index=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨­å®š\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    if end_index is None:\n",
        "        end_index = len(df)\n",
        "\n",
        "    # å®Ÿéš›ã«å‡¦ç†ã•ã‚ŒãŸç”»åƒã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "    processed_count = 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "        if index < start_index or index >= end_index:\n",
        "            continue\n",
        "\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        # æ‹¡å¼µå­ã‚’è©¦ã™\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«å‡¦ç†\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«YOLOV5GradCAMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆãƒ»ç ´æ£„\n",
        "                        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=\"gradcampp\")\n",
        "                        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                        if len(masks) > 0 and len(boxes) > 0:\n",
        "                            mask = masks[0][0]\n",
        "                            bbox = boxes[0][0]\n",
        "\n",
        "                            aoi = get_aoi(bbox, [mask], threshold)\n",
        "                            df.at[index, col_name] = aoi\n",
        "\n",
        "                        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "                        del saliency_method\n",
        "                        if device == \"cuda\":\n",
        "                            torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_count += 1\n",
        "                        if error_count <= 5:\n",
        "                            print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                # é€²æ—è¡¨ç¤º\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    print(f\"\\nå‡¦ç†å®Œäº†: {processed_count}å€‹ã®ç”»åƒã‚’å‡¦ç†\")\n",
        "    print(f\"è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã¨saliency_methodã®å®šç¾©\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "# å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®target_layersï¼ˆconvã«ä¿®æ­£ãŒå¿…è¦ãªå ´åˆï¼‰\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    \"model_20_cv3_conv\",  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    \"model_23_cv3_conv\",  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ\n",
        "USE_BATCH_MODE = False  # ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
        "BATCH_SIZE = 20       # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’ç¢ºèª\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    total_images = len(df_check)\n",
        "    print(f\"ç·ç”»åƒæ•°: {total_images}\")\n",
        "\n",
        "    # calculate_aoié–¢æ•°å†…ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯saliency_methodsã‚’ç”Ÿæˆã—ãªã„\n",
        "    saliency_methods = [] # ç©ºã®ãƒªã‚¹ãƒˆã¨ã—ã¦å®šç¾©\n",
        "\n",
        "    print(\"\\nè§£æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ\n",
        "    for batch_start in range(0, total_images, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, total_images)\n",
        "        print(f\"\\n=== ãƒãƒƒãƒ {batch_start//BATCH_SIZE + 1}/{(total_images-1)//BATCH_SIZE + 1}: \"\n",
        "              f\"ç”»åƒ {batch_start+1}-{batch_end}/{total_images} ===\")\n",
        "\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=batch_start, end_index=batch_end)\n",
        "\n",
        "        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"ãƒãƒƒãƒ {batch_start//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # å‡¦ç†ç¯„å›²ã®è¨­å®š\n",
        "    start_index = 30\n",
        "    end_index = 100  # Noneã§å…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
        "\n",
        "    # calculate_aoié–¢æ•°å†…ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯saliency_methodsã‚’ç”Ÿæˆã—ãªã„\n",
        "    saliency_methods = [] # ç©ºã®ãƒªã‚¹ãƒˆã¨ã—ã¦å®šç¾©\n",
        "\n",
        "    print(\"\\nè§£æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "    calculate_aoi(folder_path, csv_path, threshold, model, target_layers, start_index, end_index)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uMYSR4IdpbaT",
        "outputId": "96083dee-b549-46c6-d85f-6c6e95f2249f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\n",
            "\n",
            "è§£æã‚’é–‹å§‹ã—ã¾ã™...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   0%|          | 0/1039 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   3%|â–         | 31/1039 [00:05<02:48,  5.97it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   3%|â–         | 32/1039 [00:07<04:11,  4.00it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   3%|â–         | 33/1039 [00:09<05:52,  2.86it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   3%|â–         | 34/1039 [00:11<08:01,  2.09it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   3%|â–         | 35/1039 [00:13<10:41,  1.57it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   3%|â–         | 36/1039 [00:15<14:10,  1.18it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 37/1039 [00:17<17:39,  1.06s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 38/1039 [00:19<20:19,  1.22s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 39/1039 [00:21<23:31,  1.41s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 40/1039 [00:23<25:37,  1.54s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 41/1039 [00:25<27:08,  1.63s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 42/1039 [00:27<27:54,  1.68s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 43/1039 [00:29<30:26,  1.83s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 44/1039 [00:31<31:03,  1.87s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 45/1039 [00:34<32:21,  1.95s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   4%|â–         | 46/1039 [00:37<38:49,  2.35s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   5%|â–         | 47/1039 [00:39<36:05,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image [bullous]ehm0128 at layer model_24_m_2: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 26467 has 14.73 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 85.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   5%|â–         | 48/1039 [00:39<28:08,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image [bullous]ehm0146_01 at layer model_17_cv3_conv: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 26467 has 14.73 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 81.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Error processing image [bullous]ehm0146_01 at layer model_20_cv3_conv: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 26467 has 14.73 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 81.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Error processing image [bullous]ehm0146_01 at layer model_23_cv3_conv: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 26467 has 14.73 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 81.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Error processing image [bullous]ehm0146_01 at layer model_24_m_0: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 26467 has 14.73 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 81.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   5%|â–Œ         | 55/1039 [00:44<12:25,  1.32it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   8%|â–Š         | 85/1039 [01:10<13:12,  1.20it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-2879449995.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nè§£æã‚’é–‹å§‹ã—ã¾ã™...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0mcalculate_aoi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-2879449995.py\u001b[0m in \u001b[0;36mcalculate_aoi\u001b[0;34m(folder_path, csv_path, threshold, model, target_layers, start_index, end_index)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg_found\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5-gradcam/utils/general.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(path, flags)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0C6tEWVkpbca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhDcRK1apbeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNL_j2lkuem5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XxKs84GkBwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE7ZYlphCpBV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVNlV1lh_rRp"
      },
      "outputs": [],
      "source": [
        "# show_result\n",
        "import pandas as pd\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# DataFrameã¨ã—ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# DataFrameã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤ºã™ã‚‹\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmGNyvTdM3c4"
      },
      "source": [
        "#**æ³¨ç›®ç‚¹è‰²å¡—ã‚Š**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx9KdyNZNzz8"
      },
      "outputs": [],
      "source": [
        "#æ³¨ç›®ç‚¹ã«è‰²ã‚’å¡—ã‚‹ï¼ˆåè»¢ã‚ã‚Šï¼‰\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•/ã‚¹ãƒãƒ›_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            color = np.array([0,0,0], dtype=np.uint8) #ãƒã‚¹ã‚¯ã®è‰²ï¼šç™½ã¯[0,0,0]\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + (1 - binary_mask[..., np.newaxis]) * color\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + (1 - black_mask[..., np.newaxis]) * color\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•/ãƒ•ã‚©ãƒˆã‚¹ãƒªãƒƒãƒˆ_serial/8.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.1  # ã“ã“ã§é–¾å€¤ã‚’æŒ‡å®š\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEn_KHC-T8C1"
      },
      "outputs": [],
      "source": [
        "# æ³¨ç›®ç‚¹ã‚’blurã™ã‚‹ï¼ˆåè»¢ã‚ã‚Šï¼‰\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•/ã‚¹ãƒãƒ›_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            # ã¼ã‹ã—åŠ¹æœã‚’é©ç”¨\n",
        "            blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + blurred_result * (1 - binary_mask[..., np.newaxis])\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + black_blurred_result * (1 - black_mask[..., np.newaxis])\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•/ãƒ•ã‚©ãƒˆã‚¹ãƒªãƒƒãƒˆ_serial/41.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.3  # ã“ã“ã§é–¾å€¤ã‚’æŒ‡å®š\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69WoakWi-ryq"
      },
      "source": [
        "#**Analyze results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h03rgBUoWi_L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV files\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# Define class names as shown in the image\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\", \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# Create confusion matrices for Slit and Sumaho datasets\n",
        "conf_matrix_slit = pd.crosstab(df_slit['class_num'], df_slit['top1'])\n",
        "conf_matrix_sumaho = pd.crosstab(df_sumaho['class_num'], df_sumaho['top1'])\n",
        "\n",
        "# Rename the index and columns for better readability\n",
        "conf_matrix_slit.index = class_names\n",
        "conf_matrix_slit.columns = class_names\n",
        "conf_matrix_sumaho.index = class_names\n",
        "conf_matrix_sumaho.columns = class_names\n",
        "\n",
        "# Create a figure with higher DPI\n",
        "plt.figure(figsize=(15, 6), dpi=350)  # æ¨ªå¹…ã‚’å°‘ã—åºƒã’ã¾ã—ãŸ\n",
        "\n",
        "# Slit Lamp Data Confusion Matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_slit, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Slit Lamp Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')  # æ¨ªè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’45åº¦å‚¾ã‘ã‚‹\n",
        "\n",
        "# Smartphone Data Confusion Matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_sumaho, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Smartphone Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')  # æ¨ªè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’45åº¦å‚¾ã‘ã‚‹\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure at 350 DPI\n",
        "plt.savefig('confusion_matrices_350dpi.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTt2gTWAAD3",
        "outputId": "7288996f-0e39-46f7-e4d6-d7808826263b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.08771929824562, 77.63157894736842)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Calculate accuracy for Slit and Sumaho datasets\n",
        "accuracy_slit = (conf_matrix_slit.values.diagonal().sum() / conf_matrix_slit.values.sum()) * 100\n",
        "accuracy_sumaho = (conf_matrix_sumaho.values.diagonal().sum() / conf_matrix_sumaho.values.sum()) * 100\n",
        "\n",
        "accuracy_slit, accuracy_sumaho\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ã‚¹ãƒãƒ›vsã‚¹ãƒªãƒƒãƒˆ (GradCAM++)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# åˆ—åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º\n",
        "for df, device_name in [(df_slit, \"Slit Lamp\"), (df_sumaho, \"Smartphone\")]:\n",
        "    print(f\"\\n============= {device_name} ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤ =============\")\n",
        "\n",
        "    # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è©³ç´°ãªçµ±è¨ˆé‡ã‚’è¨ˆç®—\n",
        "    for column in columns_of_interest:\n",
        "        stats = df[column].describe()\n",
        "        percentiles = df[column].quantile([0.1, 0.25, 0.75, 0.9])\n",
        "\n",
        "        print(f\"\\n------- {column} ã®çµ±è¨ˆå€¤ -------\")\n",
        "        print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {int(stats['count'])}\")\n",
        "        print(f\"å¹³å‡å€¤: {stats['mean']:.6f}\")\n",
        "        print(f\"æ¨™æº–åå·®: {stats['std']:.6f}\")\n",
        "        print(f\"æœ€å°å€¤: {stats['min']:.6f}\")\n",
        "        print(f\"10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {percentiles[0.1]:.6f}\")\n",
        "        print(f\"25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {stats['25%']:.6f}\")\n",
        "        print(f\"ä¸­å¤®å€¤: {stats['50%']:.6f}\")\n",
        "        print(f\"75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {stats['75%']:.6f}\")\n",
        "        print(f\"90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {percentiles[0.9]:.6f}\")\n",
        "        print(f\"æœ€å¤§å€¤: {stats['max']:.6f}\")\n",
        "\n",
        "        # æ­ªåº¦ã¨å°–åº¦ã‚‚è¨ˆç®—\n",
        "        skewness = df[column].skew()\n",
        "        kurtosis = df[column].kurtosis()\n",
        "        print(f\"æ­ªåº¦: {skewness:.6f}\")\n",
        "        print(f\"å°–åº¦: {kurtosis:.6f}\")\n",
        "\n",
        "    # 3ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆè¨ˆå€¤ã®çµ±è¨ˆã‚’è¨ˆç®—\n",
        "    layer_sum = df[columns_of_interest].sum(axis=1)\n",
        "    sum_stats = layer_sum.describe()\n",
        "    sum_percentiles = layer_sum.quantile([0.1, 0.9])\n",
        "    sum_skewness = layer_sum.skew()\n",
        "    sum_kurtosis = layer_sum.kurtosis()\n",
        "\n",
        "    print(f\"\\n------- 3ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆè¨ˆå€¤ã®çµ±è¨ˆ -------\")\n",
        "    print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {int(sum_stats['count'])}\")\n",
        "    print(f\"å¹³å‡å€¤: {sum_stats['mean']:.6f}\")\n",
        "    print(f\"æ¨™æº–åå·®: {sum_stats['std']:.6f}\")\n",
        "    print(f\"æœ€å°å€¤: {sum_stats['min']:.6f}\")\n",
        "    print(f\"10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_percentiles[0.1]:.6f}\")\n",
        "    print(f\"25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_stats['25%']:.6f}\")\n",
        "    print(f\"ä¸­å¤®å€¤: {sum_stats['50%']:.6f}\")\n",
        "    print(f\"75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_stats['75%']:.6f}\")\n",
        "    print(f\"90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_percentiles[0.9]:.6f}\")\n",
        "    print(f\"æœ€å¤§å€¤: {sum_stats['max']:.6f}\")\n",
        "\n",
        "    # ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—\n",
        "    print(f\"\\n------- ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®ç›¸é–¢ä¿‚æ•° -------\")\n",
        "    correlation_matrix = df[columns_of_interest].corr()\n",
        "    print(correlation_matrix.round(6))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã®è¡¨ç¤º"
      ],
      "metadata": {
        "id": "NDEHCADOprhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v9DUXYHkJ5x",
        "outputId": "0e9b02c0-f9aa-48b8-b46c-6ed8a95f44d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Â± SD AOI for Slit Lamp Data\n",
            "              AOI_0.5_layer24_m_0      AOI_0.5_layer24_m_1       \\\n",
            "                             mean  std                mean  std   \n",
            "infection                     0.0  0.0                 0.0  0.0   \n",
            "normal                        0.0  0.0                 0.0  0.0   \n",
            "non-infection                 0.0  0.0                 0.0  0.0   \n",
            "scar                          0.0  0.0                 0.0  0.0   \n",
            "tumor                         0.0  0.0                 0.0  0.0   \n",
            "deposit                       0.0  0.0                 0.0  0.0   \n",
            "APAC                          0.0  0.0                 0.0  0.0   \n",
            "lens opacity                  0.0  0.0                 0.0  0.0   \n",
            "bullous                       0.0  0.0                 0.0  0.0   \n",
            "\n",
            "              AOI_0.5_layer24_m_2            \n",
            "                             mean       std  \n",
            "infection                0.047342  0.023897  \n",
            "normal                   0.026101  0.010555  \n",
            "non-infection            0.116942  0.075453  \n",
            "scar                     0.073900  0.030724  \n",
            "tumor                    0.112100  0.069113  \n",
            "deposit                  0.036668  0.025242  \n",
            "APAC                     0.063290  0.037150  \n",
            "lens opacity             0.072591  0.040474  \n",
            "bullous                  0.067483  0.022869  \n",
            "\n",
            "Mean Â± SD AOI for Smartphone Data\n",
            "              AOI_0.5_layer24_m_0      AOI_0.5_layer24_m_1            \\\n",
            "                             mean  std                mean       std   \n",
            "infection                     0.0  0.0            0.000000  0.000000   \n",
            "normal                        0.0  0.0            0.001019  0.004992   \n",
            "non-infection                 0.0  0.0            0.000000  0.000000   \n",
            "scar                          0.0  0.0            0.000000  0.000000   \n",
            "tumor                         0.0  0.0            0.000000  0.000000   \n",
            "deposit                       0.0  0.0            0.005983  0.034369   \n",
            "APAC                          0.0  0.0            0.000000  0.000000   \n",
            "lens opacity                  0.0  0.0            0.000000  0.000000   \n",
            "bullous                       0.0  0.0            0.000000  0.000000   \n",
            "\n",
            "              AOI_0.5_layer24_m_2            \n",
            "                             mean       std  \n",
            "infection                0.063889  0.037739  \n",
            "normal                   0.037293  0.024043  \n",
            "non-infection            0.178614  0.139032  \n",
            "scar                     0.125316  0.081693  \n",
            "tumor                    0.165420  0.065204  \n",
            "deposit                  0.083059  0.058240  \n",
            "APAC                     0.079910  0.030570  \n",
            "lens opacity             0.111812  0.069523  \n",
            "bullous                  0.082440  0.040750  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# åˆ—åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "columns_of_interest = ['AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚¯ãƒ©ã‚¹åã‚’è¨­å®š\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "print(\"Mean Â± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean Â± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ã‚¹ãƒªãƒƒãƒˆï¼ˆGradCAM vs GradCAM++ï¼‰\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# åˆ—åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º\n",
        "for df, device_name in [(df_slit, \"GradCAM\"), (df_sumaho, \"GradCAM++\")]:\n",
        "    print(f\"\\n============= {device_name} ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤ =============\")\n",
        "\n",
        "    # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è©³ç´°ãªçµ±è¨ˆé‡ã‚’è¨ˆç®—\n",
        "    for column in columns_of_interest:\n",
        "        stats = df[column].describe()\n",
        "        percentiles = df[column].quantile([0.1, 0.25, 0.75, 0.9])\n",
        "\n",
        "        print(f\"\\n------- {column} ã®çµ±è¨ˆå€¤ -------\")\n",
        "        print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {int(stats['count'])}\")\n",
        "        print(f\"å¹³å‡å€¤: {stats['mean']:.6f}\")\n",
        "        print(f\"æ¨™æº–åå·®: {stats['std']:.6f}\")\n",
        "        print(f\"æœ€å°å€¤: {stats['min']:.6f}\")\n",
        "        print(f\"10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {percentiles[0.1]:.6f}\")\n",
        "        print(f\"25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {stats['25%']:.6f}\")\n",
        "        print(f\"ä¸­å¤®å€¤: {stats['50%']:.6f}\")\n",
        "        print(f\"75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {stats['75%']:.6f}\")\n",
        "        print(f\"90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {percentiles[0.9]:.6f}\")\n",
        "        print(f\"æœ€å¤§å€¤: {stats['max']:.6f}\")\n",
        "\n",
        "        # æ­ªåº¦ã¨å°–åº¦ã‚‚è¨ˆç®—\n",
        "        skewness = df[column].skew()\n",
        "        kurtosis = df[column].kurtosis()\n",
        "        print(f\"æ­ªåº¦: {skewness:.6f}\")\n",
        "        print(f\"å°–åº¦: {kurtosis:.6f}\")\n",
        "\n",
        "    # 3ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆè¨ˆå€¤ã®çµ±è¨ˆã‚’è¨ˆç®—\n",
        "    layer_sum = df[columns_of_interest].sum(axis=1)\n",
        "    sum_stats = layer_sum.describe()\n",
        "    sum_percentiles = layer_sum.quantile([0.1, 0.9])\n",
        "    sum_skewness = layer_sum.skew()\n",
        "    sum_kurtosis = layer_sum.kurtosis()\n",
        "\n",
        "    print(f\"\\n------- 3ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆè¨ˆå€¤ã®çµ±è¨ˆ -------\")\n",
        "    print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {int(sum_stats['count'])}\")\n",
        "    print(f\"å¹³å‡å€¤: {sum_stats['mean']:.6f}\")\n",
        "    print(f\"æ¨™æº–åå·®: {sum_stats['std']:.6f}\")\n",
        "    print(f\"æœ€å°å€¤: {sum_stats['min']:.6f}\")\n",
        "    print(f\"10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_percentiles[0.1]:.6f}\")\n",
        "    print(f\"25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_stats['25%']:.6f}\")\n",
        "    print(f\"ä¸­å¤®å€¤: {sum_stats['50%']:.6f}\")\n",
        "    print(f\"75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_stats['75%']:.6f}\")\n",
        "    print(f\"90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_percentiles[0.9]:.6f}\")\n",
        "    print(f\"æœ€å¤§å€¤: {sum_stats['max']:.6f}\")\n",
        "\n",
        "    # ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—\n",
        "    print(f\"\\n------- ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®ç›¸é–¢ä¿‚æ•° -------\")\n",
        "    correlation_matrix = df[columns_of_interest].corr()\n",
        "    print(correlation_matrix.round(6))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã®è¡¨ç¤º"
      ],
      "metadata": {
        "id": "sMhTeXg39b16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# åˆ—åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—\n",
        "mean_sd_aoi_gradcam = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_gradcampp = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚¯ãƒ©ã‚¹åã‚’è¨­å®š\n",
        "mean_sd_aoi_gradcam.index = class_names\n",
        "mean_sd_aoi_gradcampp.index = class_names\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "print(\"Mean Â± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_gradcam)\n",
        "print(\"\\nMean Â± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_gradcampp)"
      ],
      "metadata": {
        "id": "XYWp2le2-Vjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2rBLnp6lbjW"
      },
      "source": [
        "##**Layerã”ã¨ã«è§£æ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkZ4ZbaoUEMX"
      },
      "outputs": [],
      "source": [
        "#@title ã‚¹ãƒªãƒƒãƒˆvsã‚¹ãƒãƒ›\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"23\"\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢ã—ã¦Seabornã§ãƒ—ãƒ­ãƒƒãƒˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# åˆç®—ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# é’ã¨ã‚ªãƒ¬ãƒ³ã‚¸ã®ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆã‚’å®šç¾©\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•1: ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ã¨ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ãƒ‡ãƒ¼ã‚¿\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=colors)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•2: åˆç®—ãƒ‡ãƒ¼ã‚¿\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='#1f77b4')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•3: slitã¨sumahoã®å…¨ä½“æ¯”è¼ƒ\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n",
        "plt.xlabel('Type', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Comparison of AOI_0.5 between Slit Lamp and Smartphone', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GradCAM vs GradCAM++ (ã‚¹ãƒªãƒƒãƒˆ)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"23\"\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢ã—ã¦Seabornã§ãƒ—ãƒ­ãƒƒãƒˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "df_slit['Type'] = 'GradCAM'\n",
        "df_sumaho['Type'] = 'GradCAM++'\n",
        "\n",
        "# å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# åˆç®—ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# é’ã¨ã‚ªãƒ¬ãƒ³ã‚¸ã®ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆã‚’å®šç¾©\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•1: ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ã¨ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ãƒ‡ãƒ¼ã‚¿\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=colors)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('gradcam_overall_comparison.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•2: åˆç®—ãƒ‡ãƒ¼ã‚¿\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='#1f77b4')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•3: slitã¨sumahoã®å…¨ä½“æ¯”è¼ƒ\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n",
        "plt.xlabel('Type', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Comparison of AOI_0.5 between GradCAM and GradCAM++', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u8ZCGTRoJO7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.labelsize': 18,\n",
        "    'axes.titlesize': 20,\n",
        "    'xtick.labelsize': 14,\n",
        "    'ytick.labelsize': 14,\n",
        "    'legend.fontsize': 14,\n",
        "    'figure.titlesize': 22\n",
        "})\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©ï¼ˆæŒ‡å®šã•ã‚ŒãŸé †åºï¼‰\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®ãƒ—ãƒ­ãƒƒãƒˆã¨çµ±è¨ˆé‡è¨ˆç®—\n",
        "for layer in [17, 20, 23]:\n",
        "    layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    # ã‚°ãƒ©ãƒ•ã®ä½œæˆï¼ˆç¸¦æ¨ªæ¯”3:4ï¼‰\n",
        "    plt.figure(figsize=(12, 9))\n",
        "\n",
        "    # boxplotã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºï¼ˆå…ˆã»ã©ã¨åŒã˜ç°è‰²ã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰\n",
        "    sns.boxplot(x='class_name', y=layer_name, data=df_slit, order=class_names,\n",
        "                color='#CCCCCC',\n",
        "                boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "                medianprops={'color':'black', 'linewidth':2.5},\n",
        "                flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "                whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "                capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "    plt.xlabel('Class', fontsize=18, labelpad=10)\n",
        "    plt.ylabel(layer_name, fontsize=18, labelpad=10)\n",
        "    plt.title(f'Box Plot of {layer_name} for Slit Lamp Data', fontsize=20, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆé«˜è§£åƒåº¦ï¼‰\n",
        "    plt.savefig(f'boxplot_{layer_name}.png', dpi=350, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º\n",
        "    stats = df_slit.groupby('class_name')[layer_name].describe()\n",
        "    # ã‚¯ãƒ©ã‚¹ã®é †åºã‚’æŒ‡å®šã—ã¦ä¸¦ã³æ›¿ãˆ\n",
        "    stats = stats.reindex(class_names)\n",
        "    print(f\"\\n{layer_name}ã®åŸºæœ¬çµ±è¨ˆé‡:\")\n",
        "    print(stats)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "v5jSgs_QuUly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJlHqzTQpuHP"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "# plt.rcParams.update({\n",
        "#     'font.size': 12,\n",
        "#     'axes.labelsize': 16,\n",
        "#     'axes.titlesize': 16,\n",
        "#     'xtick.labelsize': 12,\n",
        "#     'ytick.labelsize': 12,\n",
        "#     'legend.fontsize': 12,\n",
        "#     'figure.titlesize': 18\n",
        "# })\n",
        "\n",
        "# # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "# file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "# file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "# df_slit = pd.read_csv(file_path1)\n",
        "# df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# # ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "# class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# # ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "# df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "# df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢ã—ã¦Seabornã§ãƒ—ãƒ­ãƒƒãƒˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "# df_slit['Type'] = 'Slit'\n",
        "# df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# # æ¯”è¼ƒã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "# layers_group1 = [\"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "# layers_group2 = [\"17\", \"20\", \"23\"]\n",
        "\n",
        "# def create_boxplots(layers, group_name):\n",
        "#     for layer in layers:\n",
        "#         layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "#         # å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "#         df_combined = pd.concat([\n",
        "#             df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "#             df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "#         ]).reset_index(drop=True)\n",
        "\n",
        "#         # ã‚°ãƒ©ãƒ•: ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ã¨ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ãƒ‡ãƒ¼ã‚¿\n",
        "#         plt.figure(figsize=(15, 10))\n",
        "#         sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=['#333333', '#999999'])\n",
        "#         plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "#         plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "#         plt.title(f'Box Plot of {layer_name} for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "#         plt.xticks(rotation=45, ha='right')\n",
        "#         legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "#         legend.get_title().set_fontsize(14)\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "\n",
        "#     # ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®æ¯”è¼ƒï¼ˆå¹…ã‚’åŠåˆ†ã«ï¼‰\n",
        "#     plt.figure(figsize=(7.5, 10))  # å¹…ã‚’åŠåˆ†ã«å¤‰æ›´\n",
        "#     df_melted = pd.melt(df_slit, id_vars=['class_name'], value_vars=[f'AOI_0.5_layer{layer}' for layer in layers], var_name='Layer', value_name='Value')\n",
        "#     sns.boxplot(x='Layer', y='Value', data=df_melted, palette='Blues')\n",
        "#     plt.xlabel('Layer', fontsize=14, labelpad=10)\n",
        "#     plt.ylabel('AOI_0.5 Value', fontsize=14, labelpad=10)\n",
        "#     plt.title(f'Comparison of AOI_0.5 Values\\nAcross Different Layers\\n({group_name})', fontsize=16, pad=20)\n",
        "#     plt.xticks(rotation=45, ha='right')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # ã‚°ãƒ«ãƒ¼ãƒ—1ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
        "# create_boxplots(layers_group1, \"Group 1: 24_m_0, 24_m_1, 24_m_2\")\n",
        "\n",
        "# # ã‚°ãƒ«ãƒ¼ãƒ—2ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
        "# create_boxplots(layers_group2, \"Group 2: 17, 20, 23\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ãƒ¬ã‚¤ãƒ¤ãƒ¼æ¯ã®AOIå€¤ï¼ˆ17, 20, 23ï¼‰(24-0, 24-1, 24-2)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®šã‚’ã•ã‚‰ã«å¤§ãã\n",
        "plt.rcParams.update({\n",
        "    'font.size': 18,          # 16 â†’ 18\n",
        "    'axes.labelsize': 24,     # 22 â†’ 24\n",
        "    'axes.titlesize': 26,     # 24 â†’ 26\n",
        "    'xtick.labelsize': 18,    # 16 â†’ 18\n",
        "    'ytick.labelsize': 18,    # 16 â†’ 18\n",
        "    'legend.fontsize': 18,    # 16 â†’ 18\n",
        "    'figure.titlesize': 28    # 26 â†’ 28\n",
        "})\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers_group1 = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\"]\n",
        "layers_group2 = [\"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’å¤‰æ›´ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•1ï¼ˆ3:4ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰\n",
        "plt.figure(figsize=(9, 12))  # 3:4ã®æ¯”ç‡\n",
        "\n",
        "df_melted_1 = pd.melt(df[layers_group1], var_name='Layer', value_name='AOI Value')\n",
        "df_melted_1['Layer'] = df_melted_1['Layer'].map(layer_mapping)\n",
        "\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted_1,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values\\nLayers 17, 20, and 23', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆ350dpiï¼‰\n",
        "plt.savefig('boxplot_aoi_comparison_group1.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•2ï¼ˆ3:4ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰\n",
        "plt.figure(figsize=(9, 12))  # 3:4ã®æ¯”ç‡\n",
        "\n",
        "df_melted_2 = pd.melt(df[layers_group2], var_name='Layer', value_name='AOI Value')\n",
        "df_melted_2['Layer'] = df_melted_2['Layer'].map(layer_mapping)\n",
        "\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted_2,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values\\nLayer 24 Outputs', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆ350dpiï¼‰\n",
        "plt.savefig('boxplot_aoi_comparison_group2.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡ - ã‚°ãƒ«ãƒ¼ãƒ—1 (Layers 17, 20, 23):\")\n",
        "print(df[layers_group1].describe())\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡ - ã‚°ãƒ«ãƒ¼ãƒ—2 (Layer 24 Outputs):\")\n",
        "print(df[layers_group2].describe())"
      ],
      "metadata": {
        "id": "C92rF-kAx8Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs --q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 18,\n",
        "    'axes.labelsize': 24,\n",
        "    'axes.titlesize': 26,\n",
        "    'xtick.labelsize': 18,\n",
        "    'ytick.labelsize': 18,\n",
        "    'legend.fontsize': 18,\n",
        "    'figure.titlesize': 28\n",
        "})\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# ã™ã¹ã¦ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’å¤‰æ›´ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢\n",
        "df_melted = pd.melt(df[layers], var_name='Layer', value_name='AOI Value')\n",
        "df_melted['Layer'] = df_melted['Layer'].map(layer_mapping)\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ä½œæˆï¼ˆ3:4ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# boxplotã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values Across Different Layers', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆ350dpiï¼‰\n",
        "plt.savefig('boxplot_aoi_comparison_all.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡:\")\n",
        "print(df[layers].describe())\n",
        "\n",
        "# Kruskal-Wallis æ¤œå®šã‚’å®Ÿæ–½\n",
        "h_statistic, p_value = stats.kruskal(*[df[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results:')\n",
        "print(f'H-statistic: {h_statistic:.3f}')\n",
        "print(f'p-value: {p_value:.3e}')\n",
        "\n",
        "# Dunn's testã«ã‚ˆã‚‹å¤šé‡æ¯”è¼ƒï¼ˆBonferroniè£œæ­£ï¼‰\n",
        "dunn = posthoc_dunn(df_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (p-values):')\n",
        "print(dunn.round(4))  # på€¤ã‚’4æ¡ã«ä¸¸ã‚ã‚‹\n",
        "\n",
        "# æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ã‚’æŠ½å‡ºï¼ˆp < 0.05ï¼‰\n",
        "significant_pairs = []\n",
        "for i in dunn.index:\n",
        "    for j in dunn.columns:\n",
        "        if i < j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€ä¸Šä¸‰è§’è¡Œåˆ—ã®ã¿ã‚’è¦‹ã‚‹\n",
        "            if dunn.loc[i, j] < 0.05:\n",
        "                significant_pairs.append((i, j, dunn.loc[i, j]))\n",
        "\n",
        "print('\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):')\n",
        "for pair in significant_pairs:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')"
      ],
      "metadata": {
        "id": "31icyQtM1UNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs --q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 22,  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’å¤§ãã‚ã«è¨­å®š\n",
        "    'axes.labelsize': 28,  # è»¸ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'axes.titlesize': 30,  # ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'xtick.labelsize': 24,  # xè»¸ã®ç›®ç››ã‚Šãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'ytick.labelsize': 24,  # yè»¸ã®ç›®ç››ã‚Šãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'legend.fontsize': 24,  # å‡¡ä¾‹ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'figure.titlesize': 32  # ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "})\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df1 = pd.read_csv(file_path1)\n",
        "df2 = pd.read_csv(file_path2)\n",
        "\n",
        "# ä½¿ç”¨ã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’å¤‰æ›´ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢\n",
        "df1_melted = pd.melt(df1[layers], var_name='Layer', value_name='AOI Value')\n",
        "df1_melted['Layer'] = df1_melted['Layer'].map(layer_mapping)\n",
        "df1_melted['Method'] = 'Grad-CAM'\n",
        "\n",
        "df2_melted = pd.melt(df2[layers], var_name='Layer', value_name='AOI Value')\n",
        "df2_melted['Layer'] = df2_melted['Layer'].map(layer_mapping)\n",
        "df2_melted['Method'] = 'Grad-CAM++'\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚Šç›´ã™\n",
        "df_combined = pd.concat([df1_melted, df2_melted], ignore_index=True)\n",
        "# ã‚ã‚‹ã„ã¯\n",
        "# df_combined = pd.concat([df1_melted, df2_melted])\n",
        "# df_combined = df_combined.reset_index(drop=True)\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ä½œæˆï¼ˆ4:3ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰\n",
        "plt.figure(figsize=(16, 12))  # 4:3ã®æ¯”ç‡ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´\n",
        "\n",
        "# boxplotã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
        "sns.boxplot(x='Layer', y='AOI Value', hue='Method', data=df_combined,\n",
        "            palette=[\"#1f77b4\", \"#ff7f0e\"],\n",
        "            boxprops={'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=28, labelpad=15)  # è»¸ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å¤§ãã\n",
        "plt.ylabel('AOI Value', fontsize=28, labelpad=15)  # è»¸ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å¤§ãã\n",
        "plt.title('Comparison of AOI Values Across Different Layers and Methods', fontsize=30, pad=25)  # ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å¤§ãã\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Type')\n",
        "plt.tight_layout()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆ350dpiï¼‰\n",
        "plt.savefig('boxplot_aoi_comparison_all_combined.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º (Grad-CAM)\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡ (Grad-CAM):\")\n",
        "print(df1[layers].describe())\n",
        "\n",
        "# åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º (Grad-CAM++)\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡ (Grad-CAM++):\")\n",
        "print(df2[layers].describe())\n",
        "\n",
        "# Kruskal-Wallis æ¤œå®šã‚’å®Ÿæ–½ (Grad-CAM)\n",
        "h_statistic1, p_value1 = stats.kruskal(*[df1[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results (Grad-CAM):')\n",
        "print(f'H-statistic: {h_statistic1:.3f}')\n",
        "print(f'p-value: {p_value1:.3e}')\n",
        "\n",
        "# Kruskal-Wallis æ¤œå®šã‚’å®Ÿæ–½ (Grad-CAM++)\n",
        "h_statistic2, p_value2 = stats.kruskal(*[df2[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results (Grad-CAM++):')\n",
        "print(f'H-statistic: {h_statistic2:.3f}')\n",
        "print(f'p-value: {p_value2:.3e}')\n",
        "\n",
        "# Dunn's testã«ã‚ˆã‚‹å¤šé‡æ¯”è¼ƒï¼ˆBonferroniè£œæ­£ï¼‰(Grad-CAM)\n",
        "dunn1 = posthoc_dunn(df1_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (Grad-CAM) (p-values):')\n",
        "print(dunn1.round(4))  # på€¤ã‚’4æ¡ã«ä¸¸ã‚ã‚‹\n",
        "\n",
        "# Dunn's testã«ã‚ˆã‚‹å¤šé‡æ¯”è¼ƒï¼ˆBonferroniè£œæ­£ï¼‰(Grad-CAM++)\n",
        "dunn2 = posthoc_dunn(df2_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (Grad-CAM++) (p-values):')\n",
        "print(dunn2.round(4))  # på€¤ã‚’4æ¡ã«ä¸¸ã‚ã‚‹\n",
        "\n",
        "# æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ã‚’æŠ½å‡ºï¼ˆp < 0.05ï¼‰(Grad-CAM)\n",
        "significant_pairs1 = []\n",
        "for i in dunn1.index:\n",
        "    for j in dunn1.columns:\n",
        "        if i < j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€ä¸Šä¸‰è§’è¡Œåˆ—ã®ã¿ã‚’è¦‹ã‚‹\n",
        "            if dunn1.loc[i, j] < 0.05:\n",
        "                significant_pairs1.append((i, j, dunn1.loc[i, j]))\n",
        "\n",
        "print('\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (Grad-CAM) (p < 0.05):')\n",
        "for pair in significant_pairs1:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')\n",
        "\n",
        "# æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ã‚’æŠ½å‡ºï¼ˆp < 0.05ï¼‰(Grad-CAM++)\n",
        "significant_pairs2 = []\n",
        "for i in dunn2.index:\n",
        "    for j in dunn2.columns:\n",
        "        if i < j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€ä¸Šä¸‰è§’è¡Œåˆ—ã®ã¿ã‚’è¦‹ã‚‹\n",
        "            if dunn2.loc[i, j] < 0.05:\n",
        "                significant_pairs2.append((i, j, dunn2.loc[i, j]))\n",
        "\n",
        "print('\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (Grad-CAM++) (p < 0.05):')\n",
        "for pair in significant_pairs2:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')"
      ],
      "metadata": {
        "id": "mRmADr6uMAci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mt7gOfde1UPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgFUt3AFWwvA"
      },
      "outputs": [],
      "source": [
        "####\n",
        "#### ã‚¹ãƒªãƒƒãƒˆ vs ã‚¹ãƒãƒ›ã®æ¯”è¼ƒ\n",
        "####\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šã‚’è¡Œã†\n",
        "results = []\n",
        "for class_name in class_names:\n",
        "    slit_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "    sumaho_data = df_sumaho[df_sumaho['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "    # å¯¾å¿œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚‹ãŸã‚ã€æœ€å°ã®é•·ã•ã«åˆã‚ã›ã‚‹\n",
        "    min_length = min(len(slit_data), len(sumaho_data))\n",
        "    slit_data = slit_data[:min_length]\n",
        "    sumaho_data = sumaho_data[:min_length]\n",
        "\n",
        "    t_stat, p_value = stats.ttest_rel(slit_data, sumaho_data)\n",
        "\n",
        "    # ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤\n",
        "    slit_mean = slit_data.mean()\n",
        "    slit_std = slit_data.std()\n",
        "\n",
        "    # ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤\n",
        "    sumaho_mean = sumaho_data.mean()\n",
        "    sumaho_std = sumaho_data.std()\n",
        "\n",
        "    results.append({\n",
        "        'class_name': class_name,\n",
        "        't_stat': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'slit_mean': slit_mean,\n",
        "        'slit_std': slit_std,\n",
        "        'sumaho_mean': sumaho_mean,\n",
        "        'sumaho_std': sumaho_std\n",
        "    })\n",
        "\n",
        "# å…¨ã‚¯ãƒ©ã‚¹ã¾ã¨ã‚ãŸå¯¾å¿œã®ã‚ã‚‹tæ¤œå®š\n",
        "all_slit_data = df_slit[layer_name].dropna().reset_index(drop=True)\n",
        "all_sumaho_data = df_sumaho[layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "# å¯¾å¿œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚‹ãŸã‚ã€æœ€å°ã®é•·ã•ã«åˆã‚ã›ã‚‹\n",
        "min_length_all = min(len(all_slit_data), len(all_sumaho_data))\n",
        "all_slit_data = all_slit_data[:min_length_all]\n",
        "all_sumaho_data = all_sumaho_data[:min_length_all]\n",
        "\n",
        "t_stat_all, p_value_all = stats.ttest_rel(all_slit_data, all_sumaho_data)\n",
        "\n",
        "# å…¨ã‚¯ãƒ©ã‚¹ã¾ã¨ã‚ãŸçµ±è¨ˆå€¤\n",
        "all_slit_mean = all_slit_data.mean()\n",
        "all_slit_std = all_slit_data.std()\n",
        "all_sumaho_mean = all_sumaho_data.mean()\n",
        "all_sumaho_std = all_sumaho_data.std()\n",
        "\n",
        "# çµæœã‚’è¿½åŠ \n",
        "results.append({\n",
        "    'class_name': 'All Classes',\n",
        "    't_stat': t_stat_all,\n",
        "    'p_value': p_value_all,\n",
        "    'slit_mean': all_slit_mean,\n",
        "    'slit_std': all_slit_std,\n",
        "    'sumaho_mean': all_sumaho_mean,\n",
        "    'sumaho_std': all_sumaho_std\n",
        "})\n",
        "\n",
        "# çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›ã—ã¦è¡¨ç¤º\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results['p_value'] = df_results['p_value'].map(lambda x: f'{x:.3f}')\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjAiQjnMYHjN"
      },
      "outputs": [],
      "source": [
        "# #ã‚¯ãƒ©ã‚¹æ¯ã®å·®ï¼ˆã‚¹ãƒãƒ›ï¼‹ã‚¹ãƒªãƒƒãƒˆï¼‰\n",
        "# from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# # ANOVAã‚’å®Ÿè¡Œ\n",
        "# anova_result = stats.f_oneway(\n",
        "#     df_combined[df_combined['class_name'] == 'infection'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'normal'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'non-infection'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'scar'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'tumor'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'deposit'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'APAC'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'lens opacity'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'bullous'][layer_name]\n",
        "# )\n",
        "\n",
        "# print(f\"ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "# # äº‹å¾Œæ¤œå®šï¼ˆTukeyã®HSDæ¤œå®šï¼‰ã‚’å®Ÿè¡Œ\n",
        "# tukey_result = pairwise_tukeyhsd(df_combined[layer_name], df_combined['class_name'])\n",
        "\n",
        "# # çµæœã‚’è¡¨ç¤º\n",
        "# print(tukey_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JihM5ooC3VCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©ï¼ˆæŒ‡å®šã•ã‚ŒãŸé †åºï¼‰\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# è§£æã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [17, 20, 23, \"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "def check_all_values_same(groups):\n",
        "    \"\"\"å…¨ã¦ã®å€¤ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "    all_values = np.concatenate([group.values for group in groups])\n",
        "    return np.all(all_values == all_values[0]), all_values[0]\n",
        "\n",
        "def dunn_test(groups, group_names):\n",
        "    \"\"\"\n",
        "    Dunn's testã®å®Ÿè£…\n",
        "    \"\"\"\n",
        "    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘\n",
        "    all_data = np.concatenate(groups)\n",
        "    ranks = stats.rankdata(all_data)\n",
        "\n",
        "    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®å¹³å‡ãƒ©ãƒ³ã‚¯ã‚’è¨ˆç®—\n",
        "    start = 0\n",
        "    mean_ranks = []\n",
        "    ns = []\n",
        "    for group in groups:\n",
        "        n = len(group)\n",
        "        group_ranks = ranks[start:start + n]\n",
        "        mean_ranks.append(np.mean(group_ranks))\n",
        "        ns.append(n)\n",
        "        start += n\n",
        "\n",
        "    # å…¨ãƒšã‚¢ã®çµ„ã¿åˆã‚ã›ã§Dunn's testã‚’å®Ÿè¡Œ\n",
        "    N = len(ranks)\n",
        "    k = len(groups)\n",
        "    comparisons = []\n",
        "    p_values = []\n",
        "\n",
        "    for i in range(k):\n",
        "        for j in range(i + 1, k):\n",
        "            # å¹³å‡ãƒ©ãƒ³ã‚¯ã®å·®\n",
        "            diff = abs(mean_ranks[i] - mean_ranks[j])\n",
        "\n",
        "            # æ¨™æº–èª¤å·®\n",
        "            se = np.sqrt((N * (N + 1) / 12) * (1/ns[i] + 1/ns[j]))\n",
        "\n",
        "            # zçµ±è¨ˆé‡\n",
        "            z = diff / se\n",
        "\n",
        "            # på€¤ï¼ˆä¸¡å´æ¤œå®šï¼‰\n",
        "            p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "\n",
        "            comparisons.append((group_names[i], group_names[j]))\n",
        "            p_values.append(p)\n",
        "\n",
        "    # Bonferroniè£œæ­£\n",
        "    rejected, p_corrected, _, _ = multipletests(p_values, method='bonferroni')\n",
        "\n",
        "    # æœ‰æ„ãªçµæœã‚’è¿”ã™\n",
        "    significant_results = []\n",
        "    for (name1, name2), p_corr, is_rej in zip(comparisons, p_corrected, rejected):\n",
        "        if is_rej:  # p < 0.05 after correction\n",
        "            significant_results.append({\n",
        "                'group1': name1,\n",
        "                'group2': name2,\n",
        "                'p_value': p_corr\n",
        "            })\n",
        "\n",
        "    return significant_results\n",
        "\n",
        "for layer in layers:\n",
        "    # ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’è¨­å®š\n",
        "    if isinstance(layer, int):\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "    else:\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"\\nAnalysis for {layer_name}\")\n",
        "    print('='*80)\n",
        "\n",
        "    # æ¬ æå€¤ã‚’é™¤å¤–ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
        "    data_for_analysis = df_slit[[layer_name, 'class_name']].dropna()\n",
        "\n",
        "    if len(data_for_analysis) > 0:\n",
        "        # ã‚¯ãƒ©ã‚¹ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆæ¬ æå€¤ã‚’é™¤å¤–ï¼‰\n",
        "        class_groups = []\n",
        "        valid_class_names = []\n",
        "\n",
        "        for class_name in class_names:\n",
        "            group_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna()\n",
        "            if len(group_data) > 0:\n",
        "                class_groups.append(group_data)\n",
        "                valid_class_names.append(class_name)\n",
        "\n",
        "        if len(class_groups) > 1:  # å°‘ãªãã¨ã‚‚2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦\n",
        "            # å…¨ã¦ã®å€¤ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯\n",
        "            is_all_same, same_value = check_all_values_same(class_groups)\n",
        "\n",
        "            if is_all_same:\n",
        "                print(f\"\\nå…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã§åŒã˜å€¤: {same_value:.4f}\")\n",
        "                print(\"çµ±è¨ˆçš„æ¤œå®šã¯ä¸è¦ã§ã™ã€‚\")\n",
        "                continue\n",
        "\n",
        "            # Kruskal-Wallis H-testã‚’å®Ÿè¡Œ\n",
        "            h_statistic, p_value = stats.kruskal(*class_groups)\n",
        "\n",
        "            print(f\"\\nKruskal-Wallis H-testçµæœ:\")\n",
        "            print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                # Dunn's testã‚’å®Ÿè¡Œ\n",
        "                significant_pairs = dunn_test(class_groups, valid_class_names)\n",
        "\n",
        "                if significant_pairs:\n",
        "                    print(\"\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (Bonferroniè£œæ­£å¾Œ p < 0.05):\")\n",
        "                    for result in significant_pairs:\n",
        "                        print(f\"{result['group1']:25} vs {result['group2']:25}: \"\n",
        "                              f\"p = {result['p_value']:.4e}\")\n",
        "        else:\n",
        "            print(f\"\\nè­¦å‘Š: {layer_name}ã®è§£æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    else:\n",
        "        print(f\"\\nè­¦å‘Š: {layer_name}ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")"
      ],
      "metadata": {
        "id": "9mmq-hNs_4Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©ï¼ˆæŒ‡å®šã•ã‚ŒãŸé †åºï¼‰\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# è§£æã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [17, 20, 23, \"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "def check_all_values_same(groups):\n",
        "    \"\"\"å…¨ã¦ã®å€¤ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "    all_values = np.concatenate([group.values for group in groups])\n",
        "    return np.all(all_values == all_values[0]), all_values[0]\n",
        "\n",
        "def perform_pairwise_mannwhitney(groups, group_names, alpha=0.05):\n",
        "    \"\"\"ãƒšã‚¢ãƒ¯ã‚¤ã‚ºMann-Whitney Uæ¤œå®šã‚’å®Ÿè¡Œ\"\"\"\n",
        "    n_groups = len(groups)\n",
        "    results = []\n",
        "\n",
        "    for i in range(n_groups):\n",
        "        for j in range(i+1, n_groups):\n",
        "            try:\n",
        "                stat, p_value = mannwhitneyu(groups[i], groups[j], alternative='two-sided')\n",
        "                n1, n2 = len(groups[i]), len(groups[j])\n",
        "                effect_size = 1 - (2 * stat) / (n1 * n2)  # Common language effect size\n",
        "\n",
        "                if p_value < alpha:\n",
        "                    results.append({\n",
        "                        'group1': group_names[i],\n",
        "                        'group2': group_names[j],\n",
        "                        'p_value': p_value,\n",
        "                        'effect_size': effect_size\n",
        "                    })\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    return results\n",
        "\n",
        "for layer in layers:\n",
        "    # ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’è¨­å®š\n",
        "    if isinstance(layer, int):\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "    else:\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"\\nAnalysis for {layer_name}\")\n",
        "    print('='*80)\n",
        "\n",
        "    # æ¬ æå€¤ã‚’é™¤å¤–ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
        "    data_for_analysis = df_slit[[layer_name, 'class_name']].dropna()\n",
        "\n",
        "    if len(data_for_analysis) > 0:\n",
        "        # ã‚¯ãƒ©ã‚¹ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆæ¬ æå€¤ã‚’é™¤å¤–ï¼‰\n",
        "        class_groups = []\n",
        "        valid_class_names = []\n",
        "\n",
        "        for class_name in class_names:\n",
        "            group_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna()\n",
        "            if len(group_data) > 0:\n",
        "                class_groups.append(group_data)\n",
        "                valid_class_names.append(class_name)\n",
        "\n",
        "        if len(class_groups) > 1:  # å°‘ãªãã¨ã‚‚2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦\n",
        "            # å…¨ã¦ã®å€¤ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯\n",
        "            is_all_same, same_value = check_all_values_same(class_groups)\n",
        "\n",
        "            if is_all_same:\n",
        "                print(f\"\\nå…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã§åŒã˜å€¤: {same_value:.4f}\")\n",
        "                print(\"çµ±è¨ˆçš„æ¤œå®šã¯ä¸è¦ã§ã™ã€‚\")\n",
        "                continue\n",
        "\n",
        "            # Kruskal-Wallis H-testã‚’å®Ÿè¡Œ\n",
        "            h_statistic, p_value = stats.kruskal(*class_groups)\n",
        "\n",
        "            print(f\"\\nKruskal-Wallis H-testçµæœ:\")\n",
        "            print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºMann-Whitney Uæ¤œå®šã‚’å®Ÿè¡Œ\n",
        "                significant_pairs = perform_pairwise_mannwhitney(\n",
        "                    class_groups,\n",
        "                    valid_class_names\n",
        "                )\n",
        "\n",
        "                if significant_pairs:\n",
        "                    print(\"\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):\")\n",
        "                    for result in significant_pairs:\n",
        "                        print(f\"{result['group1']:25} vs {result['group2']:25}: \"\n",
        "                              f\"p = {result['p_value']:.4e}, effect size = {result['effect_size']:.4f}\")\n",
        "        else:\n",
        "            print(f\"\\nè­¦å‘Š: {layer_name}ã®è§£æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    else:\n",
        "        print(f\"\\nè­¦å‘Š: {layer_name}ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NShTeYYE3VEr",
        "outputId": "e35d081f-480c-4c59-d3a1-b0a860b0d097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer17\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-testçµæœ:\n",
            "H-statistic: 17.9227\n",
            "p-value: 2.1813e-02\n",
            "\n",
            "æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):\n",
            "Infectious keratitis      vs Scar                     : p = 2.2595e-03, effect size = -0.4349\n",
            "Infectious keratitis      vs Tumor                    : p = 4.3770e-04, effect size = -0.5172\n",
            "Infectious keratitis      vs Deposit                  : p = 3.4077e-02, effect size = -0.3068\n",
            "Infectious keratitis      vs APAC                     : p = 3.9997e-02, effect size = -0.4545\n",
            "Infectious keratitis      vs Lens opacity             : p = 3.8009e-04, effect size = -0.5981\n",
            "Infectious keratitis      vs Bullous keratopathy      : p = 7.7172e-03, effect size = -0.4079\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer20\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-testçµæœ:\n",
            "H-statistic: 14.8646\n",
            "p-value: 6.1833e-02\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer23\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-testçµæœ:\n",
            "H-statistic: 98.2819\n",
            "p-value: 9.5787e-18\n",
            "\n",
            "æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):\n",
            "Normal                    vs Infectious keratitis     : p = 2.0957e-03, effect size = -0.4487\n",
            "Normal                    vs Non-infection keratitis  : p = 4.6782e-04, effect size = 0.6774\n",
            "Normal                    vs Scar                     : p = 6.4358e-06, effect size = 0.6528\n",
            "Normal                    vs Tumor                    : p = 9.5263e-05, effect size = 0.5828\n",
            "Normal                    vs APAC                     : p = 4.3688e-03, effect size = -0.6344\n",
            "Normal                    vs Lens opacity             : p = 1.1138e-02, effect size = 0.4329\n",
            "Infectious keratitis      vs Non-infection keratitis  : p = 4.0088e-06, effect size = 0.8834\n",
            "Infectious keratitis      vs Scar                     : p = 1.1665e-07, effect size = 0.7540\n",
            "Infectious keratitis      vs Tumor                    : p = 1.0299e-05, effect size = 0.6485\n",
            "Infectious keratitis      vs Deposit                  : p = 1.9169e-02, effect size = 0.3390\n",
            "Infectious keratitis      vs Lens opacity             : p = 5.5807e-06, effect size = 0.7640\n",
            "Infectious keratitis      vs Bullous keratopathy      : p = 4.4250e-05, effect size = 0.6247\n",
            "Non-infection keratitis   vs Deposit                  : p = 1.2420e-05, effect size = -0.8413\n",
            "Non-infection keratitis   vs APAC                     : p = 1.8356e-04, effect size = -0.9658\n",
            "Non-infection keratitis   vs Lens opacity             : p = 5.7299e-03, effect size = -0.5870\n",
            "Non-infection keratitis   vs Bullous keratopathy      : p = 6.4636e-04, effect size = -0.6805\n",
            "Scar                      vs Deposit                  : p = 1.2680e-08, effect size = -0.8162\n",
            "Scar                      vs APAC                     : p = 3.5528e-05, effect size = -0.9085\n",
            "Scar                      vs Lens opacity             : p = 6.2173e-04, effect size = -0.5728\n",
            "Scar                      vs Bullous keratopathy      : p = 1.7963e-05, effect size = -0.6516\n",
            "Tumor                     vs Deposit                  : p = 1.1342e-06, effect size = -0.7208\n",
            "Tumor                     vs APAC                     : p = 3.8497e-04, effect size = -0.7926\n",
            "Tumor                     vs Lens opacity             : p = 9.8957e-04, effect size = -0.5649\n",
            "Tumor                     vs Bullous keratopathy      : p = 7.7762e-05, effect size = -0.6179\n",
            "Deposit                   vs APAC                     : p = 2.2393e-02, effect size = -0.5069\n",
            "Deposit                   vs Lens opacity             : p = 7.9724e-05, effect size = 0.6678\n",
            "Deposit                   vs Bullous keratopathy      : p = 2.3572e-03, effect size = 0.4688\n",
            "APAC                      vs Lens opacity             : p = 8.3075e-05, effect size = 0.9415\n",
            "APAC                      vs Bullous keratopathy      : p = 2.3333e-04, effect size = 0.8376\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_0\n",
            "================================================================================\n",
            "\n",
            "å…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã§åŒã˜å€¤: 0.0000\n",
            "çµ±è¨ˆçš„æ¤œå®šã¯ä¸è¦ã§ã™ã€‚\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_1\n",
            "================================================================================\n",
            "\n",
            "å…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã§åŒã˜å€¤: 0.0000\n",
            "çµ±è¨ˆçš„æ¤œå®šã¯ä¸è¦ã§ã™ã€‚\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_2\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-testçµæœ:\n",
            "H-statistic: 84.7932\n",
            "p-value: 5.2761e-15\n",
            "\n",
            "æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):\n",
            "Normal                    vs Infectious keratitis     : p = 2.3201e-05, effect size = -0.6168\n",
            "Normal                    vs Non-infection keratitis  : p = 3.6507e-03, effect size = 0.5633\n",
            "Normal                    vs Scar                     : p = 2.2951e-04, effect size = 0.5332\n",
            "Normal                    vs Tumor                    : p = 2.4453e-05, effect size = 0.6301\n",
            "Normal                    vs Deposit                  : p = 4.3993e-02, effect size = -0.2964\n",
            "Normal                    vs Lens opacity             : p = 3.4122e-02, effect size = 0.3616\n",
            "Normal                    vs Bullous keratopathy      : p = 4.4571e-03, effect size = 0.4417\n",
            "Infectious keratitis      vs Non-infection keratitis  : p = 3.7397e-05, effect size = 0.7902\n",
            "Infectious keratitis      vs Scar                     : p = 8.3081e-10, effect size = 0.8734\n",
            "Infectious keratitis      vs Tumor                    : p = 2.7847e-06, effect size = 0.6889\n",
            "Infectious keratitis      vs Deposit                  : p = 2.9880e-02, effect size = 0.3144\n",
            "Infectious keratitis      vs APAC                     : p = 1.0952e-02, effect size = 0.5623\n",
            "Infectious keratitis      vs Lens opacity             : p = 1.0401e-05, effect size = 0.7416\n",
            "Infectious keratitis      vs Bullous keratopathy      : p = 3.6722e-10, effect size = 0.9580\n",
            "Non-infection keratitis   vs Deposit                  : p = 4.3364e-04, effect size = -0.6779\n",
            "Scar                      vs Tumor                    : p = 4.2495e-03, effect size = 0.4176\n",
            "Scar                      vs Deposit                  : p = 1.0491e-06, effect size = -0.7004\n",
            "Tumor                     vs Deposit                  : p = 6.3253e-06, effect size = -0.6687\n",
            "Tumor                     vs APAC                     : p = 4.3709e-02, effect size = -0.4519\n",
            "Tumor                     vs Lens opacity             : p = 2.8844e-02, effect size = -0.3754\n",
            "Tumor                     vs Bullous keratopathy      : p = 1.3958e-03, effect size = -0.5000\n",
            "Deposit                   vs APAC                     : p = 3.6195e-02, effect size = 0.4653\n",
            "Deposit                   vs Lens opacity             : p = 2.9648e-03, effect size = 0.5033\n",
            "Deposit                   vs Bullous keratopathy      : p = 1.6502e-05, effect size = 0.6635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# è§£æã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’é•·å½¢å¼ã«å¤‰æ›\n",
        "df_melted = pd.melt(df[layers], var_name='Layer', value_name='AOI Value')\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’ç°¡ç•¥åŒ–\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "df_melted['Layer'] = df_melted['Layer'].map(layer_mapping)\n",
        "\n",
        "# æ¬ æå€¤ã‚’é™¤å¤–\n",
        "df_melted = df_melted.dropna()\n",
        "\n",
        "# Kruskal-Wallis H-testã‚’å®Ÿè¡Œ\n",
        "groups = [group['AOI Value'].values for name, group in df_melted.groupby('Layer')]\n",
        "h_statistic, p_value = stats.kruskal(*groups)\n",
        "\n",
        "print(\"Kruskal-Wallis H-testçµæœ:\")\n",
        "print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒï¼ˆMann-Whitney U testï¼‰ã‚’å®Ÿè¡Œ\n",
        "layer_names = sorted(df_melted['Layer'].unique())\n",
        "significant_pairs = []\n",
        "\n",
        "print(\"\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):\")\n",
        "for i, layer1 in enumerate(layer_names):\n",
        "    for layer2 in layer_names[i+1:]:\n",
        "        group1 = df_melted[df_melted['Layer'] == layer1]['AOI Value']\n",
        "        group2 = df_melted[df_melted['Layer'] == layer2]['AOI Value']\n",
        "\n",
        "        try:\n",
        "            stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
        "            n1, n2 = len(group1), len(group2)\n",
        "            effect_size = 1 - (2 * stat) / (n1 * n2)  # Common language effect size\n",
        "\n",
        "            if p_value < 0.05:  # Bonferroniè£œæ­£ã‚’é©ç”¨ã™ã‚‹å ´åˆã¯ 0.05/len(pairs) ã‚’ä½¿ç”¨\n",
        "                print(f\"{layer1:10} vs {layer2:10}: p = {p_value:.4e}, effect size = {effect_size:.4f}\")\n",
        "                significant_pairs.append((layer1, layer2, p_value, effect_size))\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "# å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åŸºæœ¬çµ±è¨ˆé‡ï¼ˆä¸­å¤®å€¤ã¨å››åˆ†ä½æ•°ã‚’å«ã‚€ï¼‰\n",
        "print(\"\\nå„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åŸºæœ¬çµ±è¨ˆé‡:\")\n",
        "stats_summary = df_melted.groupby('Layer')['AOI Value'].agg([\n",
        "    ('count', 'count'),\n",
        "    ('median', 'median'),\n",
        "    ('mean', 'mean'),\n",
        "    ('std', 'std'),\n",
        "    ('Q1', lambda x: x.quantile(0.25)),\n",
        "    ('Q3', lambda x: x.quantile(0.75))\n",
        "]).round(4)\n",
        "\n",
        "print(stats_summary)\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å½¢å¼ã§æœ‰æ„å·®ã‚’è¡¨ç¤º\n",
        "print(\"\\næœ‰æ„å·®ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ (â˜…: p < 0.05)\")\n",
        "matrix = pd.DataFrame(index=layer_names, columns=layer_names)\n",
        "np.fill_diagonal(matrix.values, '-')\n",
        "matrix = matrix.fillna('ã€€')\n",
        "\n",
        "for pair in significant_pairs:\n",
        "    matrix.loc[pair[0], pair[1]] = \"â˜…\"\n",
        "    matrix.loc[pair[1], pair[0]] = \"â˜…\"\n",
        "\n",
        "print(matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJkb_wra3VGX",
        "outputId": "462b1142-f699-4bdc-a5b3-2ec012506bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kruskal-Wallis H-testçµæœ:\n",
            "H-statistic: 1075.0618\n",
            "p-value: 3.3610e-230\n",
            "\n",
            "æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):\n",
            "Layer 17   vs Layer 20  : p = 1.7921e-09, effect size = -0.3264\n",
            "Layer 17   vs Layer 23  : p = 1.3526e-20, effect size = -0.5048\n",
            "Layer 17   vs Layer 24_0: p = 1.5463e-83, effect size = -0.9780\n",
            "Layer 17   vs Layer 24_1: p = 1.5463e-83, effect size = -0.9780\n",
            "Layer 17   vs Layer 24_2: p = 5.2958e-44, effect size = -0.7548\n",
            "Layer 20   vs Layer 23  : p = 3.5406e-09, effect size = -0.3203\n",
            "Layer 20   vs Layer 24_0: p = 1.5463e-83, effect size = -0.9780\n",
            "Layer 20   vs Layer 24_1: p = 1.5463e-83, effect size = -0.9780\n",
            "Layer 20   vs Layer 24_2: p = 1.4529e-46, effect size = -0.7774\n",
            "Layer 23   vs Layer 24_0: p = 1.9684e-86, effect size = -1.0000\n",
            "Layer 23   vs Layer 24_1: p = 1.9684e-86, effect size = -1.0000\n",
            "Layer 23   vs Layer 24_2: p = 7.0350e-40, effect size = -0.7171\n",
            "Layer 24_0 vs Layer 24_2: p = 5.7682e-83, effect size = 0.9736\n",
            "Layer 24_1 vs Layer 24_2: p = 5.7682e-83, effect size = 0.9736\n",
            "\n",
            "å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åŸºæœ¬çµ±è¨ˆé‡:\n",
            "            count  median    mean     std      Q1      Q3\n",
            "Layer                                                    \n",
            "Layer 17      227  0.4042  0.3627  0.2008  0.2019  0.5234\n",
            "Layer 20      227  0.2569  0.2629  0.1501  0.1512  0.3702\n",
            "Layer 23      227  0.1524  0.1851  0.1189  0.1004  0.2474\n",
            "Layer 24_0    227  0.0000  0.0000  0.0000  0.0000  0.0000\n",
            "Layer 24_1    227  0.0000  0.0000  0.0000  0.0000  0.0000\n",
            "Layer 24_2    227  0.0543  0.0643  0.0481  0.0318  0.0828\n",
            "\n",
            "æœ‰æ„å·®ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ (â˜…: p < 0.05)\n",
            "           Layer 17 Layer 20 Layer 23 Layer 24_0 Layer 24_1 Layer 24_2\n",
            "Layer 17          -        â˜…        â˜…          â˜…          â˜…          â˜…\n",
            "Layer 20          â˜…        -        â˜…          â˜…          â˜…          â˜…\n",
            "Layer 23          â˜…        â˜…        -          â˜…          â˜…          â˜…\n",
            "Layer 24_0        â˜…        â˜…        â˜…          -          ã€€          â˜…\n",
            "Layer 24_1        â˜…        â˜…        â˜…          ã€€          -          â˜…\n",
            "Layer 24_2        â˜…        â˜…        â˜…          â˜…          â˜…          -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59zc1fuTqBK7"
      },
      "source": [
        "###**ANOVA_heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exil8PT2ZzsS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# äº‹å¾Œæ¤œå®šï¼ˆTukeyã®HSDæ¤œå®šï¼‰ã‚’å®Ÿè¡Œ\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# çµæœã‚’DataFrameã«å¤‰æ›\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å›³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã‚¯ãƒ©ã‚¹åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã«på€¤ã‚’å…¥åŠ›\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# åŒã˜ã‚¯ãƒ©ã‚¹é–“ã®ã‚»ãƒ«ã‚’NaNã«è¨­å®š\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'})\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6M5cbfdFlv3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# äº‹å¾Œæ¤œå®šï¼ˆTukeyã®HSDæ¤œå®šï¼‰ã‚’å®Ÿè¡Œ\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# çµæœã‚’DataFrameã«å¤‰æ›\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å›³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã‚¯ãƒ©ã‚¹åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã«på€¤ã‚’å…¥åŠ›\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# åŒã˜ã‚¯ãƒ©ã‚¹é–“ã®ã‚»ãƒ«ã‚’NaNã«è¨­å®š\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’ä½œæˆï¼ˆã‚ªãƒ¬ãƒ³ã‚¸ã€ã‚°ãƒ¬ãƒ¼ã€ç™½ï¼‰\n",
        "colors = ['#FFA500', '#D3D3D3', 'white']  # ã‚ªãƒ¬ãƒ³ã‚¸ã€ã‚°ãƒ¬ãƒ¼ã€ç™½\n",
        "custom_cmap = ListedColormap(colors)\n",
        "\n",
        "# på€¤ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆNaNã¯2ã€p<0.05ã¯0ã€ãã‚Œä»¥å¤–ã¯1ï¼‰\n",
        "color_data = np.where(np.isnan(matrix), 2, np.where(matrix < 0.05, 0, 1))\n",
        "\n",
        "# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(color_data,\n",
        "            annot=matrix,  # å…ƒã®på€¤ã‚’è¡¨ç¤º\n",
        "            fmt=\".3f\",\n",
        "            cmap=custom_cmap,\n",
        "            cbar=False,  # ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã‚’éè¡¨ç¤ºã«\n",
        "            xticklabels=class_names,  # xè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’ã‚¯ãƒ©ã‚¹åã«è¨­å®š\n",
        "            yticklabels=class_names)  # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’ã‚¯ãƒ©ã‚¹åã«è¨­å®š\n",
        "\n",
        "# ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã‚’è¿½åŠ \n",
        "sm = plt.cm.ScalarMappable(cmap=ListedColormap(colors[:2]), norm=plt.Normalize(vmin=0, vmax=1))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, label='p-value', ticks=[0.25, 0.75])\n",
        "cbar.set_ticklabels(['p < 0.05', 'p â‰¥ 0.05'])\n",
        "\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.xticks(rotation=45, ha='right')  # xè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’45åº¦å›è»¢\n",
        "plt.yticks(rotation=0)  # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’æ°´å¹³ã«\n",
        "plt.tight_layout()  # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’èª¿æ•´\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbxW3P2OqFTb"
      },
      "source": [
        "###**Tukey_sumaho/slitåˆ¥**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m7Z9FCzbSmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "def perform_anova_and_tukey(df, title):\n",
        "    # æ¬ æå€¤ã‚’å‰Šé™¤\n",
        "    df = df.dropna(subset=[layer_name])\n",
        "\n",
        "    # ANOVAã‚’å®Ÿè¡Œ\n",
        "    anova_result = stats.f_oneway(\n",
        "        df[df['class_name'] == 'infection'][layer_name],\n",
        "        df[df['class_name'] == 'normal'][layer_name],\n",
        "        df[df['class_name'] == 'non-infection'][layer_name],\n",
        "        df[df['class_name'] == 'scar'][layer_name],\n",
        "        df[df['class_name'] == 'tumor'][layer_name],\n",
        "        df[df['class_name'] == 'deposit'][layer_name],\n",
        "        df[df['class_name'] == 'APAC'][layer_name],\n",
        "        df[df['class_name'] == 'lens opacity'][layer_name],\n",
        "        df[df['class_name'] == 'bullous'][layer_name]\n",
        "    )\n",
        "\n",
        "    print(f\"{title} ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "    # äº‹å¾Œæ¤œå®šï¼ˆTukeyã®HSDæ¤œå®šï¼‰ã‚’å®Ÿè¡Œ\n",
        "    tukey_result = pairwise_tukeyhsd(endog=df[layer_name], groups=df['class_name'])\n",
        "\n",
        "    # çµæœã‚’DataFrameã«å¤‰æ›\n",
        "    tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "    # ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å›³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã‚¯ãƒ©ã‚¹åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨\n",
        "    matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "    # ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã«på€¤ã‚’å…¥åŠ›\n",
        "    for i in range(len(tukey_result_df)):\n",
        "        group1 = tukey_result_df.loc[i, 'group1']\n",
        "        group2 = tukey_result_df.loc[i, 'group2']\n",
        "        p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "        matrix.loc[group1, group2] = p_value\n",
        "        matrix.loc[group2, group1] = p_value\n",
        "\n",
        "    # åŒã˜ã‚¯ãƒ©ã‚¹é–“ã®ã‚»ãƒ«ã‚’NaNã«è¨­å®š\n",
        "    np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'}, mask=matrix.isnull())\n",
        "    plt.title(f'Pairwise Comparison P-Values (Tukey HSD) - {title}')\n",
        "    plt.show()\n",
        "\n",
        "# ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã®è§£æ\n",
        "perform_anova_and_tukey(df_slit, \"Slit Lamp Data\")\n",
        "\n",
        "# ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãƒ‡ãƒ¼ã‚¿ã®è§£æ\n",
        "perform_anova_and_tukey(df_sumaho, \"Smartphone Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ubPn9Erqnid",
        "outputId": "a4e19c97-a3c2-44cd-9dac-99c4b7acdb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t-statistic: 5.652411183767835, p-value: 2.8095252129652275e-08\n",
            "scar + non-infection mean: 0.11625931718387256, std: 0.08804314854287512\n",
            "others mean: 0.07454914396969002, std: 0.05818428866497734\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ\n",
        "df_combined = pd.concat([df_slit, df_sumaho]).reset_index(drop=True)\n",
        "\n",
        "# \"scar\" + \"non-infection\" ã‚°ãƒ«ãƒ¼ãƒ—ã¨ãã®ä»–ã®ã‚¯ãƒ©ã‚¹ã«åˆ†é¡\n",
        "df_combined['group'] = df_combined['class_name'].apply(lambda x: 'scar + non-infection' if x in ['scar', 'non-infection'] else 'others')\n",
        "\n",
        "# ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "scar_non_infection_group = df_combined[df_combined['group'] == 'scar + non-infection'][layer_name].dropna()\n",
        "others_group = df_combined[df_combined['group'] == 'others'][layer_name].dropna()\n",
        "\n",
        "# tæ¤œå®šã‚’å®Ÿè¡Œ\n",
        "t_stat, p_value = stats.ttest_ind(scar_non_infection_group, others_group)\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
        "\n",
        "# çµ±è¨ˆå€¤ã‚’è¨ˆç®—\n",
        "scar_non_infection_mean = scar_non_infection_group.mean()\n",
        "scar_non_infection_std = scar_non_infection_group.std()\n",
        "others_mean = others_group.mean()\n",
        "others_std = others_group.std()\n",
        "\n",
        "print(f\"scar + non-infection mean: {scar_non_infection_mean}, std: {scar_non_infection_std}\")\n",
        "print(f\"others mean: {others_mean}, std: {others_std}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcxqbJ_GrbPY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNXGYjBItupIfeUTO0HP0wq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}