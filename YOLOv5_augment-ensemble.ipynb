{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWdNwWpfptaJRvRpEVe601",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_augment-ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 inference_augment-ensemble**"
      ],
      "metadata": {
        "id": "Un512TpLoNtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup"
      ],
      "metadata": {
        "id": "k2ciP17pzhrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F7fjEQUV-pd",
        "outputId": "f714320c-eb3e-4f0a-b14c-5f9f94bf9019"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYRY9egwjuIs",
        "outputId": "4689bdd3-5c8b-480c-8d35-8d31e30d45b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "opencv-python-headless>=4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "# torch>=1.7.0\n",
        "# torchvision>=0.8.1\n",
        "tqdm>=4.41.0\n",
        "\n",
        "# Logging -------------------------------------\n",
        "##tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "##seaborn>=0.11.0\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUeMX07NqirS",
        "outputId": "5afae9e5-dc4f-4a87-86d8-5e22cf4f9aae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Tfb6NYZIBGm1",
        "outputId": "80767314-1ccf-4cc0-ffa2-8700323e97cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.6.0.66 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.9.0.80)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (4.66.4)\n",
            "Collecting thop (from -r requirements.txt (line 37))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop->-r requirements.txt (line 37)) (2.3.0+cu121)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->thop->-r requirements.txt (line 37))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->thop->-r requirements.txt (line 37))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop->-r requirements.txt (line 37)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop->-r requirements.txt (line 37)) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#最新バージョンでも動くので削除\n",
        "# !pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install torchvision==0.11.2+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "ESI_x2upsdf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "num = 5\n",
        "img_dir = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data\"\n",
        "img = glob.glob(f\"{img_dir}/*\")[num]\n",
        "img\n",
        "\n",
        "img = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg\""
      ],
      "metadata": {
        "id": "Bnz_lfT_l7NM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "BB9E6zqwD0Py",
        "outputId": "f7a82b96-0193-42e5-81b3-d12b40e8354c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple inference\n",
        "model = DetectMultiBackend(weight_path, device=\"cpu\", dnn=False)\n",
        "top_classes = inference_top3(img, model)\n",
        "print(top_classes)"
      ],
      "metadata": {
        "id": "7UmC2m6cE160"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_classes[0][0]"
      ],
      "metadata": {
        "id": "dtxWJRzvJR_0",
        "outputId": "a4d383f0-7903-4258-da2e-6d06fc86b8a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference Maehara's 100 questions**\n",
        "画像データ\n",
        "\n",
        "sumaho: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\n",
        "\n",
        "slit: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_sumaho\n",
        "\n",
        "結果CSV: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv"
      ],
      "metadata": {
        "id": "IAcFpqSIMyKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir_sumaho = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\"\n",
        "img_dir_slit = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_sumaho\"\n",
        "csv_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\""
      ],
      "metadata": {
        "id": "FzGFtY4YW0YJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルをPandas DataFrameとして読み込む\n",
        "df = pd.read_csv(csv_dir)\n",
        "\n",
        "# データフレームの内容を表示\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV4NdejfW0dV",
        "outputId": "090fa615-a950-4e7e-f248-fbd69b8d287c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   serial_number  basename disease_English  class_num  top1  ...  top2  \\\n",
            "0              1  slit_001           tumor          4   NaN  ...   NaN   \n",
            "1              2  slit_002            scar          3   NaN  ...   NaN   \n",
            "2              3  slit_005       infection          0   NaN  ...   NaN   \n",
            "3              4  slit_007    lens-opacity          7   NaN  ...   NaN   \n",
            "4              5  slit_008          normal          1   NaN  ...   NaN   \n",
            "\n",
            "   top2_prob  top3  top3_prob  Unnamed: 9  \n",
            "0        NaN   NaN        NaN         NaN  \n",
            "1        NaN   NaN        NaN         NaN  \n",
            "2        NaN   NaN        NaN         NaN  \n",
            "3        NaN   NaN        NaN         NaN  \n",
            "4        NaN   NaN        NaN         NaN  \n",
            "\n",
            "[5 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import v2 as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "\n",
        "augment_list = [None, \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\", \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\",\"saturation\"]\n",
        "\n",
        "def inference_augment(img_path, model, augment):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "    img_tensor /= 255\n",
        "\n",
        "    # Convert to uint8 if required for JPEG compression\n",
        "    if augment == \"jpegcompression\":\n",
        "        img_tensor = (img_tensor * 255).byte()\n",
        "\n",
        "    # Apply augmentation based on the 'augment' argument\n",
        "    if augment == \"horizontalflip\":\n",
        "        img_tensor = T.functional.hflip(img_tensor)\n",
        "    elif augment == \"grayscale\":\n",
        "        img_tensor = T.functional.rgb_to_grayscale(img_tensor, num_output_channels=3)\n",
        "    elif augment == \"autocontrast\":\n",
        "        img_tensor = T.functional.autocontrast(img_tensor)\n",
        "    elif augment == \"rotate\":\n",
        "        img_tensor = T.functional.rotate(img_tensor, 10)\n",
        "    elif augment == \"blur\":\n",
        "        img_tensor = T.functional.gaussian_blur(img_tensor, kernel_size=(5, 9), sigma=(0.1, 5))\n",
        "    elif augment == \"jpegcompression\":\n",
        "        img_tensor = T.functional.jpeg(img_tensor, quality=30)\n",
        "        img_tensor = img_tensor.float() / 255  # Convert back to float\n",
        "    elif augment == \"addnoise\":\n",
        "        noise = torch.randn(img_tensor.size()) * 0.1\n",
        "        img_tensor = img_tensor + noise\n",
        "        img_tensor = torch.clamp(img_tensor, 0, 1)\n",
        "    elif augment == \"clahe\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.CLAHE(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"sharpen\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.Sharpen(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"gamma\":\n",
        "        img_tensor = T.functional.adjust_gamma(img_tensor, gamma=1.8, gain=1.0)\n",
        "    elif augment == \"colorjitter\":\n",
        "        color_jitter = T.ColorJitter(brightness=0, contrast=0, saturation=1, hue=0)\n",
        "        img_tensor = color_jitter(img_tensor)\n",
        "    elif augment == \"tosepia\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.ToSepia(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"saturation\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.HueSaturationValue(hue_shift_limit=0, sat_shift_limit=70, val_shift_limit=0, p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "\n",
        "\n",
        "    img_augmented = img_tensor.numpy()\n",
        "    img_augmented = img_augmented[::-1].transpose((1, 2, 0))\n",
        "    img_augmented = (img_augmented * 255).astype(np.uint8)\n",
        "\n",
        "    # show images\n",
        "\n",
        "    # img_mpl_augmented = cv2.cvtColor(img_augmented, cv2.COLOR_BGR2RGB)\n",
        "    # plt.figure(figsize=(4, 4))\n",
        "    # plt.imshow(img_mpl_augmented)\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None, multi_label=True, max_det=1000)\n",
        "\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(min(8, len(pred[0])))]\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    print(f\"Top 3 Classes and Likelihoods (augment: {augment})\")\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "O-dW3spkKldb"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample inference\n",
        "img = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit/slit_090.jpg\"\n",
        "csv_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\"\n",
        "\n",
        "augment_list = [None, \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\", \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\",\"saturation\"]\n",
        "\n",
        "model = DetectMultiBackend(weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "for augment in augment_list:\n",
        "    top_classes = inference_augment(img, model, augment=augment)\n",
        "    print(top_classes)\n"
      ],
      "metadata": {
        "id": "Ns8PmWL9HGm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_dir)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MkzwKvfVlDU5",
        "outputId": "abb4d58d-b2b5-428e-9722-38c7992bd0a7"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   serial_number  basename disease_English  class_num\n",
              "0              1  slit_001           tumor          4\n",
              "1              2  slit_002            scar          3\n",
              "2              3  slit_005       infection          0\n",
              "3              4  slit_007    lens-opacity          7\n",
              "4              5  slit_008          normal          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a23c9796-6e0e-4248-a123-13669c9d6329\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>serial_number</th>\n",
              "      <th>basename</th>\n",
              "      <th>disease_English</th>\n",
              "      <th>class_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>slit_001</td>\n",
              "      <td>tumor</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>slit_002</td>\n",
              "      <td>scar</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>slit_005</td>\n",
              "      <td>infection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>slit_007</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>slit_008</td>\n",
              "      <td>normal</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a23c9796-6e0e-4248-a123-13669c9d6329')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a23c9796-6e0e-4248-a123-13669c9d6329 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a23c9796-6e0e-4248-a123-13669c9d6329');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a16b415-eedf-47d0-bea4-bccd319addbb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a16b415-eedf-47d0-bea4-bccd319addbb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a16b415-eedf-47d0-bea4-bccd319addbb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"serial_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 1,\n        \"max\": 100,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          84,\n          54,\n          71\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"basename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"sumaho_069\",\n          \"sumaho_007\",\n          \"sumaho_048\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disease_English\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"deposit\",\n          \"scar\",\n          \"bullous\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          5,\n          3,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "img_dir_sumaho = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\"\n",
        "img_dir_slit = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_sumaho\"\n",
        "csv_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\"\n",
        "\n",
        "# open csv\n",
        "df = pd.read_csv(csv_dir)\n",
        "\n",
        "# List all files in the given directories without extensions\n",
        "files_sumaho = [os.path.splitext(file)[0] for file in os.listdir(img_dir_sumaho)]\n",
        "files_slit = [os.path.splitext(file)[0] for file in os.listdir(img_dir_slit)]\n",
        "\n",
        "# set model for inference\n",
        "model = DetectMultiBackend(weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "# set augment\n",
        "augment_list = [None, \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\", \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\",\"saturation\"]\n",
        "\n",
        "# Check each basename in the dataframe to see if it matches any files in the directories\n",
        "for row, basename in enumerate(df[\"basename\"]):\n",
        "    if basename in files_sumaho:\n",
        "        file_name = os.path.join(img_dir_sumaho, basename + \".jpg\")\n",
        "        print(f\"groundtruth: {df['class_num'][row]}\")\n",
        "        for augment in augment_list:\n",
        "            top1 = inference_augment(file_name, model, augment)\n",
        "            top1_class, top1_prob = int(top1[0][0]), top1[0][1]\n",
        "            augment_name = \"original\" if augment is None else augment\n",
        "            df.at[row, augment_name] = top1_class\n",
        "            df.at[row, augment_name + \"_prob\"] = top1_prob\n",
        "    elif basename in files_slit:\n",
        "        file_name = os.path.join(img_dir_slit, basename + \".jpg\")\n",
        "        print(f\"groundtruth: {df['class_num'][row]}\")\n",
        "        for augment in augment_list:\n",
        "            top1 = inference_augment(file_name, model, augment)\n",
        "            top1_class, top1_prob = int(top1[0][0]), top1[0][1]\n",
        "            augment_name = \"original\" if augment is None else augment\n",
        "            df.at[row, augment_name] = top1_class\n",
        "            df.at[row, augment_name + \"_prob\"] = top1_prob\n",
        "    else:\n",
        "        file_name = None\n",
        "    print(file_name)\n",
        "\n",
        "df.to_csv(csv_dir, index=False)\n",
        "print(f\"推論結果がCSVファイル '{csv_dir}' に上書き保存されました。\")\n",
        ""
      ],
      "metadata": {
        "id": "m_XAZMqYW0fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5J2pUkgcW0jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PseCS6cMW0lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference Maehara's 240 questions**"
      ],
      "metadata": {
        "id": "4UUn62c4Z7jT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKA2zJAPNbod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/corr_table_all.csv\""
      ],
      "metadata": {
        "id": "eWcLAWPBaFMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_rows = 300\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_path)\n",
        "df['class_num'] = df['class_num'].astype(int)\n",
        "\n",
        "# Add empty columns for top1, top1_prob, top2, top2_prob, top3, top3_prob\n",
        "for col in ['top1', 'top1_prob', 'top2', 'top2_prob', 'top3', 'top3_prob']:\n",
        "    df[col] = None\n",
        "\n",
        "# Show the first few rows to verify the new columns\n",
        "df.head()\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "cKcj7IoHs3Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def update_dataframe_with_inference_results(df, image_dir, model):\n",
        "    # Loop through all images in the directory\n",
        "    for img_name in os.listdir(image_dir):\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        # Perform inference on the image using inference_top3 function\n",
        "        print(\"\")\n",
        "        print(f\"{os.path.basename(img_name)}\")\n",
        "        top_classes = inference_top3(img_path, model)\n",
        "\n",
        "        # Get the basename without the file extension\n",
        "        basename = os.path.splitext(img_name)[0]\n",
        "        # Find the row in the DataFrame that matches the image basename\n",
        "        row_index = df[df['basename'] == basename].index\n",
        "\n",
        "        if not row_index.empty:\n",
        "            # Unpack the top classes and probabilities\n",
        "            top1, top1_prob = top_classes[0]\n",
        "            top2, top2_prob = top_classes[1]\n",
        "            top3, top3_prob = top_classes[2]\n",
        "\n",
        "            # Update the DataFrame\n",
        "            df.at[row_index[0], 'top1'] = top1\n",
        "            df.at[row_index[0], 'top1_prob'] = top1_prob\n",
        "            df.at[row_index[0], 'top2'] = top2\n",
        "            df.at[row_index[0], 'top2_prob'] = top2_prob\n",
        "            df.at[row_index[0], 'top3'] = top3\n",
        "            df.at[row_index[0], 'top3_prob'] = top3_prob\n",
        "\n",
        "        # Optionally print the top classes for debugging or verification\n",
        "        print(top_classes)\n",
        "    return df\n",
        "\n",
        "df = update_dataframe_with_inference_results(df, image_dir, model)\n",
        "df"
      ],
      "metadata": {
        "id": "MZpFUwJms3U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated DataFrame to a new CSV file\n",
        "updated_csv_path = '/content/updated_corr_table_all.csv'\n",
        "df.to_csv(updated_csv_path, index=False)"
      ],
      "metadata": {
        "id": "8WQJt9tX8AP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Maehara_total (250+100)_dataset**"
      ],
      "metadata": {
        "id": "F0tQgaIOjTAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def update_dataframe_with_inference_results(df, image_dir, model):\n",
        "    # 指定されたディレクトリ内の画像ファイル名を取得\n",
        "    image_names = os.listdir(image_dir)\n",
        "\n",
        "    # 画像ファイル名を数値部分に基づいてソート\n",
        "    sorted_image_names = sorted(image_names, key=lambda x: int(os.path.splitext(x)[0]))\n",
        "\n",
        "    # ソートされた画像ファイル名を反復処理\n",
        "    for img_name in sorted_image_names:\n",
        "        # 画像ファイルのパスを作成\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "\n",
        "        # inference_top3関数を使用して画像の推論を実行\n",
        "        print(\"\")\n",
        "        print(f\"{os.path.basename(img_name)}\")\n",
        "        top_classes = inference_top3(img_path, model)\n",
        "\n",
        "        # 画像ファイル名から行インデックスを取得\n",
        "        row_index = int(os.path.splitext(img_name)[0]) - 1\n",
        "        if 0 <= row_index < len(df):\n",
        "            # 上位のクラスと確率をアンパック\n",
        "            top1, top1_prob = top_classes[0]\n",
        "            top2, top2_prob = top_classes[1]\n",
        "            top3, top3_prob = top_classes[2]\n",
        "\n",
        "            # DataFrameを更新\n",
        "            df.at[row_index, 'top1'] = top1\n",
        "            df.at[row_index, 'top1_prob'] = top1_prob\n",
        "            df.at[row_index, 'top2'] = top2\n",
        "            df.at[row_index, 'top2_prob'] = top2_prob\n",
        "            df.at[row_index, 'top3'] = top3\n",
        "            df.at[row_index, 'top3_prob'] = top3_prob\n",
        "\n",
        "            # デバッグや検証のために上位のクラスを出力（オプション）\n",
        "            print(top_classes)\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_data(image_dir, csv_path, updated_csv_path):\n",
        "    # 表示する最大行数を設定\n",
        "    pd.options.display.max_rows = 300\n",
        "\n",
        "    # CSVファイルをDataFrameとして読み込む\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # 'class_num'列をint型に変換\n",
        "    df['class_num'] = df['class_num'].astype(int)\n",
        "\n",
        "    # 新しい列（top1, top1_prob, top2, top2_prob, top3, top3_prob）を追加し、初期値をNoneに設定\n",
        "    for col in ['top1', 'top1_prob', 'top2', 'top2_prob', 'top3', 'top3_prob']:\n",
        "        df[col] = None\n",
        "\n",
        "    # 新しい列を確認するために最初の数行を表示\n",
        "    df.head()\n",
        "\n",
        "    # update_dataframe_with_inference_results関数を呼び出してDataFrameを更新\n",
        "    df = update_dataframe_with_inference_results(df, image_dir, model)\n",
        "\n",
        "    # 更新後のDataFrameを新しいCSVファイルに保存\n",
        "    df.to_csv(updated_csv_path, index=False)"
      ],
      "metadata": {
        "id": "90as_LsEvDiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_slit_total.csv\"\n",
        "updated_csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# データ処理を実行\n",
        "process_data(image_dir, csv_path, updated_csv_path)"
      ],
      "metadata": {
        "id": "wlZW90kUvDkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_sumaho_total.csv\"\n",
        "updated_csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_sumaho_total_updated.csv\"\n",
        "\n",
        "# データ処理を実行\n",
        "process_data(image_dir, csv_path, updated_csv_path)"
      ],
      "metadata": {
        "id": "dQ0F8BJCvDl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e8rdxxXX7joT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_accuracy(updated_csv_path):\n",
        "    # 更新後のCSVファイルを読み込む\n",
        "    df = pd.read_csv(updated_csv_path)\n",
        "\n",
        "    # クラスごとの正解率を計算\n",
        "    class_accuracy = {}\n",
        "    for class_num in df['class_num'].unique():\n",
        "        class_df = df[df['class_num'] == class_num]\n",
        "        correct_predictions = (class_df['class_num'] == class_df['top1']).sum()\n",
        "        total_predictions = len(class_df)\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "        class_accuracy[class_num] = accuracy\n",
        "\n",
        "    # 全体の正解率を計算\n",
        "    overall_correct_predictions = (df['class_num'] == df['top1']).sum()\n",
        "    overall_total_predictions = len(df)\n",
        "    overall_accuracy = overall_correct_predictions / overall_total_predictions\n",
        "\n",
        "    return class_accuracy, overall_accuracy\n",
        "\n",
        "def display_accuracy(class_accuracy, overall_accuracy):\n",
        "    # クラス番号を0から順番に並べ替え\n",
        "    sorted_classes = sorted(class_accuracy.keys())\n",
        "\n",
        "    # クラスごとの正解率を表示\n",
        "    print(\"クラスごとの正解率:\")\n",
        "    for class_num in sorted_classes:\n",
        "        accuracy = class_accuracy[class_num]\n",
        "        print(f\"クラス {class_num}: {accuracy:.2f}\")\n",
        "\n",
        "    # 全体の正解率を表示\n",
        "    print(f\"\\n全体の正解率: {overall_accuracy:.2f}\")\n",
        "\n",
        "# パスを指定\n",
        "updated_csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_sumaho_total_updated.csv\"\n",
        "\n",
        "# 正解率を計算\n",
        "class_accuracy, overall_accuracy = calculate_accuracy(updated_csv_path)\n",
        "\n",
        "# 正解率を表示\n",
        "display_accuracy(class_accuracy, overall_accuracy)"
      ],
      "metadata": {
        "id": "JUHSxH607jp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要な部分だけ抜粋"
      ],
      "metadata": {
        "id": "5BD1oZ3nFO6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference using Cornea journal dataset**"
      ],
      "metadata": {
        "id": "CGUbtkDXypdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/corneaのスマホ判定_滝先生.xlsx\"\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/3.CorneAIを混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\""
      ],
      "metadata": {
        "id": "dGKLXhUeG5ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file starting from row 13 (which is indexed as 12 in Python) for headers\n",
        "df = pd.read_excel(excel_path, header=12)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bjsM7cUC3jFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame and it's already loaded\n",
        "# df = pd.read_csv('your_dataframe.csv')  # Replace with your DataFrame loading method\n",
        "\n",
        "# Directory path\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# Define the columns for the new DataFrame\n",
        "columns = [\"image_num\", \"class\"]\n",
        "\n",
        "# Create an empty DataFrame with the specified columns\n",
        "cornea_journal_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Iterating over each file in the directory\n",
        "for filename in os.listdir(images_dir):\n",
        "    if filename.endswith(\".png\"):\n",
        "        # Extracting the numeric part (image number) from the filename\n",
        "        image_num = int(filename.split('.')[0])\n",
        "\n",
        "        # Match with the DataFrame and extract 'クラス' value\n",
        "        matched_row = df[df['Number'] == image_num]\n",
        "        if not matched_row.empty:\n",
        "            class_value = matched_row['クラス'].iloc[0]\n",
        "            # Creating a new row as a DataFrame\n",
        "            row_data = pd.DataFrame({\"image_num\": [image_num], \"class\": [class_value], \"class_name\": [class_names[class_value]]})\n",
        "            # Concatenating the new row DataFrame with the main DataFrame\n",
        "            cornea_journal_df = pd.concat([cornea_journal_df, row_data], ignore_index=True)\n",
        "\n",
        "# Sort the DataFrame by the image_num column\n",
        "cornea_journal_df.sort_values(by='image_num', inplace=True)\n",
        "\n",
        "# Displaying the sorted DataFrame\n",
        "print(cornea_journal_df)\n"
      ],
      "metadata": {
        "id": "_EPP9cvRAMD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像のクラスを確認\n",
        "# Getting the distribution of the 'class' column and sorting by class values\n",
        "class_distribution = cornea_journal_df['class'].value_counts().sort_index()\n",
        "\n",
        "# Displaying the distribution\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOLIBDKDAxPa",
        "outputId": "1ac4960d-8822-41e5-a34a-9396f9bf21aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    46\n",
            "1     7\n",
            "2     9\n",
            "3    31\n",
            "4    33\n",
            "5    32\n",
            "8     4\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "vJab4D4MH2MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "Y6SOama5H51r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# New model\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\"\n",
        "\"\"\"\n",
        "\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(mixed_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "ZhqjG7deIJkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "C-uVI3bJIOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/baseline_to_mixed.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "PrxCWw7EIQkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Analyze_results***"
      ],
      "metadata": {
        "id": "CKH8THhJLdyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "#csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/compare_accuracy.csv\"\n",
        "csv_path = dst_path\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "eSFoOT4-LhBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# Create a confusion matrix for top1_baseline\n",
        "cm_baseline = confusion_matrix(df['class_name'], df['top1_baseline'], labels=class_names_order)\n",
        "\n",
        "# Create a confusion matrix for top1_new\n",
        "cm_new = confusion_matrix(df['class_name'], df['top1_new'], labels=class_names_order)\n",
        "\n",
        "\n",
        "# Plot for top1_baseline\n",
        "fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_baseline, annot=True, ax=ax1, fmt='d', cmap='Blues')\n",
        "ax1.set_title('Confusion Matrix for top1_baseline')\n",
        "ax1.set_xlabel('Predicted Labels')\n",
        "ax1.set_ylabel('True Labels')\n",
        "ax1.set_xticklabels(class_names_order, rotation=45)\n",
        "ax1.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot for top1_new\n",
        "fig2, ax2 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_new, annot=True, ax=ax2, fmt='d', cmap='Greens')\n",
        "ax2.set_title('Confusion Matrix for top1_new')\n",
        "ax2.set_xlabel('Predicted Labels')\n",
        "ax2.set_ylabel('True Labels')\n",
        "ax2.set_xticklabels(class_names_order, rotation=45)\n",
        "ax2.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yN7BMwQZLmgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for top1_baseline and top1_new\n",
        "accuracy_baseline = np.trace(cm_baseline) / np.sum(cm_baseline)\n",
        "accuracy_new = np.trace(cm_new) / np.sum(cm_new)\n",
        "\n",
        "accuracy_baseline, accuracy_new\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYZWAdVhM87P",
        "outputId": "1a02d428-e668-4758-f2ef-fc0a0197a673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MultiModel evaluation pipeline**"
      ],
      "metadata": {
        "id": "NrWAyrggO9nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #######################\n",
        "# # MultiModel evaluation pipeline #\n",
        "# #######################\n",
        "\n",
        "# from models.common import DetectMultiBackend\n",
        "# #from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "# from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "#                            increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "# #from utils.plots import Annotator, colors, save_one_box\n",
        "# #from utils.torch_utils import select_device, time_sync\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# from torchvision import models, transforms\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# device = 'cpu'\n",
        "# #model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# # model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "# class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "#     # Resize and pad image while meeting stride-multiple constraints\n",
        "#     shape = im.shape[:2]  # current shape [height, width]\n",
        "#     if isinstance(new_shape, int):\n",
        "#         new_shape = (new_shape, new_shape)\n",
        "\n",
        "#     # Scale ratio (new / old)\n",
        "#     r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "#     if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "#         r = min(r, 1.0)\n",
        "\n",
        "#     # Compute padding\n",
        "#     # ratio = r, r  # width, height ratios\n",
        "#     new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "#     dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "#     if auto:  # minimum rectangle\n",
        "#         dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "#     elif scaleFill:  # stretch\n",
        "#         dw, dh = 0.0, 0.0\n",
        "#         new_unpad = (new_shape[1], new_shape[0])\n",
        "#         # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "#     dw /= 2  # divide padding into 2 sides\n",
        "#     dh /= 2\n",
        "\n",
        "#     if shape[::-1] != new_unpad:  # resize\n",
        "#         im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "#     top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "#     left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "#     im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "#     return im\n",
        "\n",
        "# def inference_top3(img_path, model):\n",
        "#     img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "#     # Apply letterbox to the image\n",
        "#     img_cv2 = letterbox_image(img_cv2)\n",
        "#     #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "#     # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "#     img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "#     # Get current figure size\n",
        "#     fig_size = plt.gcf().get_size_inches()\n",
        "#     # Set new size (half the current size)\n",
        "#     plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "#     # plt.imshow(img_mpl)\n",
        "#     # plt.axis('off')  # Turn off axis numbers\n",
        "#     # plt.show()\n",
        "\n",
        "#     img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "#     img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "#     img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "#     img_tensor /= 255\n",
        "#     img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "#     pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "#     # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "#     pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "#     # 全てのクラスとlikelihoodのペアを取得\n",
        "#     class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "#     # likelihoodで降順にソート\n",
        "#     class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "#     # 上位3つのクラスとlikelihoodを出力\n",
        "#     print(\"Top 3 Classes and Likelihoods:\")\n",
        "#     for i in range(3):\n",
        "#         likelihood, class_num = class_likelihood_pairs[i]\n",
        "#         print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "#         print(\"\")\n",
        "\n",
        "#     top_classes = []\n",
        "#     for i in range(min(3, len(class_likelihood_pairs))):\n",
        "#         likelihood, class_num = class_likelihood_pairs[i]\n",
        "#         top_classes.append((class_num, likelihood))\n",
        "\n",
        "#     return top_classes\n",
        "\n",
        "# def evaluation(model_path, title):\n",
        "#     model = DetectMultiBackend(model_path, device=\"cpu\", dnn=False)\n",
        "#     for num in cornea_journal_df[\"image_num\"]:\n",
        "#         img = f\"{images_dir}/{num}.png\"\n",
        "#         top3_results = inference_top3(img, model)\n",
        "\n",
        "#         # Update the DataFrame with the results\n",
        "#         for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "#             cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "#             cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "# evaluation(baseline_weight_path, \"baseline\")\n",
        "# evaluation(mixed_weight_path, \"mixed\")\n",
        "# evaluation(finetune_mixed_weight_path, \"finetune_mixed\")\n",
        "# evaluation(finetune_100ep_weight_path, \"finetune_100ep\")\n",
        "# evaluation(finetune_150ep_weight_path, \"finetune_150ep\")\n",
        "# evaluation(finetune_200ep_weight_path, \"finetune_200ep\")\n",
        "# evaluation(finetune_250ep_weight_path, \"finetune_250ep\")\n",
        "# evaluation(finetune_300ep_weight_path, \"finetune_300ep\")\n",
        "# evaluation(finetune_350ep_weight_path, \"finetune_350ep\")\n",
        "# evaluation(finetune_400ep_weight_path, \"finetune_400ep\")"
      ],
      "metadata": {
        "id": "tXmOPrcKL526"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    from utils.general import non_max_suppression\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    top_classes = [(class_num, likelihood) for _, class_num in sorted(class_likelihood_pairs[:3])]\n",
        "    return top_classes\n",
        "\n",
        "# def evaluate_model(model_path, data_df, title, images_dir):\n",
        "#     model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "#     for num in data_df[\"image_num\"]:\n",
        "#         img_path = f\"{images_dir}/{num}.png\"\n",
        "#         img_tensor = preprocess_image(img_path)\n",
        "#         top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "#         for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "#             data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "#             data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "def evaluate_model(model_path, data_df, title, images_dir):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "\n",
        "    # Wrapping the iteration with tqdm for progress tracking\n",
        "    for num in tqdm(data_df[\"image_num\"], desc=f\"Evaluating {title}\"):\n",
        "        img_path = f\"{images_dir}/{num}.png\"\n",
        "        img_tensor = preprocess_image(img_path)\n",
        "        top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "        for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "            data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "            data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "# Example usage\n",
        "weight_paths = [baseline_weight_path, mixed_weight_path, finetune_mixed_weight_path, finetune_100ep_weight_path, finetune_150ep_weight_path, finetune_200ep_weight_path, finetune_250ep_weight_path, finetune_300ep_weight_path, finetune_350ep_weight_path, finetune_400ep_weight_path]\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "for path, title in zip(weight_paths, titles):\n",
        "    evaluate_model(path, cornea_journal_df, title, images_dir)\n"
      ],
      "metadata": {
        "id": "lxMkzMo-jNog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/cornea_journal_all.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "e2YGhZgdSg1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load data\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/cornea_journal_all.csv\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "cornea_journal_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "cornea_journal_df.head()\n"
      ],
      "metadata": {
        "id": "dYatGMIrolDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "r8xMxymdUoq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = cornea_journal_df\n",
        "\n",
        "# Titles for the different model evaluations\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "def draw_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for top1_{title}')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Calculate and display confusion matrices\n",
        "for title in titles:\n",
        "    cm = confusion_matrix(df['class_name'], df[f'top1_{title}'], labels=class_names_order)\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    accuracies[title] = accuracy\n",
        "    draw_confusion_matrix(cm, title)\n",
        "\n",
        "# Print all accuracies at the end\n",
        "for title, accuracy in accuracies.items():\n",
        "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "vdMnTluOeH3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference_Maehara_dataset**"
      ],
      "metadata": {
        "id": "bTj36ASWlo3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the zip file and the extraction directory\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "new_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\""
      ],
      "metadata": {
        "id": "aLTTuAJ6lstO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# DataFrame 'df' にExcelファイルを読み込む\n",
        "df = pd.read_excel(excel_path)\n",
        "df.head()\n",
        "\n",
        "# maehara_dfを作成し、1列目にslit_id、2列目にdisease_Englishを含める\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "maehara_df.head()\n"
      ],
      "metadata": {
        "id": "4FM6clySpXX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ファイルの存在確認\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Excelファイルを読み込む\n",
        "excel_data = pd.read_excel(excel_path)\n",
        "\n",
        "# 'basename'列の値を文字列に変換\n",
        "excel_data['basename'] = excel_data['basename'].astype(str)\n",
        "\n",
        "# チェックするディレクトリのパス\n",
        "image_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット'\n",
        "\n",
        "# エラーメッセージのリスト\n",
        "errors = []\n",
        "\n",
        "# 各basenameに対して、.jpgまたは.pngファイルがあるかどうかを確認\n",
        "for basename in excel_data['basename']:\n",
        "    jpg_file = os.path.join(image_dir, basename + '.jpg')\n",
        "    JPG_file = os.path.join(image_dir, basename + '.JPG')\n",
        "    png_file = os.path.join(image_dir, basename + '.png')\n",
        "\n",
        "    if not os.path.exists(jpg_file) and not os.path.exists(png_file) and not os.path.exists(JPG_file):\n",
        "        errors.append(f\"Error: Neither {basename}.jpg nor {basename}.JPG nor {basename}.png exists in the directory.\")\n",
        "\n",
        "# エラーメッセージを表示\n",
        "for error in errors:\n",
        "    print(error)\n",
        "\n",
        "# エラーがあるかどうかを確認\n",
        "if len(errors) > 0:\n",
        "    print(f\"Total missing files: {len(errors)}\")\n",
        "else:\n",
        "    print(\"All files are present.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uQmF9ZzyazAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(image, size=(640, 480)):\n",
        "    ih, iw = image.shape[:2]\n",
        "    w, h = size\n",
        "\n",
        "    # Calculate padding to maintain aspect ratio\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw, nh = int(scale * iw), int(scale * ih)\n",
        "    image = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    # Calculate padding dimensions\n",
        "    top = (h - nh) // 2\n",
        "    bottom = h - nh - top\n",
        "    left = (w - nw) // 2\n",
        "    right = w - nw - left\n",
        "\n",
        "    # Add padding to the image\n",
        "    return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2, size=(640, 480))\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "RO0ce7B3rPZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "nyBUtMe3rPbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "\n",
        "    # # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    # img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # # Get current figure size\n",
        "    # fig_size = plt.gcf().get_size_inches()\n",
        "    # # Set new size (half the current size)\n",
        "    # plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    # plt.imshow(img_mpl)\n",
        "    # plt.axis('off')  # Turn off axis numbers\n",
        "    # plt.show()\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    # print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        # print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "        # print(\"\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model_path, maehara_df, title, images_dir):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "    results = []\n",
        "\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = maehara_df.copy()\n",
        "\n",
        "    for index, row in tqdm(df_copy.iterrows(), total=df_copy.shape[0], desc=f\"Evaluating {title}\"):\n",
        "        img_id = row['basename']\n",
        "        # Try different file extensions\n",
        "        for ext in ['.jpg', '.png', '.JPG']:\n",
        "            img_path = os.path.join(images_dir, f\"{img_id}{ext}\")\n",
        "            if os.path.exists(img_path):\n",
        "                print(f\"Processing image: {img_path}\")\n",
        "                img_tensor = preprocess_image(img_path)\n",
        "                top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "                # Store results in a list to avoid SettingWithCopyWarning\n",
        "                for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "                    results.append((index, f'top{i+1}_{title}', class_names[class_num]))\n",
        "                    results.append((index, f'top{i+1}_{title}_likelihood', likelihood))\n",
        "                break\n",
        "        else:\n",
        "            print(f\"No image found for ID {img_id} with any of the extensions .jpg, .png, .JPG\")\n",
        "\n",
        "    # Apply the updates outside the loop\n",
        "    for index, column, value in results:\n",
        "        df_copy.at[index, column] = value\n",
        "\n",
        "    # Return the modified DataFrame\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "bA-QeMo7kX10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "weight_paths = [baseline_weight_path, mixed_weight_path, finetune_mixed_weight_path, finetune_100ep_weight_path, finetune_150ep_weight_path, finetune_200ep_weight_path, finetune_250ep_weight_path, finetune_300ep_weight_path, finetune_350ep_weight_path, finetune_400ep_weight_path]\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "# weight_paths = [baseline_weight_path]\n",
        "# titles = [\"baseline\"]\n",
        "\n",
        "for path, title in zip(weight_paths, titles):\n",
        "    maehara_df = evaluate_model(path, maehara_df, title, images_dir)"
      ],
      "metadata": {
        "id": "oWVXdG61kaXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maehara_df"
      ],
      "metadata": {
        "id": "aCY-E6Bqfmkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = maehara_df.dropna() #画像がなく評価が行われていない行を削除\n",
        "\n",
        "# Titles for the different model evaluations\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "def draw_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for top1_{title}')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Calculate and display confusion matrices\n",
        "for title in titles:\n",
        "    cm = confusion_matrix(df['disease_English'], df[f'top1_{title}'], labels=class_names_order)\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    accuracies[title] = accuracy\n",
        "    draw_confusion_matrix(cm, title)\n",
        "\n",
        "# Print all accuracies at the end\n",
        "for title, accuracy in accuracies.items():\n",
        "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "UtMAk6uZrPfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_df.xlsx\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "dqlVIZ1zrPhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load data\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_df.xlsx\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "maehara_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "maehara_df.head()\n"
      ],
      "metadata": {
        "id": "V5zPrMJLrPjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Augmentation_ideas**"
      ],
      "metadata": {
        "id": "OBSNECHYW6zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import glob\n",
        "\n",
        "# Define the path to your image\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = f\"{images_dir}/29.png\"\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[1]\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "def rgb_to_bgr(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to BGR. \"\"\"\n",
        "    # Reverse the order of the first and third channel\n",
        "    return img[..., ::-1]\n",
        "\n",
        "def rgb_to_grb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to GRB. \"\"\"\n",
        "    # Swap the red and green channels\n",
        "    return img[..., [1, 0, 2]]\n",
        "\n",
        "def rgb_to_rbg(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 2, 1]]\n",
        "\n",
        "\n",
        "\n",
        "# Define an augmentation pipeline with Albumentations\n",
        "augmentation = A.Compose([\n",
        "    A.Lambda(image=rgb_to_bgr, p=1.0), # Custom RGB to BGR conversion\n",
        "    #A.Lambda(image=rgb_to_rbg, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.Lambda(image=rgb_to_grb, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.RandomBrightnessContrast(p=1.0, brightness_limit=[0.3,0.3], contrast_limit=[-0.5,-0.5]),\n",
        "])\n",
        "\n",
        "# Apply the augmentation pipeline to the image\n",
        "augmented_img = augmentation(image=img)['image']\n",
        "\n",
        "# Display the augmented image using matplotlib\n",
        "plt.imshow(augmented_img)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GCOz71B2XAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "\n",
        "    # # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    # img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # # Get current figure size\n",
        "    # fig_size = plt.gcf().get_size_inches()\n",
        "    # # Set new size (half the current size)\n",
        "    # plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    # plt.imshow(img_mpl)\n",
        "    # plt.axis('off')  # Turn off axis numbers\n",
        "    # plt.show()\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    # print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        # print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "        # print(\"\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "def setup_model(model_path):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def check_groundtruth(maehara_df, image_dir):\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = maehara_df.copy()\n",
        "\n",
        "    img_id = os.path.basename(image_dir).split(\".\")[0]\n",
        "    groundtruth = df_copy.loc[df_copy['basename'] == img_id, 'disease_English']\n",
        "    return groundtruth.item()\n",
        "\n",
        "def bright_contarst_augment(img, brightness, contrast):\n",
        "    augmentation = A.Compose([\n",
        "        A.RandomBrightnessContrast(p=1.0, brightness_limit=[brightness,brightness], contrast_limit=[contrast,contrast])\n",
        "    ])\n",
        "    # Apply the augmentation\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n",
        "def coarse_dropout(img):\n",
        "    drop_size = round(img.shape[1]*0.02)\n",
        "    augmentation = A.Compose([\n",
        "        #A.CoarseDropout(p=1.0, max_holes=200, max_height=drop_size, max_width=drop_size, min_holes=200, min_height=drop_size, min_width=drop_size, fill_value=(114,114,114))\n",
        "        A.CoarseDropout(p=1.0, max_holes=200, max_height=drop_size, max_width=drop_size, min_holes=200, min_height=drop_size, min_width=drop_size, fill_value=(60,68,124))\n",
        "        ])\n",
        "    # Apply the augmentation\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n",
        "def rgb_to_bgr(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to BGR. \"\"\"\n",
        "    # Reverse the order of the first and third channel\n",
        "    return img[..., ::-1]\n",
        "\n",
        "def rgb_to_grb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to GRB. \"\"\"\n",
        "    # Swap the red and green channels\n",
        "    return img[..., [1, 0, 2]]\n",
        "\n",
        "def rgb_to_rbg(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 2, 1]]\n",
        "\n",
        "def no_augment(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 1, 2]]\n",
        "\n",
        "def rgb_to_gb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to _GR (Red channel set to 0). \"\"\"\n",
        "    # Set the red channel (first channel) to 0\n",
        "    img[..., 0] = 0\n",
        "    return img\n",
        "\n",
        "def rgb_to_rb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RB (Green channel set to 0). \"\"\"\n",
        "    # Set the green channel (second channel) to 0\n",
        "    img[..., 1] = 0\n",
        "    return img\n",
        "\n",
        "def rgb_to_sepia(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RB (Green channel set to 0). \"\"\"\n",
        "    #全画素についてB(青)の輝度を0.3倍、G(緑)の輝度を0.8倍、R(赤)の輝度はそのまま\n",
        "    img[:, :, 0] =img[:, :, 0] * 0.3\n",
        "    img[:, :, 1] =img[:, :, 1] * 0.8\n",
        "    img[:, :, 2] =img[:, :, 2]\n",
        "    return img\n",
        "\n",
        "\n",
        "def rgb_to_grayscale(img, **kwargs):\n",
        "    \"\"\" Convert a 3-channel RGB image to a 3-channel grayscale image. \"\"\"\n",
        "    # Convert to grayscale using OpenCV\n",
        "    grayscale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    # Stack the grayscale values across the three channels\n",
        "    grayscale_3channel = cv2.cvtColor(grayscale, cv2.COLOR_GRAY2RGB)\n",
        "    return grayscale_3channel\n",
        "\n",
        "def RGB_augment(img, colorpattern):\n",
        "    augmentation = A.Compose([\n",
        "        A.Lambda(image=colorpattern, p=1.0), # rgb_to_bgr, rgb_to_grb, or rgb_to_rbg\n",
        "    ])\n",
        "    # Apply the augmentation pipeline to the image\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n"
      ],
      "metadata": {
        "id": "eIJREB8qXAXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### RGB to BGR, RBG, & GRB ###############\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "df = pd.read_excel(excel_path)\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "model = setup_model(baseline_weight_path)\n",
        "\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[40]\n",
        "\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = glob.glob(f\"{images_dir}/*.png\")[2]\n",
        "\n",
        "\n",
        "try:\n",
        "    groundtruth = check_groundtruth(maehara_df, image_path)\n",
        "except:\n",
        "    groundtruth = None\n",
        "    pass\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img2 = img.copy()\n",
        "\n",
        "for colorpattern in [no_augment, rgb_to_bgr, rgb_to_grb, rgb_to_rbg, rgb_to_grayscale, rgb_to_gb, rgb_to_sepia]:\n",
        "    print(f\"Applying transformation: {colorpattern.__name__}\")\n",
        "    augmented_img = RGB_augment(img2, colorpattern)\n",
        "    augmented_img = coarse_dropout(augmented_img)\n",
        "    # augmented_img = RGB_augment(img2, colorpattern)\n",
        "    img = letterbox_image(augmented_img)\n",
        "    img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img_tensor = torch.from_numpy(img).float() / 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "    top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "    top1_class = class_names[top3_results[0][0]]\n",
        "    top1_likelihood = top3_results[0][1]\n",
        "    try:\n",
        "        print(f\"Groundtruth: {groundtruth}\")\n",
        "    except:\n",
        "        pass\n",
        "    print(f\"pred: {top1_class}, likelihood: {top1_likelihood}\")\n",
        "\n",
        "    augmented_img = cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB) #cv2 --> PIL\n",
        "    plt.imshow(augmented_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "G616y4PX5Tpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## Brightness & contrast ############\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "df = pd.read_excel(excel_path)\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "model = setup_model(baseline_weight_path)\n",
        "\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[37]\n",
        "\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = glob.glob(f\"{images_dir}/*.png\")[14]\n",
        "\n",
        "try:\n",
        "    groundtruth = check_groundtruth(maehara_df, image_path)\n",
        "except:\n",
        "    groundtruth = None\n",
        "    pass\n",
        "\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img2 = img.copy()\n",
        "\n",
        "for brightness in [-0.2, 0, 0.2, 0.4]:\n",
        "    for contrast in [-0.8, -0.4, -0.2, 0, 0.2, 0.4]:\n",
        "        print(f\"Brightness: {brightness}, Contrast: {contrast}\")\n",
        "        augmented_img = coarse_dropout(img2)\n",
        "        augmented_img = bright_contarst_augment(augmented_img, brightness, contrast)\n",
        "        img = letterbox_image(augmented_img)\n",
        "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img_tensor = torch.from_numpy(img).float() / 255\n",
        "        img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "        top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "        top1_class = class_names[top3_results[0][0]]\n",
        "        top1_likelihood = top3_results[0][1]\n",
        "        try:\n",
        "            print(f\"Groundtruth: {groundtruth}\")\n",
        "        except:\n",
        "            pass\n",
        "        print(f\"pred: {top1_class}, likelihood: {top1_likelihood}\")\n",
        "\n",
        "        augmented_img = cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB) #cv2 --> PIL\n",
        "        plt.imshow(augmented_img)\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "lhvjVRbhAg6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an augmentation pipeline with Albumentations\n",
        "augmentation = A.Compose([\n",
        "    A.Lambda(image=rgb_to_bgr, p=1.0), # Custom RGB to BGR conversion\n",
        "    #A.Lambda(image=rgb_to_rbg, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.Lambda(image=rgb_to_grb, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.RandomBrightnessContrast(p=1.0, brightness_limit=[0.3,0.3], contrast_limit=[-0.5,-0.5]),\n",
        "])"
      ],
      "metadata": {
        "id": "LwCaj-QgXAaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}