{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuDTC0FwERE86nZSEZod8/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_augment-ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 inference_augment-ensemble**"
      ],
      "metadata": {
        "id": "Un512TpLoNtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup"
      ],
      "metadata": {
        "id": "k2ciP17pzhrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F7fjEQUV-pd",
        "outputId": "6b3ab1c8-fc9c-4681-bbca-febb3e1c9365"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYRY9egwjuIs",
        "outputId": "82f22ef5-0311-40f8-d5d5-c2acc76c1535"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "opencv-python-headless>=4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "# torch>=1.7.0\n",
        "# torchvision>=0.8.1\n",
        "tqdm>=4.41.0\n",
        "\n",
        "# Logging -------------------------------------\n",
        "##tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "##seaborn>=0.11.0\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUeMX07NqirS",
        "outputId": "d9f187e0-b0e8-4d08-e46b-a042d27587b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Tfb6NYZIBGm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#最新バージョンでも動くので削除\n",
        "# !pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install torchvision==0.11.2+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "ESI_x2upsdf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "num = 5\n",
        "img_dir = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data\"\n",
        "img = glob.glob(f\"{img_dir}/*\")[num]\n",
        "img\n",
        "\n",
        "img = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg\""
      ],
      "metadata": {
        "id": "Bnz_lfT_l7NM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "BB9E6zqwD0Py",
        "outputId": "39f4ac14-e3c2-4abc-94a3-b6b2d4ac2e38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple inference\n",
        "model = DetectMultiBackend(weight_path, device=\"cpu\", dnn=False)\n",
        "top_classes = inference_top3(img, model)\n",
        "print(top_classes)"
      ],
      "metadata": {
        "id": "7UmC2m6cE160"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_classes[0][0]"
      ],
      "metadata": {
        "id": "dtxWJRzvJR_0",
        "outputId": "a4d383f0-7903-4258-da2e-6d06fc86b8a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference Maehara's 100 questions**\n",
        "画像データ\n",
        "\n",
        "sumaho: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\n",
        "\n",
        "slit: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_sumaho\n",
        "\n",
        "結果CSV: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv"
      ],
      "metadata": {
        "id": "IAcFpqSIMyKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir_sumaho = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\"\n",
        "img_dir_slit = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_sumaho\"\n",
        "csv_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\""
      ],
      "metadata": {
        "id": "FzGFtY4YW0YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルをPandas DataFrameとして読み込む\n",
        "df = pd.read_csv(csv_dir)\n",
        "\n",
        "# データフレームの内容を表示\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "GV4NdejfW0dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import v2 as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "\n",
        "augment_list = [None, \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\", \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\",\"saturation\"]\n",
        "\n",
        "def inference_augment(img_path, model, augment):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "    img_tensor /= 255\n",
        "\n",
        "    # Convert to uint8 if required for JPEG compression\n",
        "    if augment == \"jpegcompression\":\n",
        "        img_tensor = (img_tensor * 255).byte()\n",
        "\n",
        "    # Apply augmentation based on the 'augment' argument\n",
        "    if augment == \"horizontalflip\":\n",
        "        img_tensor = T.functional.hflip(img_tensor)\n",
        "    elif augment == \"grayscale\":\n",
        "        img_tensor = T.functional.rgb_to_grayscale(img_tensor, num_output_channels=3)\n",
        "    elif augment == \"autocontrast\":\n",
        "        img_tensor = T.functional.autocontrast(img_tensor)\n",
        "    elif augment == \"rotate\":\n",
        "        img_tensor = T.functional.rotate(img_tensor, 10)\n",
        "    elif augment == \"blur\":\n",
        "        img_tensor = T.functional.gaussian_blur(img_tensor, kernel_size=(5, 9), sigma=(0.1, 5))\n",
        "    elif augment == \"jpegcompression\":\n",
        "        img_tensor = T.functional.jpeg(img_tensor, quality=30)\n",
        "        img_tensor = img_tensor.float() / 255  # Convert back to float\n",
        "    elif augment == \"addnoise\":\n",
        "        noise = torch.randn(img_tensor.size()) * 0.1\n",
        "        img_tensor = img_tensor + noise\n",
        "        img_tensor = torch.clamp(img_tensor, 0, 1)\n",
        "    elif augment == \"clahe\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.CLAHE(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"sharpen\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.Sharpen(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"gamma\":\n",
        "        img_tensor = T.functional.adjust_gamma(img_tensor, gamma=1.8, gain=1.0)\n",
        "    elif augment == \"colorjitter\":\n",
        "        color_jitter = T.ColorJitter(brightness=0, contrast=0, saturation=1, hue=0)\n",
        "        img_tensor = color_jitter(img_tensor)\n",
        "    elif augment == \"tosepia\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.ToSepia(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"saturation\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.HueSaturationValue(hue_shift_limit=0, sat_shift_limit=70, val_shift_limit=0, p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "\n",
        "\n",
        "    img_augmented = img_tensor.numpy()\n",
        "    img_augmented = img_augmented[::-1].transpose((1, 2, 0))\n",
        "    img_augmented = (img_augmented * 255).astype(np.uint8)\n",
        "\n",
        "    # show images\n",
        "\n",
        "    # img_mpl_augmented = cv2.cvtColor(img_augmented, cv2.COLOR_BGR2RGB)\n",
        "    # plt.figure(figsize=(4, 4))\n",
        "    # plt.imshow(img_mpl_augmented)\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None, multi_label=True, max_det=1000)\n",
        "\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(min(8, len(pred[0])))]\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    print(f\"Top 3 Classes and Likelihoods (augment: {augment})\")\n",
        "    top_classes = []\n",
        "    for i in range(min(1, len(class_likelihood_pairs))): #3つ表示したければ3に\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "O-dW3spkKldb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample inference\n",
        "img = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit/slit_090.jpg\"\n",
        "csv_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\"\n",
        "\n",
        "augment_list = [None, \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\", \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\",\"saturation\"]\n",
        "\n",
        "model = DetectMultiBackend(weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "for augment in augment_list:\n",
        "    top_classes = inference_augment(img, model, augment=augment)\n",
        "    print(top_classes)\n"
      ],
      "metadata": {
        "id": "Ns8PmWL9HGm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_dir)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MkzwKvfVlDU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "img_dir_sumaho = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\"\n",
        "img_dir_slit = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_sumaho\"\n",
        "csv_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\"\n",
        "\n",
        "# open csv\n",
        "df = pd.read_csv(csv_dir)\n",
        "\n",
        "# List all files in the given directories without extensions\n",
        "files_sumaho = [os.path.splitext(file)[0] for file in os.listdir(img_dir_sumaho)]\n",
        "files_slit = [os.path.splitext(file)[0] for file in os.listdir(img_dir_slit)]\n",
        "\n",
        "# set model for inference\n",
        "model = DetectMultiBackend(weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "# set augment\n",
        "augment_list = [None, \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\", \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\",\"saturation\"]\n",
        "\n",
        "# Check each basename in the dataframe to see if it matches any files in the directories\n",
        "for row, basename in enumerate(df[\"basename\"]):\n",
        "    if basename in files_sumaho:\n",
        "        file_name = os.path.join(img_dir_sumaho, basename + \".jpg\")\n",
        "        print(f\"groundtruth: {df['class_num'][row]}\")\n",
        "        for augment in augment_list:\n",
        "            top1 = inference_augment(file_name, model, augment)\n",
        "            top1_class, top1_prob = int(top1[0][0]), top1[0][1]\n",
        "            augment_name = \"original\" if augment is None else augment\n",
        "            df.at[row, augment_name] = top1_class\n",
        "            df.at[row, augment_name + \"_prob\"] = top1_prob\n",
        "    elif basename in files_slit:\n",
        "        file_name = os.path.join(img_dir_slit, basename + \".jpg\")\n",
        "        print(f\"groundtruth: {df['class_num'][row]}\")\n",
        "        for augment in augment_list:\n",
        "            top1 = inference_augment(file_name, model, augment)\n",
        "            top1_class, top1_prob = int(top1[0][0]), top1[0][1]\n",
        "            augment_name = \"original\" if augment is None else augment\n",
        "            df.at[row, augment_name] = top1_class\n",
        "            df.at[row, augment_name + \"_prob\"] = top1_prob\n",
        "    else:\n",
        "        file_name = None\n",
        "    print(file_name)\n",
        "\n",
        "df.to_csv(csv_dir, index=False)\n",
        "print(f\"推論結果がCSVファイル '{csv_dir}' に上書き保存されました。\")\n"
      ],
      "metadata": {
        "id": "m_XAZMqYW0fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h87-9lqGp7ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agCmQvanp7qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Analyze enhance effects**\n",
        "\n",
        "アンサンブルを行うことにより実際に精度が向上するかどうかを確認"
      ],
      "metadata": {
        "id": "4Eo6Y-aFp8oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#元のモデルのマトリックス図を表示\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\"\n",
        "maehara_data = pd.read_csv(file_path)\n",
        "\n",
        "# Define class names dictionary\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "# Adding a new column to indicate if the image is from a smartphone or slit based on the basename\n",
        "maehara_data['image_type'] = maehara_data['basename'].apply(lambda x: 'smartphone' if 'sumaho' in x else 'slit')\n",
        "\n",
        "# Separate the data into smartphone and slit datasets\n",
        "smartphone_data = maehara_data[maehara_data['image_type'] == 'smartphone']\n",
        "slit_data = maehara_data[maehara_data['image_type'] == 'slit']\n",
        "\n",
        "# Create confusion matrix for original class predictions versus true class_num for smartphone images (count)\n",
        "confusion_matrix_smartphone_count = pd.crosstab(smartphone_data['class_num'], smartphone_data['original'], rownames=['True Class'], colnames=['Predicted Class'])\n",
        "\n",
        "# Create confusion matrix for original class predictions versus true class_num for slit images (count)\n",
        "confusion_matrix_slit_count = pd.crosstab(slit_data['class_num'], slit_data['original'], rownames=['True Class'], colnames=['Predicted Class'])\n",
        "\n",
        "# Mapping the class names to the confusion matrices\n",
        "confusion_matrix_smartphone_count_named = confusion_matrix_smartphone_count.rename(index=class_names, columns=class_names)\n",
        "confusion_matrix_slit_count_named = confusion_matrix_slit_count.rename(index=class_names, columns=class_names)\n",
        "\n",
        "# Plotting confusion matrix for smartphone images with class names\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(confusion_matrix_smartphone_count_named, annot=True, fmt='d', cbar=False, linewidths=.5, linecolor='black', cmap='gray_r')\n",
        "plt.title('Confusion Matrix for Original Predictions (Smartphone Images)')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('True Class')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n",
        "\n",
        "# Plotting confusion matrix for slit images with class names\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(confusion_matrix_slit_count_named, annot=True, fmt='d', cbar=False, linewidths=.5, linecolor='black', cmap='gray_r')\n",
        "plt.title('Confusion Matrix for Original Predictions (Slit Images)')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('True Class')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n",
        "\n",
        "# Create confusion matrix for original class predictions versus true class_num for all images (count)\n",
        "confusion_matrix_all_count = pd.crosstab(maehara_data['class_num'], maehara_data['original'], rownames=['True Class'], colnames=['Predicted Class'])\n",
        "\n",
        "# Mapping the class names to the confusion matrix\n",
        "confusion_matrix_all_count_named = confusion_matrix_all_count.rename(index=class_names, columns=class_names)\n",
        "\n",
        "# Plotting confusion matrix for all images with class names\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(confusion_matrix_all_count_named, annot=True, fmt='d', cbar=False, linewidths=.5, linecolor='black', cmap='gray_r')\n",
        "plt.title('Confusion Matrix for Original Predictions (All Images)')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('True Class')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8OIpZ2nQp7tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#それぞれの画像処理について、各クラスの正解率を表示\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\"\n",
        "maehara_data = pd.read_csv(file_path)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "# Define the transformations\n",
        "transformations = [\"original\", \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\",\n",
        "                   \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\", \"saturation\"]\n",
        "\n",
        "# Calculate accuracy for each transformation and each true class\n",
        "class_labels = sorted(maehara_data['class_num'].unique())\n",
        "accuracy_data = []\n",
        "\n",
        "for class_label in class_labels:\n",
        "    row = {'True Class': class_label}\n",
        "    class_data = maehara_data[maehara_data['class_num'] == class_label]\n",
        "\n",
        "    for transform in transformations:\n",
        "        correct_predictions = class_data[class_data['class_num'] == class_data[transform]]\n",
        "        accuracy = len(correct_predictions) / len(class_data) if len(class_data) > 0 else 0\n",
        "        row[transform] = accuracy\n",
        "\n",
        "    accuracy_data.append(row)\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "accuracy_df = pd.DataFrame(accuracy_data)\n",
        "\n",
        "# Display the DataFrame\n",
        "accuracy_df\n",
        "\n"
      ],
      "metadata": {
        "id": "PseCS6cMW0lE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "2bd00693-fee9-4268-8e00-a3d2169d8f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   True Class  original  horizontalflip  grayscale  autocontrast    rotate  \\\n",
              "0           0  0.916667        0.916667   0.583333      0.916667  1.000000   \n",
              "1           1  0.857143        0.928571   0.928571      0.928571  0.928571   \n",
              "2           2  0.600000        0.700000   0.700000      0.600000  0.600000   \n",
              "3           3  1.000000        1.000000   0.875000      1.000000  0.875000   \n",
              "4           4  1.000000        1.000000   0.916667      1.000000  1.000000   \n",
              "5           5  0.785714        0.785714   0.928571      0.857143  0.785714   \n",
              "6           6  0.625000        0.625000   0.375000      0.500000  0.625000   \n",
              "7           7  0.833333        0.750000   0.666667      0.833333  1.000000   \n",
              "8           8  0.800000        0.800000   0.800000      0.800000  0.900000   \n",
              "\n",
              "       blur  jpegcompression  addnoise     clahe   sharpen     gamma  \\\n",
              "0  0.916667         0.916667  1.000000  0.833333  1.000000  1.000000   \n",
              "1  0.857143         0.857143  0.500000  0.642857  1.000000  0.928571   \n",
              "2  0.500000         0.500000  0.200000  0.500000  0.400000  0.700000   \n",
              "3  1.000000         0.750000  0.375000  1.000000  0.625000  0.625000   \n",
              "4  1.000000         1.000000  0.500000  1.000000  1.000000  1.000000   \n",
              "5  0.785714         0.785714  0.142857  0.785714  0.714286  0.857143   \n",
              "6  0.625000         0.375000  0.375000  0.500000  0.500000  0.500000   \n",
              "7  0.500000         0.583333  0.500000  0.916667  0.833333  0.666667   \n",
              "8  1.000000         0.800000  0.700000  0.600000  0.700000  0.800000   \n",
              "\n",
              "   colorjitter   tosepia  saturation  \n",
              "0     0.916667  0.333333    0.916667  \n",
              "1     0.928571  1.000000    0.928571  \n",
              "2     0.600000  0.600000    0.500000  \n",
              "3     1.000000  0.500000    0.875000  \n",
              "4     1.000000  0.750000    1.000000  \n",
              "5     0.785714  0.928571    0.785714  \n",
              "6     0.625000  0.500000    0.625000  \n",
              "7     0.833333  0.500000    0.833333  \n",
              "8     0.800000  0.800000    0.800000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9670c6f1-7e3f-46af-b691-648a3c43f135\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>True Class</th>\n",
              "      <th>original</th>\n",
              "      <th>horizontalflip</th>\n",
              "      <th>grayscale</th>\n",
              "      <th>autocontrast</th>\n",
              "      <th>rotate</th>\n",
              "      <th>blur</th>\n",
              "      <th>jpegcompression</th>\n",
              "      <th>addnoise</th>\n",
              "      <th>clahe</th>\n",
              "      <th>sharpen</th>\n",
              "      <th>gamma</th>\n",
              "      <th>colorjitter</th>\n",
              "      <th>tosepia</th>\n",
              "      <th>saturation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.785714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9670c6f1-7e3f-46af-b691-648a3c43f135')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9670c6f1-7e3f-46af-b691-648a3c43f135 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9670c6f1-7e3f-46af-b691-648a3c43f135');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ad55664a-983c-4a4a-b099-05a959bf89b8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ad55664a-983c-4a4a-b099-05a959bf89b8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ad55664a-983c-4a4a-b099-05a959bf89b8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "accuracy_df",
              "summary": "{\n  \"name\": \"accuracy_df\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"True Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          7,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1429786189873449,\n        \"min\": 0.6,\n        \"max\": 1.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8571428571428571,\n          0.625,\n          0.9166666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"horizontalflip\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13373015873015873,\n        \"min\": 0.625,\n        \"max\": 1.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9285714285714286,\n          0.625,\n          0.9166666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"grayscale\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1895595295830097,\n        \"min\": 0.375,\n        \"max\": 0.9285714285714286,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9285714285714286,\n          0.375,\n          0.5833333333333334\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"autocontrast\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1726067319087722,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9285714285714286,\n          0.5,\n          0.9166666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rotate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15556226196265846,\n        \"min\": 0.6,\n        \"max\": 1.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1.0,\n          0.9285714285714286,\n          0.625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"blur\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20847028756196967,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9166666666666666,\n          0.8571428571428571,\n          0.625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jpegcompression\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20394528466802558,\n        \"min\": 0.375,\n        \"max\": 1.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.5833333333333334,\n          0.8571428571428571,\n          0.7857142857142857\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"addnoise\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2583683046331385,\n        \"min\": 0.14285714285714285,\n        \"max\": 1.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1.0,\n          0.5,\n          0.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clahe\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19996456602208076,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.8333333333333334,\n          0.6428571428571429,\n          0.9166666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sharpen\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2232678066346912,\n        \"min\": 0.4,\n        \"max\": 1.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1.0,\n          0.4,\n          0.8333333333333334\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gamma\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17554489907092463,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9285714285714286,\n          0.5,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"colorjitter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14696234496973115,\n        \"min\": 0.6,\n        \"max\": 1.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9285714285714286,\n          0.625,\n          0.9166666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tosepia\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22429166799184017,\n        \"min\": 0.3333333333333333,\n        \"max\": 1.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.3333333333333333,\n          1.0,\n          0.9285714285714286\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"saturation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15702218490801859,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.8333333333333334,\n          0.9285714285714286,\n          0.7857142857142857\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "file_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100.csv\"\n",
        "maehara_data = pd.read_csv(file_path)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "# Define the transformations\n",
        "transformations = [\"original\", \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\",\n",
        "                   \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\", \"saturation\"]\n",
        "\n",
        "# Calculate accuracy for each transformation and each true class\n",
        "class_labels = sorted(maehara_data['class_num'].unique())\n",
        "accuracy_data = []\n",
        "\n",
        "for class_label in class_labels:\n",
        "    row = {'True Class': class_label}\n",
        "    class_data = maehara_data[maehara_data['class_num'] == class_label]\n",
        "\n",
        "    for transform in transformations:\n",
        "        correct_predictions = class_data[class_data['class_num'] == class_data[transform]]\n",
        "        accuracy = len(correct_predictions) / len(class_data) if len(class_data) > 0 else 0\n",
        "        row[transform] = accuracy\n",
        "\n",
        "    accuracy_data.append(row)\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "accuracy_df = pd.DataFrame(accuracy_data)\n",
        "\n",
        "# Calculate overall accuracy for each transformation\n",
        "overall_accuracy = {}\n",
        "\n",
        "for transform in transformations:\n",
        "    correct_predictions = maehara_data[maehara_data['class_num'] == maehara_data[transform]]\n",
        "    accuracy = len(correct_predictions) / len(maehara_data)\n",
        "    overall_accuracy[transform] = accuracy\n",
        "\n",
        "overall_accuracy_df = pd.DataFrame(list(overall_accuracy.items()), columns=['Transformation', 'Overall Accuracy'])\n",
        "\n",
        "# Display the DataFrames\n",
        "accuracy_df_sorted = accuracy_df.sort_values(by='True Class')\n",
        "accuracy_df_sorted, overall_accuracy_df\n"
      ],
      "metadata": {
        "id": "3drwlfGKyz33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f90beb-41a6-4e80-f345-6a2f4978dfc5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   True Class  original  horizontalflip  grayscale  autocontrast    rotate  \\\n",
              " 0           0  0.916667        0.916667   0.583333      0.916667  1.000000   \n",
              " 1           1  0.857143        0.928571   0.928571      0.928571  0.928571   \n",
              " 2           2  0.600000        0.700000   0.700000      0.600000  0.600000   \n",
              " 3           3  1.000000        1.000000   0.875000      1.000000  0.875000   \n",
              " 4           4  1.000000        1.000000   0.916667      1.000000  1.000000   \n",
              " 5           5  0.785714        0.785714   0.928571      0.857143  0.785714   \n",
              " 6           6  0.625000        0.625000   0.375000      0.500000  0.625000   \n",
              " 7           7  0.833333        0.750000   0.666667      0.833333  1.000000   \n",
              " 8           8  0.800000        0.800000   0.800000      0.800000  0.900000   \n",
              " \n",
              "        blur  jpegcompression  addnoise     clahe   sharpen     gamma  \\\n",
              " 0  0.916667         0.916667  1.000000  0.833333  1.000000  1.000000   \n",
              " 1  0.857143         0.857143  0.500000  0.642857  1.000000  0.928571   \n",
              " 2  0.500000         0.500000  0.200000  0.500000  0.400000  0.700000   \n",
              " 3  1.000000         0.750000  0.375000  1.000000  0.625000  0.625000   \n",
              " 4  1.000000         1.000000  0.500000  1.000000  1.000000  1.000000   \n",
              " 5  0.785714         0.785714  0.142857  0.785714  0.714286  0.857143   \n",
              " 6  0.625000         0.375000  0.375000  0.500000  0.500000  0.500000   \n",
              " 7  0.500000         0.583333  0.500000  0.916667  0.833333  0.666667   \n",
              " 8  1.000000         0.800000  0.700000  0.600000  0.700000  0.800000   \n",
              " \n",
              "    colorjitter   tosepia  saturation  \n",
              " 0     0.916667  0.333333    0.916667  \n",
              " 1     0.928571  1.000000    0.928571  \n",
              " 2     0.600000  0.600000    0.500000  \n",
              " 3     1.000000  0.500000    0.875000  \n",
              " 4     1.000000  0.750000    1.000000  \n",
              " 5     0.785714  0.928571    0.785714  \n",
              " 6     0.625000  0.500000    0.625000  \n",
              " 7     0.833333  0.500000    0.833333  \n",
              " 8     0.800000  0.800000    0.800000  ,\n",
              "      Transformation  Overall Accuracy\n",
              " 0          original              0.83\n",
              " 1    horizontalflip              0.84\n",
              " 2         grayscale              0.77\n",
              " 3      autocontrast              0.84\n",
              " 4            rotate              0.87\n",
              " 5              blur              0.80\n",
              " 6   jpegcompression              0.75\n",
              " 7          addnoise              0.48\n",
              " 8             clahe              0.76\n",
              " 9           sharpen              0.78\n",
              " 10            gamma              0.81\n",
              " 11      colorjitter              0.84\n",
              " 12          tosepia              0.68\n",
              " 13       saturation              0.82)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_df_sorted"
      ],
      "metadata": {
        "id": "UpgVY-wq1zHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 事後確率を利用して、アンサンブルinference\n",
        "\n",
        "```\n",
        "プロトコル\n",
        "pred = 0 → そのまま\n",
        "pred = 1 →tosepiaで判定。あるいはoriginal, tosepia, grayscaleでアンサンブル\n",
        "pred = 2→そのまま\n",
        "pred = 3→grayscaleで判定。\n",
        "pred = 4→そのまま\n",
        "pred = 5→horizontalflip, rotate, blurのアンサンブル\n",
        "pred = 6→そのまま\n",
        "pred = 7→claheで7以外だった場合にgrayscaleで判定\n",
        "pred = 8→autocontrastで5を分別→clahe, gamma, sharpenで0を分別\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "PhUCIicQGU8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision.transforms import v2 as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "\n",
        "augment_list = [None, \"horizontalflip\", \"grayscale\", \"autocontrast\", \"rotate\", \"blur\", \"jpegcompression\", \"addnoise\", \"clahe\", \"sharpen\", \"gamma\", \"colorjitter\", \"tosepia\",\"saturation\"]\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "\n",
        "def apply_augmentation(img_tensor, augment):\n",
        "    if augment == \"horizontalflip\":\n",
        "        img_tensor = T.functional.hflip(img_tensor)\n",
        "    elif augment == \"grayscale\":\n",
        "        img_tensor = T.functional.rgb_to_grayscale(img_tensor, num_output_channels=3)\n",
        "    elif augment == \"autocontrast\":\n",
        "        img_tensor = T.functional.autocontrast(img_tensor)\n",
        "    elif augment == \"rotate\":\n",
        "        img_tensor = T.functional.rotate(img_tensor, 10)\n",
        "    elif augment == \"blur\":\n",
        "        img_tensor = T.functional.gaussian_blur(img_tensor, kernel_size=(5, 9), sigma=(0.1, 5))\n",
        "    elif augment == \"jpegcompression\":\n",
        "        img_tensor = T.functional.jpeg(img_tensor, quality=30)\n",
        "        img_tensor = img_tensor.float() / 255\n",
        "    elif augment == \"addnoise\":\n",
        "        noise = torch.randn(img_tensor.size()) * 0.1\n",
        "        img_tensor = img_tensor + noise\n",
        "        img_tensor = torch.clamp(img_tensor, 0, 1)\n",
        "    elif augment == \"clahe\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.CLAHE(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"sharpen\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.Sharpen(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"gamma\":\n",
        "        img_tensor = T.functional.adjust_gamma(img_tensor, gamma=1.8, gain=1.0)\n",
        "    elif augment == \"colorjitter\":\n",
        "        color_jitter = T.ColorJitter(brightness=0, contrast=0, saturation=1, hue=0)\n",
        "        img_tensor = color_jitter(img_tensor)\n",
        "    elif augment == \"tosepia\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.ToSepia(p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "    elif augment == \"saturation\":\n",
        "        img_cv2 = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        img_cv2 = img_cv2.transpose((1, 2, 0))\n",
        "        transform = A.HueSaturationValue(hue_shift_limit=0, sat_shift_limit=70, val_shift_limit=0, p=1.0)\n",
        "        img_augmented = transform(image=img_cv2)['image']\n",
        "        img_tensor = torch.from_numpy(img_augmented.transpose((2, 0, 1))).float() / 255\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "def inference_augment(img_path, model, augment):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "    img_tensor /= 255\n",
        "\n",
        "    if augment is not None:\n",
        "        img_tensor = apply_augmentation(img_tensor, augment)\n",
        "\n",
        "    img_augmented = img_tensor.numpy()\n",
        "    img_augmented = img_augmented[::-1].transpose((1, 2, 0))\n",
        "    img_augmented = (img_augmented * 255).astype(np.uint8)\n",
        "\n",
        "    ######################################\n",
        "    # show images\n",
        "    img_mpl_augmented = cv2.cvtColor(img_augmented, cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img_mpl_augmented)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    #####################################\n",
        "\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None, multi_label=True, max_det=1000)\n",
        "\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(min(8, len(pred[0])))]\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    print(f\"Top 3 Classes and Likelihoods (augment: {augment})\")\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "\n",
        "def ensemble_predict(file_name, model):\n",
        "    # オリジナルの予測を取得\n",
        "    original_pred = inference_augment(file_name, model, None)\n",
        "    original_class, original_prob = int(original_pred[0][0]), original_pred[0][1]\n",
        "    print(f\"オリジナルの予測: {class_names[original_class]}, 確率 {original_prob:.5f}\")\n",
        "\n",
        "    if original_class == 0:\n",
        "        print(\"判定は infection でした。オリジナルの予測を返します\")\n",
        "        return original_class, original_prob\n",
        "\n",
        "    elif original_class == 1:\n",
        "        print(\"判定は normal でした。tosepia と grayscale の拡張を用いてアンサンブルを実行します\")\n",
        "        tosepia_pred = inference_augment(file_name, model, \"tosepia\")\n",
        "        tosepia_class, tosepia_prob = int(tosepia_pred[0][0]), tosepia_pred[0][1]\n",
        "        print(f\"Tosepia の予測: {class_names[tosepia_class]}, 確率 {tosepia_prob:.5f}\")\n",
        "\n",
        "        grayscale_pred = inference_augment(file_name, model, \"grayscale\")\n",
        "        grayscale_class, grayscale_prob = int(grayscale_pred[0][0]), grayscale_pred[0][1]\n",
        "        print(f\"Grayscale の予測: {class_names[grayscale_class]}, 確率 {grayscale_prob:.5f}\")\n",
        "\n",
        "        ensemble_classes = [original_class, tosepia_class, grayscale_class]\n",
        "        ensemble_probs = [original_prob, tosepia_prob, grayscale_prob]\n",
        "\n",
        "        # 多数決で判定\n",
        "        ensemble_class = max(set(ensemble_classes), key=ensemble_classes.count)\n",
        "\n",
        "        # 同数の場合は、確率が最大のものを選択\n",
        "        if ensemble_classes.count(ensemble_class) < len(ensemble_classes) / 2:\n",
        "            ensemble_class = ensemble_classes[ensemble_probs.index(max(ensemble_probs))]\n",
        "\n",
        "        ensemble_prob = ensemble_probs[ensemble_classes.index(ensemble_class)]\n",
        "        print(f\"アンサンブルの結果を返します: {class_names[ensemble_class]}, 確率 {ensemble_prob:.5f}\")\n",
        "\n",
        "        return ensemble_class, ensemble_prob\n",
        "\n",
        "    elif original_class == 2:\n",
        "        print(\"判定は non-infection でした。オリジナルの予測を返します\")\n",
        "        return original_class, original_prob\n",
        "\n",
        "    elif original_class == 3:\n",
        "        print(\"判定は scar でした。grayscale の拡張を用いて予測を行います\")\n",
        "        grayscale_pred = inference_augment(file_name, model, \"grayscale\")\n",
        "        grayscale_class, grayscale_prob = int(grayscale_pred[0][0]), grayscale_pred[0][1]\n",
        "        print(f\"Grayscale の予測を返します: {class_names[grayscale_class]}, 確率 {grayscale_prob:.5f}\")\n",
        "\n",
        "        return grayscale_class, grayscale_prob\n",
        "\n",
        "    elif original_class == 4:\n",
        "        print(\"判定は tumor でした。オリジナルの予測を返します\")\n",
        "        return original_class, original_prob\n",
        "\n",
        "    elif original_class == 5:\n",
        "        print(\"判定は deposit でした。horizontalflip, rotate, blur の拡張を用いてアンサンブルを実行します\")\n",
        "        hflip_pred = inference_augment(file_name, model, \"horizontalflip\")\n",
        "        hflip_class, hflip_prob = int(hflip_pred[0][0]), hflip_pred[0][1]\n",
        "        print(f\"Horizontalflip の予測: {class_names[hflip_class]}, 確率 {hflip_prob:.5f}\")\n",
        "\n",
        "        rotate_pred = inference_augment(file_name, model, \"rotate\")\n",
        "        rotate_class, rotate_prob = int(rotate_pred[0][0]), rotate_pred[0][1]\n",
        "        print(f\"Rotate の予測: {class_names[rotate_class]}, 確率 {rotate_prob:.5f}\")\n",
        "\n",
        "        blur_pred = inference_augment(file_name, model, \"blur\")\n",
        "        blur_class, blur_prob = int(blur_pred[0][0]), blur_pred[0][1]\n",
        "        print(f\"Blur の予測: {class_names[blur_class]}, 確率 {blur_prob:.5f}\")\n",
        "\n",
        "        ensemble_classes = [hflip_class, rotate_class, blur_class]\n",
        "        ensemble_probs = [hflip_prob, rotate_prob, blur_prob]\n",
        "\n",
        "        # 多数決で判定\n",
        "        ensemble_class = max(set(ensemble_classes), key=ensemble_classes.count)\n",
        "\n",
        "        # 同数の場合は、確率が最大のものを選択\n",
        "        if ensemble_classes.count(ensemble_class) < len(ensemble_classes) / 2:\n",
        "            ensemble_class = ensemble_classes[ensemble_probs.index(max(ensemble_probs))]\n",
        "\n",
        "        ensemble_prob = ensemble_probs[ensemble_classes.index(ensemble_class)]\n",
        "        print(f\"アンサンブルの結果: {class_names[ensemble_class]}, 確率 {ensemble_prob:.5f}\")\n",
        "\n",
        "        return ensemble_class, ensemble_prob\n",
        "\n",
        "    elif original_class == 6:\n",
        "        print(\"判定は APAC でした。オリジナルの予測を返します\")\n",
        "        return original_class, original_prob\n",
        "\n",
        "    elif original_class == 7:\n",
        "        print(\"判定は lens opacity でした。clahe の拡張を用いて予測を行います\")\n",
        "        clahe_pred = inference_augment(file_name, model, \"clahe\")\n",
        "        clahe_class, clahe_prob = int(clahe_pred[0][0]), clahe_pred[0][1]\n",
        "        print(f\"CLAHE: {class_names[clahe_class]}, 確率 {clahe_prob:.5f}\")\n",
        "\n",
        "        if clahe_class != 7:\n",
        "            print(\"CLAHE の予測が lens opacity ではありませんでした。grayscale の拡張を用いて予測を行います\")\n",
        "            grayscale_pred = inference_augment(file_name, model, \"grayscale\")\n",
        "            grayscale_class, grayscale_prob = int(grayscale_pred[0][0]), grayscale_pred[0][1]\n",
        "            print(f\"Grayscale の予測を返します: {class_names[grayscale_class]}, 確率 {grayscale_prob:.5f}\")\n",
        "\n",
        "            return grayscale_class, grayscale_prob\n",
        "        else:\n",
        "            print(\"CLAHE の予測が lens opacity でした。それを返します\")\n",
        "            return clahe_class, clahe_prob\n",
        "\n",
        "    elif original_class == 8:\n",
        "        print(\"判定は bullous でした。autocontrast の拡張を用いて予測を行います\")\n",
        "        autocontrast_pred = inference_augment(file_name, model, \"autocontrast\")\n",
        "        autocontrast_class, autocontrast_prob = int(autocontrast_pred[0][0]), autocontrast_pred[0][1]\n",
        "        print(f\"Autocontrast の予測: {class_names[autocontrast_class]}, 確率 {autocontrast_prob:.5f}\")\n",
        "\n",
        "        if autocontrast_class == 5:\n",
        "            print(\"Autocontrast の予測は deposit でした。この予測を返します\")\n",
        "            return autocontrast_class, autocontrast_prob\n",
        "        else:\n",
        "            print(\"Autocontrast の予測が deposit ではありませんでした。clahe, gamma, sharpen の拡張を用いてアンサンブルを実行します\")\n",
        "            clahe_pred = inference_augment(file_name, model, \"clahe\")\n",
        "            clahe_class, clahe_prob = int(clahe_pred[0][0]), clahe_pred[0][1]\n",
        "            print(f\"CLAHE の予測: {class_names[clahe_class]}, 確率 {clahe_prob:.5f}\")\n",
        "\n",
        "            gamma_pred = inference_augment(file_name, model, \"gamma\")\n",
        "            gamma_class, gamma_prob = int(gamma_pred[0][0]), gamma_pred[0][1]\n",
        "            print(f\"Gamma の予測: {class_names[gamma_class]}, 確率 {gamma_prob:.5f}\")\n",
        "\n",
        "            sharpen_pred = inference_augment(file_name, model, \"sharpen\")\n",
        "            sharpen_class, sharpen_prob = int(sharpen_pred[0][0]), sharpen_pred[0][1]\n",
        "            print(f\"Sharpen の予測: {class_names[sharpen_class]}, 確率 {sharpen_prob:.5f}\")\n",
        "\n",
        "            ensemble_classes = [clahe_class, gamma_class, sharpen_class]\n",
        "            ensemble_probs = [clahe_prob, gamma_prob, sharpen_prob]\n",
        "\n",
        "            # 多数決で判定\n",
        "            ensemble_class = max(set(ensemble_classes), key=ensemble_classes.count)\n",
        "\n",
        "            # 同数の場合は、確率が最大のものを選択\n",
        "            if ensemble_classes.count(ensemble_class) < len(ensemble_classes) / 2:\n",
        "                ensemble_class = ensemble_classes[ensemble_probs.index(max(ensemble_probs))]\n",
        "\n",
        "            ensemble_prob = ensemble_probs[ensemble_classes.index(ensemble_class)]\n",
        "            print(f\"アンサンブルの結果: {class_names[ensemble_class]}, 確率 {ensemble_prob:.5f}\")\n",
        "\n",
        "            if ensemble_class == 0:\n",
        "                print(f\"アンサンブルの予測結果が infectionなので、それを返します\")\n",
        "                return ensemble_class, ensemble_prob\n",
        "            else:\n",
        "                print(f\"アンサンブルの予測結果がinfectionではありませんでした。オリジナルの予測を返します\")\n",
        "                return original_class, original_prob"
      ],
      "metadata": {
        "id": "h88gIfdNh6au"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir_sumaho = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\"\n",
        "img_dir_slit = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_sumaho\"\n",
        "csv_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100_ensemble.csv\"\n",
        "\n",
        "# open csv\n",
        "df = pd.read_csv(csv_dir)\n",
        "\n",
        "# List all files in the given directories without extensions\n",
        "files_sumaho = [os.path.splitext(file)[0] for file in os.listdir(img_dir_sumaho)]\n",
        "files_slit = [os.path.splitext(file)[0] for file in os.listdir(img_dir_slit)]\n",
        "\n",
        "# set model for inference\n",
        "model = DetectMultiBackend(weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "# Check each basename in the dataframe to see if it matches any files in the directories\n",
        "for row, basename in enumerate(df[\"basename\"]):\n",
        "\n",
        "    if basename in files_sumaho:\n",
        "        file_name = os.path.join(img_dir_sumaho, basename + \".jpg\")\n",
        "    elif basename in files_slit:\n",
        "        file_name = os.path.join(img_dir_slit, basename + \".jpg\")\n",
        "    else:\n",
        "        file_name = None\n",
        "\n",
        "    if file_name is not None:\n",
        "        print(f\"groundtruth: {df['class_num'][row]}\")\n",
        "        ensemble_class, ensemble_prob = ensemble_predict(file_name, model)\n",
        "        df.at[row, \"ensemble_pred\"] = ensemble_class\n",
        "        df.at[row, \"ensemble_prob\"] = ensemble_prob\n",
        "        print(\"\")\n",
        "    else:\n",
        "        print(file_name)\n",
        "\n",
        "df.to_csv(csv_dir, index=False)\n",
        "print(f\"推論結果がCSVファイル '{csv_dir}' に上書き保存されました。\")"
      ],
      "metadata": {
        "id": "iKzqt7MexPxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#推論結果のマトリックス表示\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara100_ensemble.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(conf_matrix, title, class_names, figsize=(6, 6)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=[class_names[i] for i in sorted(class_names.keys())],\n",
        "                yticklabels=[class_names[i] for i in sorted(class_names.keys())])\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Actual Class')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Extracting the true labels and predicted labels\n",
        "true_labels = data['class_num']\n",
        "pred_labels = data['ensemble_pred']\n",
        "\n",
        "# Create and plot confusion matrix for all data\n",
        "conf_matrix_all = confusion_matrix(true_labels, pred_labels)\n",
        "plot_confusion_matrix(conf_matrix_all, 'Confusion Matrix: True vs Predicted Labels', class_names, figsize=(6, 6))\n",
        "\n",
        "# Separate data into 'sumaho' and 'slit' based on basename containing 'sumaho' or 'slit'\n",
        "sumaho_data = data[data['basename'].str.contains('sumaho', na=False)]\n",
        "slit_data = data[data['basename'].str.contains('slit', na=False)]\n",
        "\n",
        "# Calculate and plot confusion matrices for sumaho and slit\n",
        "conf_matrix_sumaho = confusion_matrix(sumaho_data['class_num'], sumaho_data['ensemble_pred'], labels=sorted(class_names.keys()))\n",
        "plot_confusion_matrix(conf_matrix_sumaho, 'Confusion Matrix of Ensemble Predictions (Sumaho)', class_names)\n",
        "\n",
        "conf_matrix_slit = confusion_matrix(slit_data['class_num'], slit_data['ensemble_pred'], labels=sorted(class_names.keys()))\n",
        "plot_confusion_matrix(conf_matrix_slit, 'Confusion Matrix of Ensemble Predictions (Slit)', class_names)\n"
      ],
      "metadata": {
        "id": "FNGZmkhch6da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBlJtPo4h6gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference Maehara's 240 questions (100問は除外)**"
      ],
      "metadata": {
        "id": "to4tD1-1j2JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sumaho_img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_resized\"\n",
        "slit_img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_resized\"\n",
        "test_csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の100問_augmentation_ensemble/maehara184new.csv\""
      ],
      "metadata": {
        "id": "bxFzqqCjh6i3"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "#df = pd.read_csv(test_csv_path)\n",
        "#df = pd.read_csv(test_csv_path, encoding='shift_jis')\n",
        "#df = pd.read_csv(test_csv_path, encoding='cp932')\n",
        "df = pd.read_csv(test_csv_path, encoding='ISO-8859-1')\n",
        "\n",
        "\n",
        "\n",
        "# Create the full path by combining the base directory, basename, and \".jpg\"\n",
        "df['image_path'] = sumaho_img_dir + \"/\" + df['basename'] + \".jpg\"\n",
        "\n",
        "# Display the first few rows to verify the new column\n",
        "df[['basename', 'image_path']].head()\n"
      ],
      "metadata": {
        "id": "fq8l6_Vz08VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sumaho_ensemble"
      ],
      "metadata": {
        "id": "K_YBlNMfCW-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    basename = row['basename']\n",
        "    image_path = f\"{sumaho_img_dir}/{basename}.jpg\"\n",
        "\n",
        "    # ensemble_predict関数を呼び出して推論を実行\n",
        "    ensemble_class, ensemble_prob = ensemble_predict(image_path, model)\n",
        "\n",
        "    # 推論結果をデータフレームに格納\n",
        "    df.at[index, 'sumaho_ensemble'] = int(ensemble_class)\n",
        "\n",
        "# 更新されたデータフレームをCSVファイルに上書き保存\n",
        "df.to_csv(test_csv_path, index=False)"
      ],
      "metadata": {
        "id": "nH5bt2P01eWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sumaho_original"
      ],
      "metadata": {
        "id": "zEZ5wP8PCdJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    basename = row['basename']\n",
        "    image_path = f\"{sumaho_img_dir}/{basename}.jpg\"\n",
        "\n",
        "    # inference_augment関数を呼び出して、augment=Noneで推論を実行\n",
        "    original_pred = inference_augment(image_path, model, None)\n",
        "    original_class, original_prob = original_pred[0]\n",
        "\n",
        "    # 推論結果をデータフレームに格納\n",
        "    df.at[index, 'sumaho_orig'] = original_class\n",
        "\n",
        "# 更新されたデータフレームをCSVファイルに上書き保存\n",
        "df.to_csv(test_csv_path, index=False)"
      ],
      "metadata": {
        "id": "hP78K6mL1eZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### slit_ensemble"
      ],
      "metadata": {
        "id": "4JRDQt8JCjbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    basename = row['basename']\n",
        "    image_path = f\"{slit_img_dir}/{basename}.jpg\"\n",
        "\n",
        "    # ensemble_predict関数を呼び出して推論を実行\n",
        "    ensemble_class, ensemble_prob = ensemble_predict(image_path, model)\n",
        "\n",
        "    # 推論結果をデータフレームに格納\n",
        "    df.at[index, 'slit_ensemble'] = int(ensemble_class)\n",
        "\n",
        "# 更新されたデータフレームをCSVファイルに上書き保存\n",
        "df.to_csv(test_csv_path, index=False)"
      ],
      "metadata": {
        "id": "ZjWlDTH01ebb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### slit_original"
      ],
      "metadata": {
        "id": "87N66cZyCvIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    basename = row['basename']\n",
        "    image_path = f\"{slit_img_dir}/{basename}.jpg\"\n",
        "\n",
        "    # inference_augment関数を呼び出して、augment=Noneで推論を実行\n",
        "    original_pred = inference_augment(image_path, model, None)\n",
        "    original_class, original_prob = original_pred[0]\n",
        "\n",
        "    # 推論結果をデータフレームに格納\n",
        "    df.at[index, 'slit_orig'] = original_class\n",
        "\n",
        "# 更新されたデータフレームをCSVファイルに上書き保存\n",
        "df.to_csv(test_csv_path, index=False)"
      ],
      "metadata": {
        "id": "IMRnpjgNCxQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "63H3qKx8vMDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DiPhpEyyvMFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CTG6NW_avMHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0h23OKpIvMJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference Maehara's 240 questions**"
      ],
      "metadata": {
        "id": "4UUn62c4Z7jT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKA2zJAPNbod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara100/maehara100_slit\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/corr_table_all.csv\""
      ],
      "metadata": {
        "id": "eWcLAWPBaFMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_rows = 300\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_path)\n",
        "df['class_num'] = df['class_num'].astype(int)\n",
        "\n",
        "# Add empty columns for top1, top1_prob, top2, top2_prob, top3, top3_prob\n",
        "for col in ['top1', 'top1_prob', 'top2', 'top2_prob', 'top3', 'top3_prob']:\n",
        "    df[col] = None\n",
        "\n",
        "# Show the first few rows to verify the new columns\n",
        "df.head()\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "cKcj7IoHs3Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def update_dataframe_with_inference_results(df, image_dir, model):\n",
        "    # Loop through all images in the directory\n",
        "    for img_name in os.listdir(image_dir):\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        # Perform inference on the image using inference_top3 function\n",
        "        print(\"\")\n",
        "        print(f\"{os.path.basename(img_name)}\")\n",
        "        top_classes = inference_top3(img_path, model)\n",
        "\n",
        "        # Get the basename without the file extension\n",
        "        basename = os.path.splitext(img_name)[0]\n",
        "        # Find the row in the DataFrame that matches the image basename\n",
        "        row_index = df[df['basename'] == basename].index\n",
        "\n",
        "        if not row_index.empty:\n",
        "            # Unpack the top classes and probabilities\n",
        "            top1, top1_prob = top_classes[0]\n",
        "            top2, top2_prob = top_classes[1]\n",
        "            top3, top3_prob = top_classes[2]\n",
        "\n",
        "            # Update the DataFrame\n",
        "            df.at[row_index[0], 'top1'] = top1\n",
        "            df.at[row_index[0], 'top1_prob'] = top1_prob\n",
        "            df.at[row_index[0], 'top2'] = top2\n",
        "            df.at[row_index[0], 'top2_prob'] = top2_prob\n",
        "            df.at[row_index[0], 'top3'] = top3\n",
        "            df.at[row_index[0], 'top3_prob'] = top3_prob\n",
        "\n",
        "        # Optionally print the top classes for debugging or verification\n",
        "        print(top_classes)\n",
        "    return df\n",
        "\n",
        "df = update_dataframe_with_inference_results(df, image_dir, model)\n",
        "df"
      ],
      "metadata": {
        "id": "MZpFUwJms3U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated DataFrame to a new CSV file\n",
        "updated_csv_path = '/content/updated_corr_table_all.csv'\n",
        "df.to_csv(updated_csv_path, index=False)"
      ],
      "metadata": {
        "id": "8WQJt9tX8AP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Maehara_total (250+100)_dataset**"
      ],
      "metadata": {
        "id": "F0tQgaIOjTAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def update_dataframe_with_inference_results(df, image_dir, model):\n",
        "    # 指定されたディレクトリ内の画像ファイル名を取得\n",
        "    image_names = os.listdir(image_dir)\n",
        "\n",
        "    # 画像ファイル名を数値部分に基づいてソート\n",
        "    sorted_image_names = sorted(image_names, key=lambda x: int(os.path.splitext(x)[0]))\n",
        "\n",
        "    # ソートされた画像ファイル名を反復処理\n",
        "    for img_name in sorted_image_names:\n",
        "        # 画像ファイルのパスを作成\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "\n",
        "        # inference_top3関数を使用して画像の推論を実行\n",
        "        print(\"\")\n",
        "        print(f\"{os.path.basename(img_name)}\")\n",
        "        top_classes = inference_top3(img_path, model)\n",
        "\n",
        "        # 画像ファイル名から行インデックスを取得\n",
        "        row_index = int(os.path.splitext(img_name)[0]) - 1\n",
        "        if 0 <= row_index < len(df):\n",
        "            # 上位のクラスと確率をアンパック\n",
        "            top1, top1_prob = top_classes[0]\n",
        "            top2, top2_prob = top_classes[1]\n",
        "            top3, top3_prob = top_classes[2]\n",
        "\n",
        "            # DataFrameを更新\n",
        "            df.at[row_index, 'top1'] = top1\n",
        "            df.at[row_index, 'top1_prob'] = top1_prob\n",
        "            df.at[row_index, 'top2'] = top2\n",
        "            df.at[row_index, 'top2_prob'] = top2_prob\n",
        "            df.at[row_index, 'top3'] = top3\n",
        "            df.at[row_index, 'top3_prob'] = top3_prob\n",
        "\n",
        "            # デバッグや検証のために上位のクラスを出力（オプション）\n",
        "            print(top_classes)\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_data(image_dir, csv_path, updated_csv_path):\n",
        "    # 表示する最大行数を設定\n",
        "    pd.options.display.max_rows = 300\n",
        "\n",
        "    # CSVファイルをDataFrameとして読み込む\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # 'class_num'列をint型に変換\n",
        "    df['class_num'] = df['class_num'].astype(int)\n",
        "\n",
        "    # 新しい列（top1, top1_prob, top2, top2_prob, top3, top3_prob）を追加し、初期値をNoneに設定\n",
        "    for col in ['top1', 'top1_prob', 'top2', 'top2_prob', 'top3', 'top3_prob']:\n",
        "        df[col] = None\n",
        "\n",
        "    # 新しい列を確認するために最初の数行を表示\n",
        "    df.head()\n",
        "\n",
        "    # update_dataframe_with_inference_results関数を呼び出してDataFrameを更新\n",
        "    df = update_dataframe_with_inference_results(df, image_dir, model)\n",
        "\n",
        "    # 更新後のDataFrameを新しいCSVファイルに保存\n",
        "    df.to_csv(updated_csv_path, index=False)"
      ],
      "metadata": {
        "id": "90as_LsEvDiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_slit_total.csv\"\n",
        "updated_csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# データ処理を実行\n",
        "process_data(image_dir, csv_path, updated_csv_path)"
      ],
      "metadata": {
        "id": "wlZW90kUvDkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_sumaho_total.csv\"\n",
        "updated_csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_sumaho_total_updated.csv\"\n",
        "\n",
        "# データ処理を実行\n",
        "process_data(image_dir, csv_path, updated_csv_path)"
      ],
      "metadata": {
        "id": "dQ0F8BJCvDl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e8rdxxXX7joT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_accuracy(updated_csv_path):\n",
        "    # 更新後のCSVファイルを読み込む\n",
        "    df = pd.read_csv(updated_csv_path)\n",
        "\n",
        "    # クラスごとの正解率を計算\n",
        "    class_accuracy = {}\n",
        "    for class_num in df['class_num'].unique():\n",
        "        class_df = df[df['class_num'] == class_num]\n",
        "        correct_predictions = (class_df['class_num'] == class_df['top1']).sum()\n",
        "        total_predictions = len(class_df)\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "        class_accuracy[class_num] = accuracy\n",
        "\n",
        "    # 全体の正解率を計算\n",
        "    overall_correct_predictions = (df['class_num'] == df['top1']).sum()\n",
        "    overall_total_predictions = len(df)\n",
        "    overall_accuracy = overall_correct_predictions / overall_total_predictions\n",
        "\n",
        "    return class_accuracy, overall_accuracy\n",
        "\n",
        "def display_accuracy(class_accuracy, overall_accuracy):\n",
        "    # クラス番号を0から順番に並べ替え\n",
        "    sorted_classes = sorted(class_accuracy.keys())\n",
        "\n",
        "    # クラスごとの正解率を表示\n",
        "    print(\"クラスごとの正解率:\")\n",
        "    for class_num in sorted_classes:\n",
        "        accuracy = class_accuracy[class_num]\n",
        "        print(f\"クラス {class_num}: {accuracy:.2f}\")\n",
        "\n",
        "    # 全体の正解率を表示\n",
        "    print(f\"\\n全体の正解率: {overall_accuracy:.2f}\")\n",
        "\n",
        "# パスを指定\n",
        "updated_csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_sumaho_total_updated.csv\"\n",
        "\n",
        "# 正解率を計算\n",
        "class_accuracy, overall_accuracy = calculate_accuracy(updated_csv_path)\n",
        "\n",
        "# 正解率を表示\n",
        "display_accuracy(class_accuracy, overall_accuracy)"
      ],
      "metadata": {
        "id": "JUHSxH607jp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要な部分だけ抜粋"
      ],
      "metadata": {
        "id": "5BD1oZ3nFO6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference using Cornea journal dataset**"
      ],
      "metadata": {
        "id": "CGUbtkDXypdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/corneaのスマホ判定_滝先生.xlsx\"\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/3.CorneAIを混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\""
      ],
      "metadata": {
        "id": "dGKLXhUeG5ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file starting from row 13 (which is indexed as 12 in Python) for headers\n",
        "df = pd.read_excel(excel_path, header=12)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bjsM7cUC3jFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame and it's already loaded\n",
        "# df = pd.read_csv('your_dataframe.csv')  # Replace with your DataFrame loading method\n",
        "\n",
        "# Directory path\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# Define the columns for the new DataFrame\n",
        "columns = [\"image_num\", \"class\"]\n",
        "\n",
        "# Create an empty DataFrame with the specified columns\n",
        "cornea_journal_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Iterating over each file in the directory\n",
        "for filename in os.listdir(images_dir):\n",
        "    if filename.endswith(\".png\"):\n",
        "        # Extracting the numeric part (image number) from the filename\n",
        "        image_num = int(filename.split('.')[0])\n",
        "\n",
        "        # Match with the DataFrame and extract 'クラス' value\n",
        "        matched_row = df[df['Number'] == image_num]\n",
        "        if not matched_row.empty:\n",
        "            class_value = matched_row['クラス'].iloc[0]\n",
        "            # Creating a new row as a DataFrame\n",
        "            row_data = pd.DataFrame({\"image_num\": [image_num], \"class\": [class_value], \"class_name\": [class_names[class_value]]})\n",
        "            # Concatenating the new row DataFrame with the main DataFrame\n",
        "            cornea_journal_df = pd.concat([cornea_journal_df, row_data], ignore_index=True)\n",
        "\n",
        "# Sort the DataFrame by the image_num column\n",
        "cornea_journal_df.sort_values(by='image_num', inplace=True)\n",
        "\n",
        "# Displaying the sorted DataFrame\n",
        "print(cornea_journal_df)\n"
      ],
      "metadata": {
        "id": "_EPP9cvRAMD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像のクラスを確認\n",
        "# Getting the distribution of the 'class' column and sorting by class values\n",
        "class_distribution = cornea_journal_df['class'].value_counts().sort_index()\n",
        "\n",
        "# Displaying the distribution\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOLIBDKDAxPa",
        "outputId": "1ac4960d-8822-41e5-a34a-9396f9bf21aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    46\n",
            "1     7\n",
            "2     9\n",
            "3    31\n",
            "4    33\n",
            "5    32\n",
            "8     4\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "vJab4D4MH2MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "Y6SOama5H51r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# New model\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\"\n",
        "\"\"\"\n",
        "\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(mixed_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "ZhqjG7deIJkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "C-uVI3bJIOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/baseline_to_mixed.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "PrxCWw7EIQkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Analyze_results***"
      ],
      "metadata": {
        "id": "CKH8THhJLdyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "#csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/compare_accuracy.csv\"\n",
        "csv_path = dst_path\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "eSFoOT4-LhBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# Create a confusion matrix for top1_baseline\n",
        "cm_baseline = confusion_matrix(df['class_name'], df['top1_baseline'], labels=class_names_order)\n",
        "\n",
        "# Create a confusion matrix for top1_new\n",
        "cm_new = confusion_matrix(df['class_name'], df['top1_new'], labels=class_names_order)\n",
        "\n",
        "\n",
        "# Plot for top1_baseline\n",
        "fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_baseline, annot=True, ax=ax1, fmt='d', cmap='Blues')\n",
        "ax1.set_title('Confusion Matrix for top1_baseline')\n",
        "ax1.set_xlabel('Predicted Labels')\n",
        "ax1.set_ylabel('True Labels')\n",
        "ax1.set_xticklabels(class_names_order, rotation=45)\n",
        "ax1.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot for top1_new\n",
        "fig2, ax2 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_new, annot=True, ax=ax2, fmt='d', cmap='Greens')\n",
        "ax2.set_title('Confusion Matrix for top1_new')\n",
        "ax2.set_xlabel('Predicted Labels')\n",
        "ax2.set_ylabel('True Labels')\n",
        "ax2.set_xticklabels(class_names_order, rotation=45)\n",
        "ax2.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yN7BMwQZLmgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for top1_baseline and top1_new\n",
        "accuracy_baseline = np.trace(cm_baseline) / np.sum(cm_baseline)\n",
        "accuracy_new = np.trace(cm_new) / np.sum(cm_new)\n",
        "\n",
        "accuracy_baseline, accuracy_new\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYZWAdVhM87P",
        "outputId": "1a02d428-e668-4758-f2ef-fc0a0197a673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MultiModel evaluation pipeline**"
      ],
      "metadata": {
        "id": "NrWAyrggO9nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #######################\n",
        "# # MultiModel evaluation pipeline #\n",
        "# #######################\n",
        "\n",
        "# from models.common import DetectMultiBackend\n",
        "# #from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "# from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "#                            increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "# #from utils.plots import Annotator, colors, save_one_box\n",
        "# #from utils.torch_utils import select_device, time_sync\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# from torchvision import models, transforms\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# device = 'cpu'\n",
        "# #model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# # model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "# class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "#     # Resize and pad image while meeting stride-multiple constraints\n",
        "#     shape = im.shape[:2]  # current shape [height, width]\n",
        "#     if isinstance(new_shape, int):\n",
        "#         new_shape = (new_shape, new_shape)\n",
        "\n",
        "#     # Scale ratio (new / old)\n",
        "#     r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "#     if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "#         r = min(r, 1.0)\n",
        "\n",
        "#     # Compute padding\n",
        "#     # ratio = r, r  # width, height ratios\n",
        "#     new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "#     dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "#     if auto:  # minimum rectangle\n",
        "#         dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "#     elif scaleFill:  # stretch\n",
        "#         dw, dh = 0.0, 0.0\n",
        "#         new_unpad = (new_shape[1], new_shape[0])\n",
        "#         # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "#     dw /= 2  # divide padding into 2 sides\n",
        "#     dh /= 2\n",
        "\n",
        "#     if shape[::-1] != new_unpad:  # resize\n",
        "#         im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "#     top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "#     left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "#     im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "#     return im\n",
        "\n",
        "# def inference_top3(img_path, model):\n",
        "#     img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "#     # Apply letterbox to the image\n",
        "#     img_cv2 = letterbox_image(img_cv2)\n",
        "#     #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "#     # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "#     img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "#     # Get current figure size\n",
        "#     fig_size = plt.gcf().get_size_inches()\n",
        "#     # Set new size (half the current size)\n",
        "#     plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "#     # plt.imshow(img_mpl)\n",
        "#     # plt.axis('off')  # Turn off axis numbers\n",
        "#     # plt.show()\n",
        "\n",
        "#     img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "#     img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "#     img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "#     img_tensor /= 255\n",
        "#     img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "#     pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "#     # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "#     pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "#     # 全てのクラスとlikelihoodのペアを取得\n",
        "#     class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "#     # likelihoodで降順にソート\n",
        "#     class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "#     # 上位3つのクラスとlikelihoodを出力\n",
        "#     print(\"Top 3 Classes and Likelihoods:\")\n",
        "#     for i in range(3):\n",
        "#         likelihood, class_num = class_likelihood_pairs[i]\n",
        "#         print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "#         print(\"\")\n",
        "\n",
        "#     top_classes = []\n",
        "#     for i in range(min(3, len(class_likelihood_pairs))):\n",
        "#         likelihood, class_num = class_likelihood_pairs[i]\n",
        "#         top_classes.append((class_num, likelihood))\n",
        "\n",
        "#     return top_classes\n",
        "\n",
        "# def evaluation(model_path, title):\n",
        "#     model = DetectMultiBackend(model_path, device=\"cpu\", dnn=False)\n",
        "#     for num in cornea_journal_df[\"image_num\"]:\n",
        "#         img = f\"{images_dir}/{num}.png\"\n",
        "#         top3_results = inference_top3(img, model)\n",
        "\n",
        "#         # Update the DataFrame with the results\n",
        "#         for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "#             cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "#             cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "# evaluation(baseline_weight_path, \"baseline\")\n",
        "# evaluation(mixed_weight_path, \"mixed\")\n",
        "# evaluation(finetune_mixed_weight_path, \"finetune_mixed\")\n",
        "# evaluation(finetune_100ep_weight_path, \"finetune_100ep\")\n",
        "# evaluation(finetune_150ep_weight_path, \"finetune_150ep\")\n",
        "# evaluation(finetune_200ep_weight_path, \"finetune_200ep\")\n",
        "# evaluation(finetune_250ep_weight_path, \"finetune_250ep\")\n",
        "# evaluation(finetune_300ep_weight_path, \"finetune_300ep\")\n",
        "# evaluation(finetune_350ep_weight_path, \"finetune_350ep\")\n",
        "# evaluation(finetune_400ep_weight_path, \"finetune_400ep\")"
      ],
      "metadata": {
        "id": "tXmOPrcKL526"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    from utils.general import non_max_suppression\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    top_classes = [(class_num, likelihood) for _, class_num in sorted(class_likelihood_pairs[:3])]\n",
        "    return top_classes\n",
        "\n",
        "# def evaluate_model(model_path, data_df, title, images_dir):\n",
        "#     model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "#     for num in data_df[\"image_num\"]:\n",
        "#         img_path = f\"{images_dir}/{num}.png\"\n",
        "#         img_tensor = preprocess_image(img_path)\n",
        "#         top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "#         for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "#             data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "#             data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "def evaluate_model(model_path, data_df, title, images_dir):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "\n",
        "    # Wrapping the iteration with tqdm for progress tracking\n",
        "    for num in tqdm(data_df[\"image_num\"], desc=f\"Evaluating {title}\"):\n",
        "        img_path = f\"{images_dir}/{num}.png\"\n",
        "        img_tensor = preprocess_image(img_path)\n",
        "        top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "        for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "            data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "            data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "# Example usage\n",
        "weight_paths = [baseline_weight_path, mixed_weight_path, finetune_mixed_weight_path, finetune_100ep_weight_path, finetune_150ep_weight_path, finetune_200ep_weight_path, finetune_250ep_weight_path, finetune_300ep_weight_path, finetune_350ep_weight_path, finetune_400ep_weight_path]\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "for path, title in zip(weight_paths, titles):\n",
        "    evaluate_model(path, cornea_journal_df, title, images_dir)\n"
      ],
      "metadata": {
        "id": "lxMkzMo-jNog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/cornea_journal_all.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "e2YGhZgdSg1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load data\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/cornea_journal_all.csv\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "cornea_journal_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "cornea_journal_df.head()\n"
      ],
      "metadata": {
        "id": "dYatGMIrolDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "r8xMxymdUoq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = cornea_journal_df\n",
        "\n",
        "# Titles for the different model evaluations\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "def draw_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for top1_{title}')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Calculate and display confusion matrices\n",
        "for title in titles:\n",
        "    cm = confusion_matrix(df['class_name'], df[f'top1_{title}'], labels=class_names_order)\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    accuracies[title] = accuracy\n",
        "    draw_confusion_matrix(cm, title)\n",
        "\n",
        "# Print all accuracies at the end\n",
        "for title, accuracy in accuracies.items():\n",
        "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "vdMnTluOeH3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference_Maehara_dataset**"
      ],
      "metadata": {
        "id": "bTj36ASWlo3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the zip file and the extraction directory\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "new_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\""
      ],
      "metadata": {
        "id": "aLTTuAJ6lstO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# DataFrame 'df' にExcelファイルを読み込む\n",
        "df = pd.read_excel(excel_path)\n",
        "df.head()\n",
        "\n",
        "# maehara_dfを作成し、1列目にslit_id、2列目にdisease_Englishを含める\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "maehara_df.head()\n"
      ],
      "metadata": {
        "id": "4FM6clySpXX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ファイルの存在確認\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Excelファイルを読み込む\n",
        "excel_data = pd.read_excel(excel_path)\n",
        "\n",
        "# 'basename'列の値を文字列に変換\n",
        "excel_data['basename'] = excel_data['basename'].astype(str)\n",
        "\n",
        "# チェックするディレクトリのパス\n",
        "image_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット'\n",
        "\n",
        "# エラーメッセージのリスト\n",
        "errors = []\n",
        "\n",
        "# 各basenameに対して、.jpgまたは.pngファイルがあるかどうかを確認\n",
        "for basename in excel_data['basename']:\n",
        "    jpg_file = os.path.join(image_dir, basename + '.jpg')\n",
        "    JPG_file = os.path.join(image_dir, basename + '.JPG')\n",
        "    png_file = os.path.join(image_dir, basename + '.png')\n",
        "\n",
        "    if not os.path.exists(jpg_file) and not os.path.exists(png_file) and not os.path.exists(JPG_file):\n",
        "        errors.append(f\"Error: Neither {basename}.jpg nor {basename}.JPG nor {basename}.png exists in the directory.\")\n",
        "\n",
        "# エラーメッセージを表示\n",
        "for error in errors:\n",
        "    print(error)\n",
        "\n",
        "# エラーがあるかどうかを確認\n",
        "if len(errors) > 0:\n",
        "    print(f\"Total missing files: {len(errors)}\")\n",
        "else:\n",
        "    print(\"All files are present.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uQmF9ZzyazAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(image, size=(640, 480)):\n",
        "    ih, iw = image.shape[:2]\n",
        "    w, h = size\n",
        "\n",
        "    # Calculate padding to maintain aspect ratio\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw, nh = int(scale * iw), int(scale * ih)\n",
        "    image = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    # Calculate padding dimensions\n",
        "    top = (h - nh) // 2\n",
        "    bottom = h - nh - top\n",
        "    left = (w - nw) // 2\n",
        "    right = w - nw - left\n",
        "\n",
        "    # Add padding to the image\n",
        "    return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2, size=(640, 480))\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "RO0ce7B3rPZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "nyBUtMe3rPbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "\n",
        "    # # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    # img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # # Get current figure size\n",
        "    # fig_size = plt.gcf().get_size_inches()\n",
        "    # # Set new size (half the current size)\n",
        "    # plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    # plt.imshow(img_mpl)\n",
        "    # plt.axis('off')  # Turn off axis numbers\n",
        "    # plt.show()\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    # print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        # print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "        # print(\"\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model_path, maehara_df, title, images_dir):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "    results = []\n",
        "\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = maehara_df.copy()\n",
        "\n",
        "    for index, row in tqdm(df_copy.iterrows(), total=df_copy.shape[0], desc=f\"Evaluating {title}\"):\n",
        "        img_id = row['basename']\n",
        "        # Try different file extensions\n",
        "        for ext in ['.jpg', '.png', '.JPG']:\n",
        "            img_path = os.path.join(images_dir, f\"{img_id}{ext}\")\n",
        "            if os.path.exists(img_path):\n",
        "                print(f\"Processing image: {img_path}\")\n",
        "                img_tensor = preprocess_image(img_path)\n",
        "                top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "                # Store results in a list to avoid SettingWithCopyWarning\n",
        "                for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "                    results.append((index, f'top{i+1}_{title}', class_names[class_num]))\n",
        "                    results.append((index, f'top{i+1}_{title}_likelihood', likelihood))\n",
        "                break\n",
        "        else:\n",
        "            print(f\"No image found for ID {img_id} with any of the extensions .jpg, .png, .JPG\")\n",
        "\n",
        "    # Apply the updates outside the loop\n",
        "    for index, column, value in results:\n",
        "        df_copy.at[index, column] = value\n",
        "\n",
        "    # Return the modified DataFrame\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "bA-QeMo7kX10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "weight_paths = [baseline_weight_path, mixed_weight_path, finetune_mixed_weight_path, finetune_100ep_weight_path, finetune_150ep_weight_path, finetune_200ep_weight_path, finetune_250ep_weight_path, finetune_300ep_weight_path, finetune_350ep_weight_path, finetune_400ep_weight_path]\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "# weight_paths = [baseline_weight_path]\n",
        "# titles = [\"baseline\"]\n",
        "\n",
        "for path, title in zip(weight_paths, titles):\n",
        "    maehara_df = evaluate_model(path, maehara_df, title, images_dir)"
      ],
      "metadata": {
        "id": "oWVXdG61kaXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maehara_df"
      ],
      "metadata": {
        "id": "aCY-E6Bqfmkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = maehara_df.dropna() #画像がなく評価が行われていない行を削除\n",
        "\n",
        "# Titles for the different model evaluations\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "def draw_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for top1_{title}')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Calculate and display confusion matrices\n",
        "for title in titles:\n",
        "    cm = confusion_matrix(df['disease_English'], df[f'top1_{title}'], labels=class_names_order)\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    accuracies[title] = accuracy\n",
        "    draw_confusion_matrix(cm, title)\n",
        "\n",
        "# Print all accuracies at the end\n",
        "for title, accuracy in accuracies.items():\n",
        "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "UtMAk6uZrPfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_df.xlsx\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "dqlVIZ1zrPhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load data\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_df.xlsx\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "maehara_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "maehara_df.head()\n"
      ],
      "metadata": {
        "id": "V5zPrMJLrPjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Augmentation_ideas**"
      ],
      "metadata": {
        "id": "OBSNECHYW6zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import glob\n",
        "\n",
        "# Define the path to your image\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = f\"{images_dir}/29.png\"\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[1]\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "def rgb_to_bgr(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to BGR. \"\"\"\n",
        "    # Reverse the order of the first and third channel\n",
        "    return img[..., ::-1]\n",
        "\n",
        "def rgb_to_grb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to GRB. \"\"\"\n",
        "    # Swap the red and green channels\n",
        "    return img[..., [1, 0, 2]]\n",
        "\n",
        "def rgb_to_rbg(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 2, 1]]\n",
        "\n",
        "\n",
        "\n",
        "# Define an augmentation pipeline with Albumentations\n",
        "augmentation = A.Compose([\n",
        "    A.Lambda(image=rgb_to_bgr, p=1.0), # Custom RGB to BGR conversion\n",
        "    #A.Lambda(image=rgb_to_rbg, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.Lambda(image=rgb_to_grb, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.RandomBrightnessContrast(p=1.0, brightness_limit=[0.3,0.3], contrast_limit=[-0.5,-0.5]),\n",
        "])\n",
        "\n",
        "# Apply the augmentation pipeline to the image\n",
        "augmented_img = augmentation(image=img)['image']\n",
        "\n",
        "# Display the augmented image using matplotlib\n",
        "plt.imshow(augmented_img)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GCOz71B2XAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "\n",
        "    # # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    # img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # # Get current figure size\n",
        "    # fig_size = plt.gcf().get_size_inches()\n",
        "    # # Set new size (half the current size)\n",
        "    # plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    # plt.imshow(img_mpl)\n",
        "    # plt.axis('off')  # Turn off axis numbers\n",
        "    # plt.show()\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    # print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        # print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "        # print(\"\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "def setup_model(model_path):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def check_groundtruth(maehara_df, image_dir):\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = maehara_df.copy()\n",
        "\n",
        "    img_id = os.path.basename(image_dir).split(\".\")[0]\n",
        "    groundtruth = df_copy.loc[df_copy['basename'] == img_id, 'disease_English']\n",
        "    return groundtruth.item()\n",
        "\n",
        "def bright_contarst_augment(img, brightness, contrast):\n",
        "    augmentation = A.Compose([\n",
        "        A.RandomBrightnessContrast(p=1.0, brightness_limit=[brightness,brightness], contrast_limit=[contrast,contrast])\n",
        "    ])\n",
        "    # Apply the augmentation\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n",
        "def coarse_dropout(img):\n",
        "    drop_size = round(img.shape[1]*0.02)\n",
        "    augmentation = A.Compose([\n",
        "        #A.CoarseDropout(p=1.0, max_holes=200, max_height=drop_size, max_width=drop_size, min_holes=200, min_height=drop_size, min_width=drop_size, fill_value=(114,114,114))\n",
        "        A.CoarseDropout(p=1.0, max_holes=200, max_height=drop_size, max_width=drop_size, min_holes=200, min_height=drop_size, min_width=drop_size, fill_value=(60,68,124))\n",
        "        ])\n",
        "    # Apply the augmentation\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n",
        "def rgb_to_bgr(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to BGR. \"\"\"\n",
        "    # Reverse the order of the first and third channel\n",
        "    return img[..., ::-1]\n",
        "\n",
        "def rgb_to_grb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to GRB. \"\"\"\n",
        "    # Swap the red and green channels\n",
        "    return img[..., [1, 0, 2]]\n",
        "\n",
        "def rgb_to_rbg(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 2, 1]]\n",
        "\n",
        "def no_augment(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 1, 2]]\n",
        "\n",
        "def rgb_to_gb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to _GR (Red channel set to 0). \"\"\"\n",
        "    # Set the red channel (first channel) to 0\n",
        "    img[..., 0] = 0\n",
        "    return img\n",
        "\n",
        "def rgb_to_rb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RB (Green channel set to 0). \"\"\"\n",
        "    # Set the green channel (second channel) to 0\n",
        "    img[..., 1] = 0\n",
        "    return img\n",
        "\n",
        "def rgb_to_sepia(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RB (Green channel set to 0). \"\"\"\n",
        "    #全画素についてB(青)の輝度を0.3倍、G(緑)の輝度を0.8倍、R(赤)の輝度はそのまま\n",
        "    img[:, :, 0] =img[:, :, 0] * 0.3\n",
        "    img[:, :, 1] =img[:, :, 1] * 0.8\n",
        "    img[:, :, 2] =img[:, :, 2]\n",
        "    return img\n",
        "\n",
        "\n",
        "def rgb_to_grayscale(img, **kwargs):\n",
        "    \"\"\" Convert a 3-channel RGB image to a 3-channel grayscale image. \"\"\"\n",
        "    # Convert to grayscale using OpenCV\n",
        "    grayscale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    # Stack the grayscale values across the three channels\n",
        "    grayscale_3channel = cv2.cvtColor(grayscale, cv2.COLOR_GRAY2RGB)\n",
        "    return grayscale_3channel\n",
        "\n",
        "def RGB_augment(img, colorpattern):\n",
        "    augmentation = A.Compose([\n",
        "        A.Lambda(image=colorpattern, p=1.0), # rgb_to_bgr, rgb_to_grb, or rgb_to_rbg\n",
        "    ])\n",
        "    # Apply the augmentation pipeline to the image\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n"
      ],
      "metadata": {
        "id": "eIJREB8qXAXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### RGB to BGR, RBG, & GRB ###############\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "df = pd.read_excel(excel_path)\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "model = setup_model(baseline_weight_path)\n",
        "\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[40]\n",
        "\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = glob.glob(f\"{images_dir}/*.png\")[2]\n",
        "\n",
        "\n",
        "try:\n",
        "    groundtruth = check_groundtruth(maehara_df, image_path)\n",
        "except:\n",
        "    groundtruth = None\n",
        "    pass\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img2 = img.copy()\n",
        "\n",
        "for colorpattern in [no_augment, rgb_to_bgr, rgb_to_grb, rgb_to_rbg, rgb_to_grayscale, rgb_to_gb, rgb_to_sepia]:\n",
        "    print(f\"Applying transformation: {colorpattern.__name__}\")\n",
        "    augmented_img = RGB_augment(img2, colorpattern)\n",
        "    augmented_img = coarse_dropout(augmented_img)\n",
        "    # augmented_img = RGB_augment(img2, colorpattern)\n",
        "    img = letterbox_image(augmented_img)\n",
        "    img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img_tensor = torch.from_numpy(img).float() / 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "    top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "    top1_class = class_names[top3_results[0][0]]\n",
        "    top1_likelihood = top3_results[0][1]\n",
        "    try:\n",
        "        print(f\"Groundtruth: {groundtruth}\")\n",
        "    except:\n",
        "        pass\n",
        "    print(f\"pred: {top1_class}, likelihood: {top1_likelihood}\")\n",
        "\n",
        "    augmented_img = cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB) #cv2 --> PIL\n",
        "    plt.imshow(augmented_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "G616y4PX5Tpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## Brightness & contrast ############\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "df = pd.read_excel(excel_path)\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "model = setup_model(baseline_weight_path)\n",
        "\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[37]\n",
        "\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = glob.glob(f\"{images_dir}/*.png\")[14]\n",
        "\n",
        "try:\n",
        "    groundtruth = check_groundtruth(maehara_df, image_path)\n",
        "except:\n",
        "    groundtruth = None\n",
        "    pass\n",
        "\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img2 = img.copy()\n",
        "\n",
        "for brightness in [-0.2, 0, 0.2, 0.4]:\n",
        "    for contrast in [-0.8, -0.4, -0.2, 0, 0.2, 0.4]:\n",
        "        print(f\"Brightness: {brightness}, Contrast: {contrast}\")\n",
        "        augmented_img = coarse_dropout(img2)\n",
        "        augmented_img = bright_contarst_augment(augmented_img, brightness, contrast)\n",
        "        img = letterbox_image(augmented_img)\n",
        "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img_tensor = torch.from_numpy(img).float() / 255\n",
        "        img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "        top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "        top1_class = class_names[top3_results[0][0]]\n",
        "        top1_likelihood = top3_results[0][1]\n",
        "        try:\n",
        "            print(f\"Groundtruth: {groundtruth}\")\n",
        "        except:\n",
        "            pass\n",
        "        print(f\"pred: {top1_class}, likelihood: {top1_likelihood}\")\n",
        "\n",
        "        augmented_img = cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB) #cv2 --> PIL\n",
        "        plt.imshow(augmented_img)\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "lhvjVRbhAg6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an augmentation pipeline with Albumentations\n",
        "augmentation = A.Compose([\n",
        "    A.Lambda(image=rgb_to_bgr, p=1.0), # Custom RGB to BGR conversion\n",
        "    #A.Lambda(image=rgb_to_rbg, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.Lambda(image=rgb_to_grb, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.RandomBrightnessContrast(p=1.0, brightness_limit=[0.3,0.3], contrast_limit=[-0.5,-0.5]),\n",
        "])"
      ],
      "metadata": {
        "id": "LwCaj-QgXAaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}