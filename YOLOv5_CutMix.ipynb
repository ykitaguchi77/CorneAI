{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoMgMUagvF4K5aDaEOJ3fF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_CutMix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_CutMix**"
      ],
      "metadata": {
        "id": "ewqwD__x-0fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iZzkoFNN-RlM",
        "outputId": "82dca0d4-6b76-4af3-9fe7-31f559899d5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# prompt: gdriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "os.listdir(folder_path)\n"
      ],
      "metadata": {
        "id": "XoZXOidNATKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94bf11d2-d709-4ccd-e00e-7477817b7904"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['back_scar',\n",
              " 'back_bullous',\n",
              " 'back_infection',\n",
              " 'back_APAC',\n",
              " 'back_deposit',\n",
              " 'back_tumor',\n",
              " 'back_lens-opacity',\n",
              " 'back_non-infection']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**inference YOLOv5**"
      ],
      "metadata": {
        "id": "T4ZqYUcWUWIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXyDmJVDZgpB",
        "outputId": "9ad26039-27df-46ab-da78-4d448f512a32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 17.92 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMj4HMy4aK2K",
        "outputId": "217498cf-4322-4e37-b214-c9e46f860924"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/yolov5-gradcam')"
      ],
      "metadata": {
        "id": "QaT7J5-8aWIX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "yDA2QCgoaO5t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "img_dir = os.path.join(folder_path, os.listdir(folder_path)[0])\n",
        "img_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "qxAxzq_Ya6K9",
        "outputId": "a64803be-b5ed-4b39-85d5-7ffb643c37ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img/back_scar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "# 以下の関数は変更なし\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "# YOLOV5GradCAMクラスを更新\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []  # クラス名を格納するリストを初期化\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        return alpha.view(b, k, 1, 1)\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)\n",
        "        weights = (positive_gradients * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        return eigen_cam.unsqueeze(1)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)"
      ],
      "metadata": {
        "id": "LqyqTc0ibAoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = os.listdir(folder_path)\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2.imwrite(output_path, final_image)\n",
        "        #cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = img_dir\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/content/outputs'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "    #for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ],
      "metadata": {
        "id": "AMp_rx-yjfal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAMなしバージョン**"
      ],
      "metadata": {
        "id": "qM6lGDVP6NcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Apply finite constraint\n",
        "            # if not torch.isfinite(x).all():\n",
        "            #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    for subdir in ['back_scar', 'back_bullous', 'back_infection', 'back_APAC', 'back_deposit', 'back_tumor', 'back_lens-opacity', 'back_non-infection']:\n",
        "        subdir_path = os.path.join(folder_path, subdir)\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            print(f\"Warning: Subdirectory {subdir} not found in {folder_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing subdirectory: {subdir}\")\n",
        "        file_list = os.listdir(subdir_path)\n",
        "\n",
        "        for item in file_list:\n",
        "            img_path = os.path.join(subdir_path, item)\n",
        "            img_basename = os.path.basename(img_path)\n",
        "            print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "            top_class, top_prob = process_image(img_path, model)\n",
        "            print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)  # 入力画像サイズ\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"])\n",
        "    folder_main(folder_path, model)"
      ],
      "metadata": {
        "id": "xd0WYkcXZgcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Apply finite constraint\n",
        "            # if not torch.isfinite(x).all():\n",
        "            #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        print(f\"Processing directory: {root}\")\n",
        "\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "                print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "                print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)  # 入力画像サイズ\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"])\n",
        "    folder_main(folder_path, model)"
      ],
      "metadata": {
        "id": "mai-uKp1ibJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe to csv\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    results = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "\n",
        "                # ファイル名をハイフンでsplitし、1番目と3番目を取る\n",
        "                parts = img_basename.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    cornea = parts[0]\n",
        "                    background = parts[2].split('.')[0]\n",
        "                    results.append({\n",
        "                        \"cornea\": cornea,\n",
        "                        \"background\": background,\n",
        "                        \"pred\": top_class,\n",
        "                        \"prob\": top_prob\n",
        "                    })\n",
        "\n",
        "                print(f\"Processing image: {img_basename} Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"apac\",\"lens-opacity\",\"bullous\"])\n",
        "\n",
        "    df = folder_main(folder_path, model)\n",
        "\n",
        "    print(\"\\nDataFrame:\")\n",
        "    print(df)\n",
        "\n",
        "    csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nCSV saved to: {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0iUX9rL7Rzm",
        "outputId": "d6e160a6-31cd-4eb3-8820-869b7e422cb5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Processing folder: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "Processing image: apac_5_scar.png Top class: bullous, Probability: 0.8800\n",
            "Processing image: apac_49_scar.png Top class: scar, Probability: 0.8100\n",
            "Processing image: apac_124_scar.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_203_scar.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_221_scar.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_scar.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_139_scar.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_scar.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_191_scar.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_scar.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_scar.png Top class: scar, Probability: 0.9200\n",
            "Processing image: infection_29_scar.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: infection_75_scar.png Top class: infection, Probability: 0.7300\n",
            "Processing image: infection_81_scar.png Top class: scar, Probability: 0.7100\n",
            "Processing image: infection_97_scar.png Top class: bullous, Probability: 0.8100\n",
            "Processing image: normal_8_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_73_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_159_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_189_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: scar_89_scar.png Top class: scar, Probability: 0.9100\n",
            "Processing image: scar_82_scar.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_114_scar.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_127_scar.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_scar.png Top class: scar, Probability: 0.8400\n",
            "Processing image: tumor_15_scar.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: scar_95_scar.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: scar_98_scar.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_183_scar.png Top class: tumor, Probability: 0.8600\n",
            "Processing image: tumor_220_scar.png Top class: tumor, Probability: 0.9300\n",
            "Processing image: deposit_70_scar.png Top class: deposit, Probability: 0.9400\n",
            "Processing image: lens-opacity_43_scar.png Top class: lens-opacity, Probability: 0.7900\n",
            "Processing image: lens-opacity_85_scar.png Top class: lens-opacity, Probability: 0.9000\n",
            "Processing image: lens-opacity_116_scar.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_196_scar.png Top class: lens-opacity, Probability: 0.9000\n",
            "Processing image: lens-opacity_205_scar.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: non-infection_99_scar.png Top class: non-infection, Probability: 0.8200\n",
            "Processing image: non-infection128_scar.png Top class: scar, Probability: 0.9100\n",
            "Processing image: non-infection_195_scar.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_210_scar.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: non-infection_223_scar.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: apac_5_bullous.png Top class: deposit, Probability: 0.6900\n",
            "Processing image: apac_49_bullous.png Top class: scar, Probability: 0.4500\n",
            "Processing image: apac_124_bullous.png Top class: apac, Probability: 0.9300\n",
            "Processing image: apac_203_bullous.png Top class: apac, Probability: 0.9400\n",
            "Processing image: apac_221_bullous.png Top class: apac, Probability: 0.9600\n",
            "Processing image: bullous_132_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_139_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_143_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_bullous.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_64_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_70_bullous.png Top class: deposit, Probability: 0.8300\n",
            "Processing image: deposit_77_bullous.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_75_bullous.png Top class: scar, Probability: 0.4900\n",
            "Processing image: infection_81_bullous.png Top class: infection, Probability: 0.6500\n",
            "Processing image: infection_97_bullous.png Top class: non-infection, Probability: 0.8500\n",
            "Processing image: normal_8_bullous.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_bullous.png Top class: non-infection, Probability: 0.6400\n",
            "Processing image: normal_110_bullous.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_159_bullous.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_189_bullous.png Top class: normal, Probability: 0.9500\n",
            "Processing image: scar_69_bullous.png Top class: scar, Probability: 0.7300\n",
            "Processing image: scar_82_bullous.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_114_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_bullous.png Top class: scar, Probability: 0.9000\n",
            "Processing image: tumor_15_bullous.png Top class: tumor, Probability: 0.8400\n",
            "Processing image: tumor_95_bullous.png Top class: tumor, Probability: 0.8400\n",
            "Processing image: tumor_98_bullous.png Top class: tumor, Probability: 0.8600\n",
            "Processing image: tumor_183_bullous.png Top class: tumor, Probability: 0.8900\n",
            "Processing image: tumor_220_bullous.png Top class: tumor, Probability: 0.8900\n",
            "Processing image: infection_29_bullous.png Top class: scar, Probability: 0.8900\n",
            "Processing image: infection_1_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: lens-opacity_85_bullous.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: lens-opacity_196_bullous.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: lens-opacity_43_bullous.png Top class: lens-opacity, Probability: 0.5400\n",
            "Processing image: lens-opacity_116_bullous.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_205_bullous.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_99_bullous.png Top class: non-infection, Probability: 0.9600\n",
            "Processing image: non-infection_210_bullous.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_223_bullous.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: non-infection_128_bullous.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: non-infection_195_bullous.png Top class: non-infection, Probability: 0.9600\n",
            "Processing image: apac_5_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: apac_124_infection.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_203_infection.png Top class: apac, Probability: 0.9800\n",
            "Processing image: apac_221_infection.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_49_infection.png Top class: infection, Probability: 0.9200\n",
            "Processing image: bullous_132_infection.png Top class: infection, Probability: 0.4200\n",
            "Processing image: bullous_139_infection.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: bullous_143_infection.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_infection.png Top class: bullous, Probability: 0.8200\n",
            "Processing image: bullous_212_infection.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: deposit_57_infection.png Top class: infection, Probability: 0.9500\n",
            "Processing image: deposit_64_infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_70_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_77_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_138_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_1_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: infection_29_infection.png Top class: infection, Probability: 0.7400\n",
            "Processing image: infection_75_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: infection_81_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: infection_97_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: normal_8_infection.png Top class: infection, Probability: 0.9400\n",
            "Processing image: normal_73_infection.png Top class: infection, Probability: 0.9000\n",
            "Processing image: normal_110_infection.png Top class: non-infection, Probability: 0.5900\n",
            "Processing image: normal_159_infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_189_infection.png Top class: infection, Probability: 0.8800\n",
            "Processing image: scar_69_infection.png Top class: infection, Probability: 0.9500\n",
            "Processing image: scar_82_infection.png Top class: non-infection, Probability: 0.7400\n",
            "Processing image: scar_114_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: scar_127_infection.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_186_infection.png Top class: infection, Probability: 0.6500\n",
            "Processing image: tumor_15_infection.png Top class: tumor, Probability: 0.8800\n",
            "Processing image: tumor_95_infection.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_98_infection.png Top class: infection, Probability: 0.6100\n",
            "Processing image: tumor_183_infection.png Top class: tumor, Probability: 0.7400\n",
            "Processing image: tumor_220_infection.png Top class: tumor, Probability: 0.9200\n",
            "Processing image: lens-opacity_43_infection.png Top class: non-infection, Probability: 0.4000\n",
            "Processing image: lens-opacity_85_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: lens-opacity_116_infection.png Top class: lens-opacity, Probability: 0.5200\n",
            "Processing image: lens-opacity_196_infection.png Top class: non-infection, Probability: 0.7300\n",
            "Processing image: lens-opacity_205_infection.png Top class: lens-opacity, Probability: 0.4700\n",
            "Processing image: non-infection_99_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_128_infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_195_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_210_infection.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: non-infection_223_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: apac_5_apac.png Top class: bullous, Probability: 0.8400\n",
            "Processing image: apac_49_apac.png Top class: infection, Probability: 0.4400\n",
            "Processing image: apac_124_apac.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_203_apac.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_221_apac.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_apac.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_139_apac.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_143_apac.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_191_apac.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_apac.png Top class: bullous, Probability: 0.9000\n",
            "Processing image: deposit_57_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_70_apac.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_77_apac.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_138_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_apac.png Top class: scar, Probability: 0.9000\n",
            "Processing image: infection_75_apac.png Top class: infection, Probability: 0.9300\n",
            "Processing image: infection_81_apac.png Top class: infection, Probability: 0.9500\n",
            "Processing image: infection_97_apac.png Top class: bullous, Probability: 0.8300\n",
            "Processing image: normal_8_apac.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_73_apac.png Top class: non-infection, Probability: 0.5800\n",
            "Processing image: normal_110_apac.png Top class: normal, Probability: 0.9300\n",
            "Processing image: normal_159_apac.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_189_apac.png Top class: normal, Probability: 0.9100\n",
            "Processing image: scar_69_apac.png Top class: bullous, Probability: 0.7400\n",
            "Processing image: scar_82_apac.png Top class: scar, Probability: 0.9000\n",
            "Processing image: scar_114_apac.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_127_apac.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_apac.png Top class: scar, Probability: 0.9100\n",
            "Processing image: tumor_15_apac.png Top class: tumor, Probability: 0.9100\n",
            "Processing image: tumor_95_apac.png Top class: tumor, Probability: 0.8800\n",
            "Processing image: tumor_98_apac.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_183_apac.png Top class: tumor, Probability: 0.9300\n",
            "Processing image: tumor_220_apac.png Top class: tumor, Probability: 0.9100\n",
            "Processing image: infection_29_apac.png Top class: bullous, Probability: 0.9000\n",
            "Processing image: lens-opacity_43_apac.png Top class: lens-opacity, Probability: 0.5400\n",
            "Processing image: lens-opacity_85_apac.png Top class: lens-opacity, Probability: 0.8700\n",
            "Processing image: lens-opacity_116_apac.png Top class: lens-opacity, Probability: 0.8600\n",
            "Processing image: lens-opacity_196_apac.png Top class: lens-opacity, Probability: 0.6600\n",
            "Processing image: lens-opacity_205_apac.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_210_apac.png Top class: non-infection, Probability: 0.8200\n",
            "Processing image: non-infection_99_apac.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: non-infection_223_apac.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_128_apac.png Top class: scar, Probability: 0.8400\n",
            "Processing image: non-infection_195_apac.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: apac_5_deposit.png Top class: bullous, Probability: 0.3000\n",
            "Processing image: apac_49_deposit.png Top class: deposit, Probability: 0.7200\n",
            "Processing image: apac_124_deposit.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_203_deposit.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_221_deposit.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_deposit.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: bullous_139_deposit.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_deposit.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_deposit.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_deposit.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: deposit_57_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_64_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_70_deposit.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_138_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: infection_1_deposit.png Top class: scar, Probability: 0.9400\n",
            "Processing image: infection_29_deposit.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: infection_75_deposit.png Top class: infection, Probability: 0.8300\n",
            "Processing image: infection_81_deposit.png Top class: deposit, Probability: 0.7600\n",
            "Processing image: infection_97_deposit.png Top class: scar, Probability: 0.4600\n",
            "Processing image: normal_8_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_73_deposit.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_159_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_189_deposit.png Top class: normal, Probability: 0.9600\n",
            "Processing image: scar_69_deposit.png Top class: scar, Probability: 0.7800\n",
            "Processing image: scar_82_deposit.png Top class: scar, Probability: 0.9100\n",
            "Processing image: scar_114_deposit.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_deposit.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_186_deposit.png Top class: scar, Probability: 0.8700\n",
            "Processing image: tumor_15_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_deposit.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_183_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_220_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: lens-opacity_43_deposit.png Top class: lens-opacity, Probability: 0.8500\n",
            "Processing image: lens-opacity_85_deposit.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: lens-opacity_116_deposit.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: lens-opacity_196_deposit.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: lens-opacity_205_deposit.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_99_deposit.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_128_deposit.png Top class: scar, Probability: 0.4800\n",
            "Processing image: non-infection_195_deposit.png Top class: non-infection, Probability: 0.8600\n",
            "Processing image: non-infection_210_deposit.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: non-infection_223_deposit.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: apac_221_tumor.png Top class: apac, Probability: 0.9000\n",
            "Processing image: apac_5_tumor.png Top class: bullous, Probability: 0.5300\n",
            "Processing image: apac_124_tumor.png Top class: apac, Probability: 0.9500\n",
            "Processing image: apac_203_tumor.png Top class: apac, Probability: 0.9300\n",
            "Processing image: bullous_132_tumor.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_139_tumor.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_143_tumor.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_191_tumor.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_tumor.png Top class: bullous, Probability: 0.9700\n",
            "Processing image: deposit_57_tumor.png Top class: deposit, Probability: 0.9500\n",
            "Processing image: deposit_64_tumor.png Top class: deposit, Probability: 0.9400\n",
            "Processing image: deposit_70_tumor.png Top class: deposit, Probability: 0.9300\n",
            "Processing image: deposit_77_tumor.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_tumor.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_tumor.png Top class: scar, Probability: 0.9000\n",
            "Processing image: infection_29_tumor.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: infection_75_tumor.png Top class: infection, Probability: 0.3700\n",
            "Processing image: infection_81_tumor.png Top class: infection, Probability: 0.6800\n",
            "Processing image: infection_97_tumor.png Top class: non-infection, Probability: 0.2700\n",
            "Processing image: normal_8_tumor.png Top class: normal, Probability: 0.9300\n",
            "Processing image: normal_73_tumor.png Top class: normal, Probability: 0.7100\n",
            "Processing image: normal_110_tumor.png Top class: normal, Probability: 0.8300\n",
            "Processing image: normal_159_tumor.png Top class: normal, Probability: 0.8500\n",
            "Processing image: normal_189_tumor.png Top class: normal, Probability: 0.7900\n",
            "Processing image: scar_69_tumor.png Top class: bullous, Probability: 0.4700\n",
            "Processing image: scar_82_tumor.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_114_tumor.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_tumor.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_186_tumor.png Top class: scar, Probability: 0.6900\n",
            "Processing image: tumor_15_tumor.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_95_tumor.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_tumor.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_183_tumor.png Top class: tumor, Probability: 0.9000\n",
            "Processing image: tumor_220_tumor.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: apac_49_tumor.png Top class: deposit, Probability: 0.4400\n",
            "Processing image: lens-opacity_43_tumor.png Top class: lens-opacity, Probability: 0.5000\n",
            "Processing image: lens-opacity_85_tumor.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: lens-opacity_116_tumor.png Top class: lens-opacity, Probability: 0.5900\n",
            "Processing image: lens-opacity_196_tumor.png Top class: lens-opacity, Probability: 0.7600\n",
            "Processing image: lens-opacity_205_tumor.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: non-infection_99_tumor.png Top class: non-infection, Probability: 0.8200\n",
            "Processing image: non-infection_128_tumor.png Top class: scar, Probability: 0.9300\n",
            "Processing image: non-infection_195_tumor.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: non-infection_210_tumor.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: non-infection_223_tumor.png Top class: non-infection, Probability: 0.7500\n",
            "Processing image: apac_5_lens-opacity.png Top class: infection, Probability: 0.4600\n",
            "Processing image: apac_49_lens-opacity.png Top class: scar, Probability: 0.6500\n",
            "Processing image: apac_124_lens-opacity.png Top class: apac, Probability: 0.9100\n",
            "Processing image: apac_203_lens-opacity.png Top class: apac, Probability: 0.9500\n",
            "Processing image: apac_221_lens-opacity.png Top class: apac, Probability: 0.8400\n",
            "Processing image: bullous_132_lens-opacity.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: bullous_139_lens-opacity.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_143_lens-opacity.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_191_lens-opacity.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_lens-opacity.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_lens-opacity.png Top class: deposit, Probability: 0.8700\n",
            "Processing image: deposit_64_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_70_lens-opacity.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_77_lens-opacity.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_138_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_lens-opacity.png Top class: infection, Probability: 0.7800\n",
            "Processing image: infection_29_lens-opacity.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: infection_75_lens-opacity.png Top class: infection, Probability: 0.7400\n",
            "Processing image: infection_81_lens-opacity.png Top class: infection, Probability: 0.7000\n",
            "Processing image: infection_97_lens-opacity.png Top class: bullous, Probability: 0.5000\n",
            "Processing image: lens-opacity_43_lens-opacity.png Top class: lens-opacity, Probability: 0.5100\n",
            "Processing image: lens-opacity_85_lens-opacity.png Top class: lens-opacity, Probability: 0.8800\n",
            "Processing image: lens-opacity_116_lens-opacity.png Top class: lens-opacity, Probability: 0.8700\n",
            "Processing image: lens-opacity_196_lens-opacity.png Top class: lens-opacity, Probability: 0.8400\n",
            "Processing image: lens-opacity_205_lens-opacity.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_99_lens-opacity.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_128_lens-opacity.png Top class: scar, Probability: 0.5700\n",
            "Processing image: non-infection_195_lens-opacity.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_210_lens-opacity.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: non-infection_223_lens-opacity.png Top class: non-infection, Probability: 0.8000\n",
            "Processing image: normal_8_lens-opacity.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_lens-opacity.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_lens-opacity.png Top class: non-infection, Probability: 0.6900\n",
            "Processing image: normal_159_lens-opacity.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_189_lens-opacity.png Top class: normal, Probability: 0.9500\n",
            "Processing image: scar_69_lens-opacity.png Top class: bullous, Probability: 0.4000\n",
            "Processing image: scar_82_lens-opacity.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_114_lens-opacity.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_lens-opacity.png Top class: scar, Probability: 0.9600\n",
            "Processing image: scar_186_lens-opacity.png Top class: scar, Probability: 0.9200\n",
            "Processing image: tumor_15_lens-opacity.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_96_lens-opacity.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_lens-opacity.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_183_lens-opacity.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_220_lens-opacity.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: apac_5_non-infection.png Top class: infection, Probability: 0.3600\n",
            "Processing image: apac_49_non-infection.png Top class: scar, Probability: 0.5400\n",
            "Processing image: apac_124_non-infection.png Top class: apac, Probability: 0.8200\n",
            "Processing image: apac_203_non-infection.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_221_non-infection.png Top class: apac, Probability: 0.8600\n",
            "Processing image: bullous_132_non-infection.png Top class: bullous, Probability: 0.8900\n",
            "Processing image: bullous_139_non-infection.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_143_non-infection.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_101_non-infection.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_212_non-infection.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_non-infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_64_non-infection.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_70_non-infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_non-infection.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_138_non-infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_non-infection.png Top class: infection, Probability: 0.7900\n",
            "Processing image: infection_29_non-infection.png Top class: bullous, Probability: 0.8900\n",
            "Processing image: infection_75_non-infection.png Top class: infection, Probability: 0.9400\n",
            "Processing image: infection_81_non-infection.png Top class: infection, Probability: 0.9200\n",
            "Processing image: infection_97_non-infection.png Top class: bullous, Probability: 0.4400\n",
            "Processing image: lens-opacity_43_non-infection.png Top class: lens-opacity, Probability: 0.5300\n",
            "Processing image: lens-opacity_85_non-infection.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_116_non-infection.png Top class: tumor, Probability: 0.5300\n",
            "Processing image: lens-opacity_196_non-infection.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: lens-opacity_205_non-infection.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_99_non-infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_128_non-infection.png Top class: scar, Probability: 0.7300\n",
            "Processing image: non-infection_195_non-infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_210_non-infection.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: non-infection_223_non-infection.png Top class: non-infection, Probability: 0.7800\n",
            "Processing image: normal_159_non-infection.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_non-infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_73_non-infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_8_non-infection.png Top class: normal, Probability: 0.9700\n",
            "Processing image: scar_159_non-infection.png Top class: bullous, Probability: 0.4600\n",
            "Processing image: scar_82_non-infection.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_114_non-infection.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_non-infection.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_186_non-infection.png Top class: scar, Probability: 0.8700\n",
            "Processing image: tumor_15_non-infection.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: tumor_98_non-infection.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_183_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: tumor_220_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: normal_189_non-infection.png Top class: normal, Probability: 0.8700\n",
            "\n",
            "DataFrame:\n",
            "     cornea     background     pred  prob\n",
            "0      apac           scar  bullous  0.88\n",
            "1      apac           scar     scar  0.81\n",
            "2      apac           scar     apac  0.96\n",
            "3      apac           scar     apac  0.97\n",
            "4      apac           scar     apac  0.97\n",
            "..      ...            ...      ...   ...\n",
            "354   tumor  non-infection    tumor  0.97\n",
            "355   tumor  non-infection    tumor  0.96\n",
            "356   tumor  non-infection    tumor  0.97\n",
            "357   tumor  non-infection    tumor  0.97\n",
            "358  normal  non-infection   normal  0.87\n",
            "\n",
            "[359 rows x 4 columns]\n",
            "\n",
            "CSV saved to: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Draw matrix**"
      ],
      "metadata": {
        "id": "jEmlfJ91BMAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "file_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 正しい判定の割合\n",
        "correct_predictions = df[df['cornea'] == df['pred']]\n",
        "accuracy = len(correct_predictions) / len(df)\n",
        "\n",
        "# 各状態に対する正しい判定の割合\n",
        "correct_by_cornea = df[df['cornea'] == df['pred']].groupby('cornea').size() / df.groupby('cornea').size()\n",
        "\n",
        "# 確率の分布\n",
        "correct_probs = correct_predictions['prob']\n",
        "incorrect_probs = df[df['cornea'] != df['pred']]['prob']\n",
        "\n",
        "# ヒストグラムをプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(correct_probs, bins=10, alpha=0.5, label='Correct Predictions', color='blue')\n",
        "plt.hist(incorrect_probs, bins=10, alpha=0.5, label='Incorrect Predictions', color='red')\n",
        "plt.xlabel('Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Probability Distribution of Correct and Incorrect Predictions')\n",
        "plt.show()\n",
        "\n",
        "# グラウンドトゥルースごとの背景に対する正解率マトリックスを作成\n",
        "matrix = df.pivot_table(index='cornea', columns='background', values='pred', aggfunc=lambda x: (x == df.loc[x.index, 'cornea']).mean())\n",
        "\n",
        "# マトリックス図をプロット\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title('Accuracy Matrix: Cornea vs Background')\n",
        "plt.xlabel('Background')\n",
        "plt.ylabel('Cornea (Ground Truth)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QOWWO0dKBQ1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SWPhfox_BbpG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}