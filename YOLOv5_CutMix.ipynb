{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRvwwCTcVoY74cIxbWvsBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_CutMix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_CutMix**"
      ],
      "metadata": {
        "id": "ewqwD__x-0fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iZzkoFNN-RlM",
        "outputId": "e83e568e-da2f-4bcd-ec61-badb0dc4a55f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# prompt: gdriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "os.listdir(folder_path)\n"
      ],
      "metadata": {
        "id": "XoZXOidNATKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46aa9a45-fbab-4088-809e-c78a40c80840"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['back_normal',\n",
              " 'back_bullous',\n",
              " 'back_infection',\n",
              " 'back_tumor',\n",
              " 'back_deposit',\n",
              " 'back_scar',\n",
              " 'back_non-infection',\n",
              " 'back_apac',\n",
              " 'back_lens-opacity']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Underbar to Hyphen**\n",
        "\n",
        "lens_opacity --> lens-opacity"
      ],
      "metadata": {
        "id": "YMqiKzZsc3i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def rename_item(old_path):\n",
        "    directory, item_name = os.path.split(old_path)\n",
        "    new_name = item_name\n",
        "    if \"lens_opacity\" in item_name:\n",
        "        new_name = new_name.replace(\"lens_opacity\", \"lens-opacity\")\n",
        "    if \"non_infection\" in item_name:\n",
        "        new_name = new_name.replace(\"non_infection\", \"non-infection\")\n",
        "\n",
        "    if new_name != item_name:\n",
        "        new_path = os.path.join(directory, new_name)\n",
        "        os.rename(old_path, new_path)\n",
        "        print(f\"Renamed: {item_name} -> {new_name}\")\n",
        "        return new_path\n",
        "    return old_path\n",
        "\n",
        "def rename_files_and_directories(folder_path):\n",
        "    for root, dirs, files in os.walk(folder_path, topdown=False):\n",
        "        # まずファイルの名前を変更\n",
        "        for file in files:\n",
        "            old_path = os.path.join(root, file)\n",
        "            rename_item(old_path)\n",
        "\n",
        "        # 次にディレクトリの名前を変更\n",
        "        for dir in dirs:\n",
        "            old_path = os.path.join(root, dir)\n",
        "            new_path = rename_item(old_path)\n",
        "\n",
        "            # ディレクトリ名が変更された場合、親ディレクトリのパスも更新\n",
        "            if new_path != old_path:\n",
        "                index = dirs.index(dir)\n",
        "                dirs[index] = os.path.basename(new_path)\n",
        "\n",
        "# 使用例\n",
        "folder_path = \"/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img\"  # ここに実際のフォルダパスを指定してください\n",
        "folder_path = \"/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea\"\n",
        "rename_files_and_directories(folder_path)"
      ],
      "metadata": {
        "id": "LeinskbYdKyK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**inference YOLOv5**"
      ],
      "metadata": {
        "id": "T4ZqYUcWUWIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXyDmJVDZgpB",
        "outputId": "45acdb6a-ece3-4fd7-974c-5392faf04f92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 11.40 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMj4HMy4aK2K",
        "outputId": "aa56c2ec-176c-495c-f868-f7a60edc9a6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/yolov5-gradcam')"
      ],
      "metadata": {
        "id": "QaT7J5-8aWIX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "# 以下の関数は変更なし\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "# YOLOV5GradCAMクラスを更新\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []  # クラス名を格納するリストを初期化\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        return alpha.view(b, k, 1, 1)\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)\n",
        "        weights = (positive_gradients * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        return eigen_cam.unsqueeze(1)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Apply finite constraint\n",
        "            # if not torch.isfinite(x).all():\n",
        "            #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0"
      ],
      "metadata": {
        "id": "yDA2QCgoaO5t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_GradCAM**"
      ],
      "metadata": {
        "id": "V0rW1tHUbFv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = os.listdir(folder_path)\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2.imwrite(output_path, final_image)\n",
        "        #cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "    img_dir = os.path.join(folder_path, os.listdir(folder_path)[0])\n",
        "\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = img_dir\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/content/outputs'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "    #for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ],
      "metadata": {
        "id": "AMp_rx-yjfal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAMなしバージョン**"
      ],
      "metadata": {
        "id": "qM6lGDVP6NcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Apply finite constraint\n",
        "            # if not torch.isfinite(x).all():\n",
        "            #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    for subdir in ['back_scar', 'back_bullous', 'back_infection', 'back_APAC', 'back_deposit', 'back_tumor', 'back_lens-opacity', 'back_non-infection']:\n",
        "        subdir_path = os.path.join(folder_path, subdir)\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            print(f\"Warning: Subdirectory {subdir} not found in {folder_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing subdirectory: {subdir}\")\n",
        "        file_list = os.listdir(subdir_path)\n",
        "\n",
        "        for item in file_list:\n",
        "            img_path = os.path.join(subdir_path, item)\n",
        "            img_basename = os.path.basename(img_path)\n",
        "            print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "            top_class, top_prob = process_image(img_path, model)\n",
        "            print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)  # 入力画像サイズ\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"])\n",
        "    folder_main(folder_path, model)"
      ],
      "metadata": {
        "id": "xd0WYkcXZgcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Apply finite constraint\n",
        "            # if not torch.isfinite(x).all():\n",
        "            #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        print(f\"Processing directory: {root}\")\n",
        "\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "                print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "                print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)  # 入力画像サイズ\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"])\n",
        "    folder_main(folder_path, model)"
      ],
      "metadata": {
        "id": "mai-uKp1ibJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMMRiqfA4e4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "## 画像のpathを指定したinference\n",
        "################################\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_single_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        print(f\"Processing directory: {root}\")\n",
        "\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "                print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "                print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)  # 入力画像サイズ\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"])\n",
        "\n",
        "    # ユーザーに画像のパスを入力してもらう\n",
        "    img_path = \"/content/non_infection_195.png\"\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        top_class, top_prob = process_single_image(img_path, model)\n",
        "        print(f\"Image: {os.path.basename(img_path)}\")\n",
        "        print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "    else:\n",
        "        print(f\"Error: The file {img_path} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j_4xO0a4e6B",
        "outputId": "fa2f410a-1907-40c3-dc7b-54ab808b10e2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "Image: non_infection_195.png\n",
            "Top class: scar, Probability: 0.3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**create dataframe to csv**\n",
        "\n",
        "cutmix-images"
      ],
      "metadata": {
        "id": "9FhPI6scaIRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "## create dataframe to csv ##\n",
        "#############################\n",
        "\n",
        "\"\"\"\n",
        "Underbar --> Hyphenを行ってから\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    results = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "\n",
        "                # ファイル名をハイフンでsplitし、1番目と3番目を取る\n",
        "                parts = img_basename.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    cornea = parts[0]\n",
        "                    image_id = f\"{parts[0]}_{parts[1]}\"\n",
        "                    background = parts[2].split('.')[0]\n",
        "                    results.append({\n",
        "                        \"basename\": img_basename,\n",
        "                        \"image_id\": image_id,\n",
        "                        \"groundtruth\": cornea,\n",
        "                        \"cornea\": cornea,\n",
        "                        \"background\": background,\n",
        "                        \"pred_cutmix\": top_class,\n",
        "                        \"prob_cutmix\": top_prob\n",
        "                    })\n",
        "\n",
        "                print(f\"Processing image: {img_basename} Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"apac\",\"lens-opacity\",\"bullous\"])\n",
        "\n",
        "    df = folder_main(folder_path, model)\n",
        "\n",
        "    print(\"\\nDataFrame:\")\n",
        "    print(df)\n",
        "\n",
        "    csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nCSV saved to: {csv_path}\")"
      ],
      "metadata": {
        "id": "B0iUX9rL7Rzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1ef9e1-ddea-476c-c5e8-e6c12ff791a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Processing folder: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: apac_5_normal.png Top class: deposit, Probability: 0.8600\n",
            "Processing image: apac_124_normal.png Top class: apac, Probability: 0.8500\n",
            "Processing image: apac_203_normal.png Top class: apac, Probability: 0.9200\n",
            "Processing image: apac_221_normal.png Top class: apac, Probability: 0.6400\n",
            "Processing image: bullous_132_normal.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: bullous_139_normal.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_normal.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_normal.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: bullous_212_normal.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: deposit_57_normal.png Top class: deposit, Probability: 0.9200\n",
            "Processing image: deposit_64_normal.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_70_normal.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_77_normal.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_normal.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_1_normal.png Top class: scar, Probability: 0.9000\n",
            "Processing image: infection_29_normal.png Top class: bullous, Probability: 0.6600\n",
            "Processing image: infection_75_normal.png Top class: infection, Probability: 0.8600\n",
            "Processing image: infection_81_normal.png Top class: infection, Probability: 0.5000\n",
            "Processing image: infection_97_normal.png Top class: scar, Probability: 0.5300\n",
            "Processing image: lens-opacity_43_normal.png Top class: lens-opacity, Probability: 0.6400\n",
            "Processing image: lens-opacity_85_normal.png Top class: lens-opacity, Probability: 0.8900\n",
            "Processing image: lens-opacity_116_normal.png Top class: lens-opacity, Probability: 0.8100\n",
            "Processing image: lens-opacity_196_normal.png Top class: lens-opacity, Probability: 0.7900\n",
            "Processing image: non-infection_99_normal.png Top class: non-infection, Probability: 0.8600\n",
            "Processing image: non-infection_128_normal.png Top class: scar, Probability: 0.9100\n",
            "Processing image: non-infection_195_normal.png Top class: normal, Probability: 0.4100\n",
            "Processing image: non-infection_210_normal.png Top class: non-infection, Probability: 0.8300\n",
            "Processing image: non-infection_223_normal.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: normal_8_normal.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_normal.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_normal.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_159_normal.png Top class: normal, Probability: 0.9800\n",
            "Processing image: normal_189_normal.png Top class: normal, Probability: 0.9600\n",
            "Processing image: scar_69_normal.png Top class: scar, Probability: 0.8600\n",
            "Processing image: scar_82_normal.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_114_normal.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_127_normal.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_normal.png Top class: scar, Probability: 0.9400\n",
            "Processing image: tumor_15_normal.png Top class: tumor, Probability: 0.9200\n",
            "Processing image: tumor_95_normal.png Top class: tumor, Probability: 0.9300\n",
            "Processing image: tumor_98_normal.png Top class: tumor, Probability: 0.9200\n",
            "Processing image: tumor_183_normal.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_220_normal.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: lens-opacity_205_normal.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: apac_234_normal.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: apac_5_bullous.png Top class: deposit, Probability: 0.6900\n",
            "Processing image: apac_124_bullous.png Top class: apac, Probability: 0.9300\n",
            "Processing image: apac_203_bullous.png Top class: apac, Probability: 0.9400\n",
            "Processing image: apac_221_bullous.png Top class: apac, Probability: 0.9600\n",
            "Processing image: bullous_132_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_139_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_143_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_bullous.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_64_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_70_bullous.png Top class: deposit, Probability: 0.8300\n",
            "Processing image: deposit_77_bullous.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_75_bullous.png Top class: scar, Probability: 0.4900\n",
            "Processing image: infection_81_bullous.png Top class: infection, Probability: 0.6500\n",
            "Processing image: infection_97_bullous.png Top class: non-infection, Probability: 0.8500\n",
            "Processing image: normal_8_bullous.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_bullous.png Top class: non-infection, Probability: 0.6400\n",
            "Processing image: normal_110_bullous.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_159_bullous.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_189_bullous.png Top class: normal, Probability: 0.9500\n",
            "Processing image: scar_69_bullous.png Top class: scar, Probability: 0.7300\n",
            "Processing image: scar_82_bullous.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_114_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_bullous.png Top class: scar, Probability: 0.9000\n",
            "Processing image: tumor_15_bullous.png Top class: tumor, Probability: 0.8400\n",
            "Processing image: tumor_95_bullous.png Top class: tumor, Probability: 0.8400\n",
            "Processing image: tumor_98_bullous.png Top class: tumor, Probability: 0.8600\n",
            "Processing image: tumor_183_bullous.png Top class: tumor, Probability: 0.8900\n",
            "Processing image: tumor_220_bullous.png Top class: tumor, Probability: 0.8900\n",
            "Processing image: infection_29_bullous.png Top class: scar, Probability: 0.8900\n",
            "Processing image: infection_1_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: apac_234_bullous.png Top class: apac, Probability: 0.6500\n",
            "Processing image: lens-opacity_43_bullous.png Top class: lens-opacity, Probability: 0.5400\n",
            "Processing image: non-infection_195_bullous.png Top class: non-infection, Probability: 0.9600\n",
            "Processing image: non-infection_223_bullous.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: lens-opacity_196_bullous.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: non-infection_210_bullous.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_99_bullous.png Top class: non-infection, Probability: 0.9600\n",
            "Processing image: lens-opacity_205_bullous.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_128_bullous.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: lens-opacity_116_bullous.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_85_bullous.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: apac_5_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: apac_124_infection.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_203_infection.png Top class: apac, Probability: 0.9800\n",
            "Processing image: apac_221_infection.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_infection.png Top class: infection, Probability: 0.4200\n",
            "Processing image: bullous_139_infection.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: bullous_143_infection.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_infection.png Top class: bullous, Probability: 0.8200\n",
            "Processing image: bullous_212_infection.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: deposit_57_infection.png Top class: infection, Probability: 0.9500\n",
            "Processing image: deposit_64_infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_70_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_77_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_138_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_1_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: infection_29_infection.png Top class: infection, Probability: 0.7400\n",
            "Processing image: infection_75_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: infection_81_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: infection_97_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: normal_8_infection.png Top class: infection, Probability: 0.9400\n",
            "Processing image: normal_73_infection.png Top class: infection, Probability: 0.9000\n",
            "Processing image: normal_110_infection.png Top class: non-infection, Probability: 0.5900\n",
            "Processing image: normal_159_infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_189_infection.png Top class: infection, Probability: 0.8800\n",
            "Processing image: scar_69_infection.png Top class: infection, Probability: 0.9500\n",
            "Processing image: scar_82_infection.png Top class: non-infection, Probability: 0.7400\n",
            "Processing image: scar_114_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: scar_127_infection.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_186_infection.png Top class: infection, Probability: 0.6500\n",
            "Processing image: tumor_15_infection.png Top class: tumor, Probability: 0.8800\n",
            "Processing image: tumor_95_infection.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_98_infection.png Top class: infection, Probability: 0.6100\n",
            "Processing image: tumor_183_infection.png Top class: tumor, Probability: 0.7400\n",
            "Processing image: tumor_220_infection.png Top class: tumor, Probability: 0.9200\n",
            "Processing image: apac_234_infection.png Top class: apac, Probability: 0.9200\n",
            "Processing image: non-infection_99_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_210_infection.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: lens-opacity_205_infection.png Top class: lens-opacity, Probability: 0.4700\n",
            "Processing image: lens-opacity_43_infection.png Top class: non-infection, Probability: 0.4000\n",
            "Processing image: lens-opacity_196_infection.png Top class: non-infection, Probability: 0.7300\n",
            "Processing image: lens-opacity_116_infection.png Top class: lens-opacity, Probability: 0.5200\n",
            "Processing image: lens-opacity_85_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_195_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_128_infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_223_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: apac_221_tumor.png Top class: apac, Probability: 0.9000\n",
            "Processing image: apac_5_tumor.png Top class: bullous, Probability: 0.5300\n",
            "Processing image: apac_124_tumor.png Top class: apac, Probability: 0.9500\n",
            "Processing image: apac_203_tumor.png Top class: apac, Probability: 0.9300\n",
            "Processing image: bullous_132_tumor.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_139_tumor.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_143_tumor.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_191_tumor.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_tumor.png Top class: bullous, Probability: 0.9700\n",
            "Processing image: deposit_57_tumor.png Top class: deposit, Probability: 0.9500\n",
            "Processing image: deposit_64_tumor.png Top class: deposit, Probability: 0.9400\n",
            "Processing image: deposit_70_tumor.png Top class: deposit, Probability: 0.9300\n",
            "Processing image: deposit_77_tumor.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_tumor.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_tumor.png Top class: scar, Probability: 0.9000\n",
            "Processing image: infection_29_tumor.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: infection_75_tumor.png Top class: infection, Probability: 0.3700\n",
            "Processing image: infection_81_tumor.png Top class: infection, Probability: 0.6800\n",
            "Processing image: infection_97_tumor.png Top class: non-infection, Probability: 0.2700\n",
            "Processing image: normal_8_tumor.png Top class: normal, Probability: 0.9300\n",
            "Processing image: normal_73_tumor.png Top class: normal, Probability: 0.7100\n",
            "Processing image: normal_110_tumor.png Top class: normal, Probability: 0.8300\n",
            "Processing image: normal_159_tumor.png Top class: normal, Probability: 0.8500\n",
            "Processing image: normal_189_tumor.png Top class: normal, Probability: 0.7900\n",
            "Processing image: scar_69_tumor.png Top class: bullous, Probability: 0.4700\n",
            "Processing image: scar_82_tumor.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_114_tumor.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_tumor.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_186_tumor.png Top class: scar, Probability: 0.6900\n",
            "Processing image: tumor_15_tumor.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_95_tumor.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_tumor.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_183_tumor.png Top class: tumor, Probability: 0.9000\n",
            "Processing image: tumor_220_tumor.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: apac_234_tumor.png Top class: apac, Probability: 0.9600\n",
            "Processing image: non-infection_210_tumor.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: non-infection_223_tumor.png Top class: non-infection, Probability: 0.7500\n",
            "Processing image: lens-opacity_85_tumor.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: non-infection_195_tumor.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: lens-opacity_43_tumor.png Top class: lens-opacity, Probability: 0.5000\n",
            "Processing image: non-infection_99_tumor.png Top class: non-infection, Probability: 0.8200\n",
            "Processing image: non-infection_128_tumor.png Top class: scar, Probability: 0.9300\n",
            "Processing image: lens-opacity_196_tumor.png Top class: lens-opacity, Probability: 0.7600\n",
            "Processing image: lens-opacity_205_tumor.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: lens-opacity_116_tumor.png Top class: lens-opacity, Probability: 0.5900\n",
            "Processing image: apac_5_deposit.png Top class: bullous, Probability: 0.3000\n",
            "Processing image: apac_124_deposit.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_203_deposit.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_221_deposit.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_deposit.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: bullous_139_deposit.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_deposit.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_deposit.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_deposit.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: deposit_57_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_64_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_70_deposit.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_138_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: infection_1_deposit.png Top class: scar, Probability: 0.9400\n",
            "Processing image: infection_29_deposit.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: infection_75_deposit.png Top class: infection, Probability: 0.8300\n",
            "Processing image: infection_81_deposit.png Top class: deposit, Probability: 0.7600\n",
            "Processing image: infection_97_deposit.png Top class: scar, Probability: 0.4600\n",
            "Processing image: normal_8_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_73_deposit.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_159_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_189_deposit.png Top class: normal, Probability: 0.9600\n",
            "Processing image: scar_69_deposit.png Top class: scar, Probability: 0.7800\n",
            "Processing image: scar_82_deposit.png Top class: scar, Probability: 0.9100\n",
            "Processing image: scar_114_deposit.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_deposit.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_186_deposit.png Top class: scar, Probability: 0.8700\n",
            "Processing image: tumor_15_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_deposit.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_183_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_220_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: apac_234_deposit.png Top class: deposit, Probability: 0.9100\n",
            "Processing image: lens-opacity_85_deposit.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: lens-opacity_43_deposit.png Top class: lens-opacity, Probability: 0.8500\n",
            "Processing image: non-infection_210_deposit.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: lens-opacity_205_deposit.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_99_deposit.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_128_deposit.png Top class: scar, Probability: 0.4800\n",
            "Processing image: lens-opacity_116_deposit.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: non-infection_195_deposit.png Top class: non-infection, Probability: 0.8600\n",
            "Processing image: non-infection_223_deposit.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: lens-opacity_196_deposit.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: apac_5_scar.png Top class: bullous, Probability: 0.8800\n",
            "Processing image: apac_124_scar.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_203_scar.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_221_scar.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_scar.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_139_scar.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_scar.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_191_scar.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_scar.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_scar.png Top class: scar, Probability: 0.9200\n",
            "Processing image: infection_29_scar.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: infection_75_scar.png Top class: infection, Probability: 0.7300\n",
            "Processing image: infection_81_scar.png Top class: scar, Probability: 0.7100\n",
            "Processing image: infection_97_scar.png Top class: bullous, Probability: 0.8100\n",
            "Processing image: normal_8_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_73_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_159_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_189_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: scar_82_scar.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_114_scar.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_127_scar.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_scar.png Top class: scar, Probability: 0.8400\n",
            "Processing image: tumor_15_scar.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_scar.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_98_scar.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_183_scar.png Top class: tumor, Probability: 0.8600\n",
            "Processing image: tumor_220_scar.png Top class: tumor, Probability: 0.9300\n",
            "Processing image: deposit_70_scar.png Top class: deposit, Probability: 0.9400\n",
            "Processing image: apac_234_scar.png Top class: deposit, Probability: 0.9400\n",
            "Processing image: lens-opacity_85_scar.png Top class: lens-opacity, Probability: 0.9000\n",
            "Processing image: non-infection_99_scar.png Top class: non-infection, Probability: 0.8200\n",
            "Processing image: non-infection_195_scar.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: lens-opacity_205_scar.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: lens-opacity_116_scar.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_43_scar.png Top class: lens-opacity, Probability: 0.7900\n",
            "Processing image: non-infection128_scar.png Top class: scar, Probability: 0.9100\n",
            "Processing image: non-infection_210_scar.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: non-infection_223_scar.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: lens-opacity_196_scar.png Top class: lens-opacity, Probability: 0.9000\n",
            "Processing image: non-infection_128_scar.png Top class: scar, Probability: 0.7500\n",
            "Processing image: scar_69_scar.png Top class: scar, Probability: 0.9100\n",
            "Processing image: apac_234_non-infection.png Top class: apac, Probability: 0.9400\n",
            "Processing image: lens-opacity_116_non-infection.png Top class: lens-opacity, Probability: 0.3900\n",
            "Processing image: tumor_183_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: lens-opacity_85_non-infection.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: deposit_77_non-infection.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: scar_127_non-infection.png Top class: scar, Probability: 0.9500\n",
            "Processing image: normal_73_non-infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: non-infection_99_non-infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: scar_82_non-infection.png Top class: scar, Probability: 0.9300\n",
            "Processing image: tumor_98_non-infection.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: infection_81_non-infection.png Top class: infection, Probability: 0.9200\n",
            "Processing image: deposit_70_non-infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: scar_114_non-infection.png Top class: scar, Probability: 0.9400\n",
            "Processing image: apac_221_non-infection.png Top class: apac, Probability: 0.8600\n",
            "Processing image: apac_124_non-infection.png Top class: apac, Probability: 0.8200\n",
            "Processing image: deposit_138_non-infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_75_non-infection.png Top class: infection, Probability: 0.9400\n",
            "Processing image: non-infection_223_non-infection.png Top class: non-infection, Probability: 0.7800\n",
            "Processing image: tumor_95_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: scar_186_non-infection.png Top class: scar, Probability: 0.8700\n",
            "Processing image: bullous_132_non-infection.png Top class: bullous, Probability: 0.8900\n",
            "Processing image: lens-opacity_43_non-infection.png Top class: lens-opacity, Probability: 0.5300\n",
            "Processing image: bullous_212_non-infection.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: normal_110_non-infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: deposit_57_non-infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: tumor_15_non-infection.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: normal_159_non-infection.png Top class: normal, Probability: 0.9700\n",
            "Processing image: apac_203_non-infection.png Top class: apac, Probability: 0.9600\n",
            "Processing image: normal_189_non-infection.png Top class: normal, Probability: 0.8700\n",
            "Processing image: lens-opacity_205_non-infection.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_195_non-infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: deposit_64_non-infection.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: normal_8_non-infection.png Top class: normal, Probability: 0.9700\n",
            "Processing image: infection_1_non-infection.png Top class: infection, Probability: 0.7900\n",
            "Processing image: bullous_143_non-infection.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_139_non-infection.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: tumor_220_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: infection_97_non-infection.png Top class: bullous, Probability: 0.4400\n",
            "Processing image: non-infection_210_non-infection.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: apac_5_non-infection.png Top class: infection, Probability: 0.3600\n",
            "Processing image: lens-opacity_196_non-infection.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: infection_29_non-infection.png Top class: bullous, Probability: 0.8900\n",
            "Processing image: scar_69_non-infection.png Top class: bullous, Probability: 0.4500\n",
            "Processing image: non-infection_128_non-infection.png Top class: non-infection, Probability: 0.7300\n",
            "Processing image: bullous_191_non-infection.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: apac_5_apac.png Top class: apac, Probability: 0.9300\n",
            "Processing image: apac_124_apac.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_203_apac.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_221_apac.png Top class: apac, Probability: 0.9400\n",
            "Processing image: apac_234_apac.png Top class: apac, Probability: 0.9600\n",
            "Processing image: bullous_132_apac.png Top class: bullous, Probability: 0.4300\n",
            "Processing image: bullous_139_apac.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_apac.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: bullous_191_apac.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_apac.png Top class: bullous, Probability: 0.8800\n",
            "Processing image: deposit_57_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_70_apac.png Top class: deposit, Probability: 0.8800\n",
            "Processing image: deposit_77_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_apac.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: infection_1_apac.png Top class: scar, Probability: 0.9100\n",
            "Processing image: infection_29_apac.png Top class: infection, Probability: 0.8600\n",
            "Processing image: infection_75_apac.png Top class: infection, Probability: 0.7800\n",
            "Processing image: infection_81_apac.png Top class: infection, Probability: 0.9400\n",
            "Processing image: infection_97_apac.png Top class: infection, Probability: 0.9300\n",
            "Processing image: lens-opacity_43_apac.png Top class: tumor, Probability: 0.6500\n",
            "Processing image: lens-opacity_85_apac.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: lens-opacity_116_apac.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: lens-opacity_196_apac.png Top class: lens-opacity, Probability: 0.8900\n",
            "Processing image: lens-opacity_205_apac.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: non-infection_99_apac.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_128_apac.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: non-infection_195_apac.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: non-infection_210_apac.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: non-infection_223_apac.png Top class: non-infection, Probability: 0.7000\n",
            "Processing image: normal_8_apac.png Top class: normal, Probability: 0.5400\n",
            "Processing image: normal_73_apac.png Top class: non-infection, Probability: 0.7600\n",
            "Processing image: normal_110_apac.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_159_apac.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_189_apac.png Top class: infection, Probability: 0.6100\n",
            "Processing image: scar_69_apac.png Top class: infection, Probability: 0.8100\n",
            "Processing image: scar_82_apac.png Top class: scar, Probability: 0.8500\n",
            "Processing image: scar_114_apac.png Top class: scar, Probability: 0.9100\n",
            "Processing image: scar_127_apac.png Top class: scar, Probability: 0.8900\n",
            "Processing image: scar_186_apac.png Top class: scar, Probability: 0.9100\n",
            "Processing image: tumor_15_apac.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_apac.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_apac.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_183_apac.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_220_apac.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: deposit_57_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_lens-opacity.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_70_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: apac_5_lens-opacity.png Top class: bullous, Probability: 0.6800\n",
            "Processing image: apac_124_lens-opacity.png Top class: apac, Probability: 0.7300\n",
            "Processing image: apac_203_lens-opacity.png Top class: apac, Probability: 0.8800\n",
            "Processing image: apac_221_lens-opacity.png Top class: apac, Probability: 0.7800\n",
            "Processing image: apac_234_lens-opacity.png Top class: deposit, Probability: 0.8900\n",
            "Processing image: bullous_132_lens-opacity.png Top class: bullous, Probability: 0.9000\n",
            "Processing image: bullous_139_lens-opacity.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_143_lens-opacity.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_lens-opacity.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_212_lens-opacity.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: infection_1_lens-opacity.png Top class: scar, Probability: 0.7100\n",
            "Processing image: infection_29_lens-opacity.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: infection_75_lens-opacity.png Top class: deposit, Probability: 0.5300\n",
            "Processing image: infection_81_lens-opacity.png Top class: deposit, Probability: 0.5700\n",
            "Processing image: infection_97_lens-opacity.png Top class: bullous, Probability: 0.3800\n",
            "Processing image: lens-opacity_85_lens-opacity.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: lens-opacity_116_lens-opacity.png Top class: lens-opacity, Probability: 0.8700\n",
            "Processing image: lens-opacity_196_lens-opacity.png Top class: lens-opacity, Probability: 0.4500\n",
            "Processing image: lens-opacity_205_lens-opacity.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: non-infection_99_lens-opacity.png Top class: non-infection, Probability: 0.8300\n",
            "Processing image: non-infection_128_lens-opacity.png Top class: scar, Probability: 0.9100\n",
            "Processing image: non-infection_195_lens-opacity.png Top class: non-infection, Probability: 0.7800\n",
            "Processing image: non-infection_210_lens-opacity.png Top class: tumor, Probability: 0.8200\n",
            "Processing image: non-infection_223_lens-opacity.png Top class: non-infection, Probability: 0.6400\n",
            "Processing image: normal_8_lens-opacity.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_lens-opacity.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_110_lens-opacity.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_159_lens-opacity.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_189_lens-opacity.png Top class: normal, Probability: 0.9200\n",
            "Processing image: scar_69_lens-opacity.png Top class: scar, Probability: 0.5700\n",
            "Processing image: scar_82_lens-opacity.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_114_lens-opacity.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_127_lens-opacity.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_186_lens-opacity.png Top class: scar, Probability: 0.8800\n",
            "Processing image: tumor_15_lens-opacity.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_95_lens-opacity.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: tumor_98_lens-opacity.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_183_lens-opacity.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_220_lens-opacity.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: lens-opacity_43_lens-opacity.png Top class: lens-opacity, Probability: 0.3300\n",
            "\n",
            "DataFrame:\n",
            "                             basename         image_id   groundtruth  \\\n",
            "0                   apac_5_normal.png           apac_5          apac   \n",
            "1                 apac_124_normal.png         apac_124          apac   \n",
            "2                 apac_203_normal.png         apac_203          apac   \n",
            "3                 apac_221_normal.png         apac_221          apac   \n",
            "4              bullous_132_normal.png      bullous_132       bullous   \n",
            "..                                ...              ...           ...   \n",
            "400         tumor_95_lens-opacity.png         tumor_95         tumor   \n",
            "401         tumor_98_lens-opacity.png         tumor_98         tumor   \n",
            "402        tumor_183_lens-opacity.png        tumor_183         tumor   \n",
            "403        tumor_220_lens-opacity.png        tumor_220         tumor   \n",
            "404  lens-opacity_43_lens-opacity.png  lens-opacity_43  lens-opacity   \n",
            "\n",
            "           cornea    background   pred_cutmix  prob_cutmix  \n",
            "0            apac        normal       deposit         0.86  \n",
            "1            apac        normal          apac         0.85  \n",
            "2            apac        normal          apac         0.92  \n",
            "3            apac        normal          apac         0.64  \n",
            "4         bullous        normal       bullous         0.91  \n",
            "..            ...           ...           ...          ...  \n",
            "400         tumor  lens-opacity         tumor         0.97  \n",
            "401         tumor  lens-opacity         tumor         0.96  \n",
            "402         tumor  lens-opacity         tumor         0.96  \n",
            "403         tumor  lens-opacity         tumor         0.97  \n",
            "404  lens-opacity  lens-opacity  lens-opacity         0.33  \n",
            "\n",
            "[405 rows x 7 columns]\n",
            "\n",
            "CSV saved to: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw matrix"
      ],
      "metadata": {
        "id": "jEmlfJ91BMAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "file_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 正しい判定の割合\n",
        "correct_predictions = df[df['cornea'] == df['pred_cutmix']]\n",
        "accuracy = len(correct_predictions) / len(df)\n",
        "\n",
        "# 各状態に対する正しい判定の割合\n",
        "correct_by_cornea = df[df['cornea'] == df['pred_cutmix']].groupby('cornea').size() / df.groupby('cornea').size()\n",
        "\n",
        "# 確率の分布\n",
        "correct_probs = correct_predictions['prob_cutmix']\n",
        "incorrect_probs = df[df['cornea'] != df['pred_cutmix']]['prob_cutmix']\n",
        "\n",
        "# ヒストグラムをプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(correct_probs, bins=10, alpha=0.5, label='Correct Predictions', color='blue')\n",
        "plt.hist(incorrect_probs, bins=10, alpha=0.5, label='Incorrect Predictions', color='red')\n",
        "plt.xlabel('Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Probability Distribution of Correct and Incorrect Predictions')\n",
        "plt.show()\n",
        "\n",
        "# グラウンドトゥルースごとの背景に対する正解率マトリックスを作成\n",
        "matrix = df.pivot_table(index='cornea', columns='background', values='pred_cutmix', aggfunc=lambda x: (x == df.loc[x.index, 'cornea']).mean())\n",
        "\n",
        "# マトリックス図をプロット\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title('Accuracy Matrix: Cornea vs Background')\n",
        "plt.xlabel('Background')\n",
        "plt.ylabel('Cornea (Ground Truth)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QOWWO0dKBQ1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkMbYsOGc1KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SWPhfox_BbpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aWI7LQJpDz0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference cornea vs original images**"
      ],
      "metadata": {
        "id": "fWXmIziTD2JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import re\n",
        "# import os\n",
        "# import cv2\n",
        "# import torch\n",
        "\n",
        "# def process_image(img_path, model):\n",
        "#     img = cv2.imread(img_path)\n",
        "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#     torch_img = model.preprocessing(img)\n",
        "#     with torch.no_grad():\n",
        "#         prediction, logits = model(torch_img)\n",
        "#     if len(prediction[2][0]) > 0:\n",
        "#         top_class = prediction[2][0][0]\n",
        "#         top_prob = prediction[3][0][0]\n",
        "#         return top_class, top_prob\n",
        "#     else:\n",
        "#         return \"No detection\", 0.0\n",
        "\n",
        "# def folder_main(folder_path, model):\n",
        "#     results = []\n",
        "#     for root, dirs, files in os.walk(folder_path):\n",
        "#         for item in files:\n",
        "#             if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "#                 img_path = os.path.join(root, item)\n",
        "#                 img_basename = os.path.basename(img_path)\n",
        "\n",
        "#                 top_class, top_prob = process_image(img_path, model)\n",
        "\n",
        "#                 # Split filename to get groundtruth\n",
        "#                 parts = img_basename.split('_')\n",
        "#                 if len(parts) >= 2:\n",
        "#                     groundtruth = parts[0]\n",
        "#                     image_id = f\"{parts[0]}_{parts[1]}\"\n",
        "#                     results.append({\n",
        "#                         \"basename\": img_basename,\n",
        "#                         \"image_id\": f\"{parts[0]}_{parts[1].split('.')[0]}\",\n",
        "#                         \"cornea_groundtruth\": groundtruth,\n",
        "#                         \"pred\": top_class,\n",
        "#                         \"prob\": top_prob\n",
        "#                     })\n",
        "\n",
        "#                 print(f\"Processing image: {img_basename} Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "#     return pd.DataFrame(results)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#     img_size = (640, 640)\n",
        "\n",
        "#     folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea'\n",
        "#     print(f\"Using device: {device}\")\n",
        "#     print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "#     # Updated class names\n",
        "#     classes = names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"apac\", \"lens-opacity\", \"bullous\"]\n",
        "\n",
        "#     model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=classes)\n",
        "\n",
        "#     df = folder_main(folder_path, model)\n",
        "\n",
        "#     print(\"\\nDataFrame:\")\n",
        "#     print(df)\n",
        "#     csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea.csv'\n",
        "#     df.to_csv(csv_path, index=False)\n",
        "#     print(f\"\\nCSV saved to: {csv_path}\")"
      ],
      "metadata": {
        "id": "qq_uhTR-Pfky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import os\n",
        "# import cv2\n",
        "# import torch\n",
        "\n",
        "# def process_image(img_path, model):\n",
        "#     img = cv2.imread(img_path)\n",
        "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#     torch_img = model.preprocessing(img)\n",
        "#     with torch.no_grad():\n",
        "#         prediction, logits = model(torch_img)\n",
        "#     if len(prediction[2][0]) > 0:\n",
        "#         top_class = prediction[2][0][0]\n",
        "#         top_prob = prediction[3][0][0]\n",
        "#         return top_class, top_prob\n",
        "#     else:\n",
        "#         return \"No detection\", 0.0\n",
        "\n",
        "# def process_folders(folder_path_cornea, folder_path_original, model):\n",
        "#     results = []\n",
        "#     for cornea_img_name in os.listdir(folder_path_cornea):\n",
        "#         if cornea_img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "#             cornea_img_path = os.path.join(folder_path_cornea, cornea_img_name)\n",
        "\n",
        "#             # Construct the corresponding original image name (always .jpg)\n",
        "#             original_img_name = os.path.splitext(cornea_img_name)[0] + '.jpg'\n",
        "#             original_img_path = os.path.join(folder_path_original, original_img_name)\n",
        "\n",
        "#             if not os.path.exists(original_img_path):\n",
        "#                 print(f\"Warning: Corresponding original image not found for {cornea_img_name}\")\n",
        "#                 continue\n",
        "\n",
        "#             # Process cornea image\n",
        "#             cornea_top_class, cornea_top_prob = process_image(cornea_img_path, model)\n",
        "\n",
        "#             # Process original image\n",
        "#             original_top_class, original_top_prob = process_image(original_img_path, model)\n",
        "\n",
        "#             # Split filename to get groundtruth\n",
        "#             parts = cornea_img_name.split('_')\n",
        "#             groundtruth = parts[0] if len(parts) >= 2 else \"Unknown\"\n",
        "\n",
        "#             results.append({\n",
        "#                 \"basename\": cornea_img_name,\n",
        "#                 \"image_id\": f\"{parts[0]}_{parts[1].split('.')[0]}\",\n",
        "#                 \"groundtruth\": groundtruth,\n",
        "#                 \"pred_original\": original_top_class,\n",
        "#                 \"prob_original\": original_top_prob,\n",
        "#                 \"pred_cornea\": cornea_top_class,\n",
        "#                 \"prob_cornea\": cornea_top_prob\n",
        "#             })\n",
        "\n",
        "#             print(f\"Processing image: {cornea_img_name}\")\n",
        "#             print(f\"  Original - Top class: {original_top_class}, Probability: {original_top_prob:.4f}\")\n",
        "#             print(f\"  Cornea   - Top class: {cornea_top_class}, Probability: {cornea_top_prob:.4f}\")\n",
        "\n",
        "#     return pd.DataFrame(results)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#     img_size = (640, 640)\n",
        "\n",
        "#     folder_path_cornea = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea'\n",
        "#     folder_path_original = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/original'\n",
        "\n",
        "#     print(f\"Using device: {device}\")\n",
        "#     print(f\"Processing cornea folder: {folder_path_cornea}\")\n",
        "#     print(f\"Processing original folder: {folder_path_original}\")\n",
        "\n",
        "#     # Updated class names\n",
        "#     classes = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"apac\", \"lens-opacity\", \"bullous\"]\n",
        "\n",
        "#     model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=classes)\n",
        "\n",
        "#     df = process_folders(folder_path_cornea, folder_path_original, model)\n",
        "\n",
        "#     print(\"\\nDataFrame:\")\n",
        "#     print(df)\n",
        "\n",
        "#     csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/comparison_results.csv'\n",
        "#     df.to_csv(csv_path, index=False)\n",
        "#     print(f\"\\nCSV saved to: {csv_path}\")"
      ],
      "metadata": {
        "id": "W_SwpatajD3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def process_folders(folder_path_cornea, folder_path_original, model):\n",
        "    results = []\n",
        "    for cornea_img_name in os.listdir(folder_path_cornea):\n",
        "        if cornea_img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "            cornea_img_path = os.path.join(folder_path_cornea, cornea_img_name)\n",
        "\n",
        "            # Construct the corresponding original image name (always .jpg)\n",
        "            original_img_name = os.path.splitext(cornea_img_name)[0] + '.jpg'\n",
        "            original_img_path = os.path.join(folder_path_original, original_img_name)\n",
        "\n",
        "            if not os.path.exists(original_img_path):\n",
        "                print(f\"Warning: Corresponding original image not found for {cornea_img_name}\")\n",
        "                continue\n",
        "\n",
        "            # Process cornea image\n",
        "            cornea_top_class, cornea_top_prob = process_image(cornea_img_path, model)\n",
        "\n",
        "            # Process original image\n",
        "            original_top_class, original_top_prob = process_image(original_img_path, model)\n",
        "\n",
        "            # Split filename to get groundtruth\n",
        "            parts = cornea_img_name.split('_')\n",
        "            groundtruth = parts[0] if len(parts) >= 2 else \"Unknown\"\n",
        "\n",
        "            results.append({\n",
        "                \"basename\": cornea_img_name,\n",
        "                \"image_id\": f\"{parts[0]}_{parts[1].split('.')[0]}\",\n",
        "                \"groundtruth\": groundtruth,\n",
        "                \"pred_original\": original_top_class,\n",
        "                \"prob_original\": original_top_prob,\n",
        "                \"pred_cornea\": cornea_top_class,\n",
        "                \"prob_cornea\": cornea_top_prob\n",
        "            })\n",
        "\n",
        "            print(f\"Processing image: {cornea_img_name}\")\n",
        "            print(f\"  Original - Top class: {original_top_class}, Probability: {original_top_prob:.4f}\")\n",
        "            print(f\"  Cornea   - Top class: {cornea_top_class}, Probability: {cornea_top_prob:.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)\n",
        "\n",
        "    folder_path_cornea = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea'\n",
        "    folder_path_original = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/original'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing cornea folder: {folder_path_cornea}\")\n",
        "    print(f\"Processing original folder: {folder_path_original}\")\n",
        "\n",
        "    # Updated class names\n",
        "    classes = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"apac\", \"lens-opacity\", \"bullous\"]\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=classes)\n",
        "\n",
        "    df = process_folders(folder_path_cornea, folder_path_original, model)\n",
        "\n",
        "    print(\"\\nDataFrame:\")\n",
        "    print(df)\n",
        "\n",
        "    csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/comparison_results.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nCSV saved to: {csv_path}\")\n"
      ],
      "metadata": {
        "id": "1rd_uwvYLcc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCtjNWnbLceo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnVYBefDpvx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def read_csv_safe(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"ファイルが見つかりません: {file_path}\")\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "try:\n",
        "    # CSVファイルのパスを指定\n",
        "    cutmix_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv'\n",
        "    comparison_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/comparison_results.csv'\n",
        "\n",
        "    # 2つのCSVファイルを読み込む\n",
        "    cutmix_df = read_csv_safe(cutmix_path)\n",
        "    comparison_df = read_csv_safe(comparison_path)\n",
        "\n",
        "    # 列名を表示して確認\n",
        "    print(\"cutmix.csv の列名:\", cutmix_df.columns)\n",
        "    print(\"comparison_results.csv の列名:\", comparison_df.columns)\n",
        "\n",
        "    # cutmix_dfに新しい列を追加\n",
        "    new_columns = [\"pred_original\", \"prob_original\", \"pred_cornea\", \"prob_cornea\"]\n",
        "    for col in new_columns:\n",
        "        cutmix_df[col] = None\n",
        "\n",
        "    # comparison_dfのimage_id列を修正（.pngを除去）\n",
        "    comparison_df['image_id'] = comparison_df['image_id'].str.replace('.png', '')\n",
        "\n",
        "    # cutmix_dfの各行に対して、comparison_dfから対応するデータを取得\n",
        "    for index, row in cutmix_df.iterrows():\n",
        "        matching_row = comparison_df[comparison_df['image_id'] == row['image_id']]\n",
        "        if not matching_row.empty:\n",
        "            for col in new_columns:\n",
        "                cutmix_df.at[index, col] = matching_row[col].values[0]\n",
        "\n",
        "    # 結果をCSVファイルに出力\n",
        "    output_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/merged_results.csv'\n",
        "    cutmix_df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"CSVファイルの結合が完了しました。結果は '{output_path}' に保存されています。\")\n",
        "\n",
        "    # 結果の最初の数行を表示\n",
        "    print(\"\\n結合されたデータの最初の数行:\")\n",
        "    print(cutmix_df.head())\n",
        "\n",
        "    # 新しく追加された列の欠損値の数を表示\n",
        "    print(\"\\n新しく追加された列の欠損値の数:\")\n",
        "    for col in new_columns:\n",
        "        print(f\"{col}: {cutmix_df[col].isnull().sum()}\")\n",
        "\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"エラー: {e}\")\n",
        "    print(\"Google Driveがマウントされていることを確認してください。\")\n",
        "except Exception as e:\n",
        "    print(f\"予期せぬエラーが発生しました: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRVkBn_kkhjO",
        "outputId": "f1382c62-1689-4995-9126-fc3f91e4ad8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cutmix.csv の列名: Index(['basename', 'image_id', 'groundtruth', 'cornea', 'background',\n",
            "       'pred_cutmix', 'prob_cutmix'],\n",
            "      dtype='object')\n",
            "comparison_results.csv の列名: Index(['basename', 'image_id', 'groundtruth', 'pred_original', 'prob_original',\n",
            "       'pred_cornea', 'prob_cornea'],\n",
            "      dtype='object')\n",
            "CSVファイルの結合が完了しました。結果は '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/merged_results.csv' に保存されています。\n",
            "\n",
            "結合されたデータの最初の数行:\n",
            "                 basename     image_id groundtruth   cornea background  ...  \\\n",
            "0       apac_5_normal.png       apac_5        apac     apac     normal  ...   \n",
            "1     apac_124_normal.png     apac_124        apac     apac     normal  ...   \n",
            "2     apac_203_normal.png     apac_203        apac     apac     normal  ...   \n",
            "3     apac_221_normal.png     apac_221        apac     apac     normal  ...   \n",
            "4  bullous_132_normal.png  bullous_132     bullous  bullous     normal  ...   \n",
            "\n",
            "  prob_cutmix  pred_original prob_original pred_cornea prob_cornea  \n",
            "0        0.86           apac          0.93     bullous        0.43  \n",
            "1        0.85           apac          0.94     bullous        0.39  \n",
            "2        0.92           apac          0.97        apac        0.82  \n",
            "3        0.64           apac          0.95        apac        0.79  \n",
            "4        0.91        bullous          0.93     bullous        0.83  \n",
            "\n",
            "[5 rows x 11 columns]\n",
            "\n",
            "新しく追加された列の欠損値の数:\n",
            "pred_original: 0\n",
            "prob_original: 0\n",
            "pred_cornea: 0\n",
            "prob_cornea: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Analyze results**"
      ],
      "metadata": {
        "id": "a9wD9Q3ZOTHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/comparison_results.csv'\n",
        "\n",
        "def analyze_comparison_results(csv_path):\n",
        "    # CSVファイルの読み込み\n",
        "    data = pd.read_csv(csv_path)\n",
        "\n",
        "    # 確率の差を計算\n",
        "    data['prob_difference'] = data['prob_original'] - data['prob_cornea']\n",
        "    data['cornea_correct'] = data['groundtruth'] == data['pred_cornea']\n",
        "\n",
        "    # クラスごとの平均確率と差、正解率を計算\n",
        "    class_prob_means = data.groupby('groundtruth')[['prob_original', 'prob_cornea']].mean().reset_index()\n",
        "    class_prob_means['prob_difference'] = class_prob_means['prob_original'] - class_prob_means['prob_cornea']\n",
        "    class_correct_rates = data.groupby('groundtruth')['cornea_correct'].mean().reset_index()\n",
        "\n",
        "    # 対応のあるt検定を実施\n",
        "    significance_results = data['groundtruth'].unique()\n",
        "    paired_significance_results = []\n",
        "\n",
        "    for class_name in significance_results:\n",
        "        class_data = data[data['groundtruth'] == class_name]\n",
        "        t_stat, p_value = ttest_rel(class_data['prob_original'], class_data['prob_cornea'])\n",
        "        paired_significance_results.append((class_name, t_stat, p_value))\n",
        "\n",
        "    # 結果をDataFrameにまとめる\n",
        "    paired_significance_df = pd.DataFrame(paired_significance_results, columns=['Class', 't_stat', 'p_value'])\n",
        "    class_prob_means = class_prob_means.merge(paired_significance_df, left_on='groundtruth', right_on='Class').drop('Class', axis=1)\n",
        "    class_prob_means = class_prob_means.merge(class_correct_rates, on='groundtruth').rename(columns={'cornea_correct': 'cornea_accuracy'})\n",
        "\n",
        "    return class_prob_means\n",
        "\n",
        "# 分析結果を取得\n",
        "result_df = analyze_comparison_results(csv_path)\n",
        "\n",
        "# 結果を表示\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAtwkM4osQlo",
        "outputId": "a5c59be8-cf83-4c04-c5db-2712587cf13f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     groundtruth  prob_original  prob_cornea  prob_difference    t_stat  \\\n",
            "0           apac          0.944        0.618            0.326  3.867274   \n",
            "1        bullous          0.928        0.800            0.128  4.967363   \n",
            "2        deposit          0.968        0.888            0.080  3.090670   \n",
            "3      infection          0.958        0.648            0.310  3.216280   \n",
            "4   lens-opacity          0.866        0.642            0.224  2.713209   \n",
            "5  non-infection          0.872        0.650            0.222  1.973509   \n",
            "6         normal          0.970        0.754            0.216  5.291956   \n",
            "7           scar          0.916        0.736            0.180  2.821469   \n",
            "8          tumor          0.864        0.884           -0.020 -0.395285   \n",
            "\n",
            "    p_value  cornea_accuracy  \n",
            "0  0.018035              0.6  \n",
            "1  0.007666              1.0  \n",
            "2  0.036551              1.0  \n",
            "3  0.032395              0.8  \n",
            "4  0.053360              0.6  \n",
            "5  0.119688              0.4  \n",
            "6  0.006120              1.0  \n",
            "7  0.047757              0.8  \n",
            "8  0.712807              1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/merged_results.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# CutMixの予測が正解かどうかを示す新しい列を作成\n",
        "#data['cutmix_correct'] = data['groundtruth'] == data['pred_cutmix']\n",
        "data['cutmix_correct'] = data['cornea'] == data['pred_cutmix']\n",
        "\n",
        "\n",
        "# ロジスティック回帰のためのデータを準備\n",
        "X = data['prob_cornea']\n",
        "y = data['cutmix_correct']\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# ロジスティック回帰モデルを適用\n",
        "logit_model = sm.Logit(y, X)\n",
        "result = logit_model.fit()\n",
        "\n",
        "# ロジスティック回帰の結果を表示\n",
        "print(result.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byRklHM4TYN5",
        "outputId": "2ceaf412-db73-4a91-8237-21432a705d39"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.392571\n",
            "         Iterations 6\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:         cutmix_correct   No. Observations:                  405\n",
            "Model:                          Logit   Df Residuals:                      403\n",
            "Method:                           MLE   Df Model:                            1\n",
            "Date:                Thu, 18 Jul 2024   Pseudo R-squ.:                  0.1169\n",
            "Time:                        14:13:13   Log-Likelihood:                -158.99\n",
            "converged:                       True   LL-Null:                       -180.04\n",
            "Covariance Type:            nonrobust   LLR p-value:                 8.656e-11\n",
            "===============================================================================\n",
            "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------\n",
            "const          -1.6344      0.506     -3.230      0.001      -2.626      -0.643\n",
            "prob_cornea     4.7270      0.741      6.376      0.000       3.274       6.180\n",
            "===============================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/merged_results.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# CutMixの予測が正解かどうかを示す新しい列を作成\n",
        "data['cutmix_correct'] = data['cornea'] == data['pred_cutmix']\n",
        "\n",
        "# 箱ひげ図を描く\n",
        "plt.figure(figsize=(10, 6))\n",
        "data.boxplot(column='prob_cornea', by='cutmix_correct', grid=False)\n",
        "plt.xlabel('CutMix Correct (False=0, True=1)')\n",
        "plt.ylabel('Prob_Cornea')\n",
        "plt.title('Box Plot of Prob_Cornea by CutMix Correct')\n",
        "plt.suptitle('')  # Remove the default suptitle\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "djHxKFOKTzDk",
        "outputId": "b67cee8d-5c51-4858-dd3e-20ca4d26d5ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/merged_results.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# CutMixの予測が正解かどうかを示す新しい列を作成\n",
        "data['cutmix_correct'] = data['cornea'] == data['pred_cutmix']\n",
        "\n",
        "# Seabornのスタイルを設定\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# 箱ひげ図を描く\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='cutmix_correct', y='prob_cornea', data=data)\n",
        "plt.xlabel('CutMix Correct (False=0, True=1)')\n",
        "plt.ylabel('Likelihood')\n",
        "plt.title('Box Plot of Prob_Cornea by CutMix Correct')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_5vTdb9fUOvC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}