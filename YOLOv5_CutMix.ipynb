{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYXT/j+PzX1JrqPW2iP+l+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_CutMix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_CutMix**"
      ],
      "metadata": {
        "id": "ewqwD__x-0fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iZzkoFNN-RlM",
        "outputId": "965fb165-f4f1-4116-fa0e-42f58b102f3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# prompt: gdriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "os.listdir(folder_path)\n"
      ],
      "metadata": {
        "id": "XoZXOidNATKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69e146a-6abd-4844-8726-9fdf595b1395"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['back_normal',\n",
              " 'back_bullous',\n",
              " 'back_infection',\n",
              " 'back_tumor',\n",
              " 'back_deposit',\n",
              " 'back_scar',\n",
              " 'back_non-infection',\n",
              " 'back_apac',\n",
              " 'back_lens-opacity']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Underbar to Hyphen**\n",
        "\n",
        "lens_opacity --> lens-opacity"
      ],
      "metadata": {
        "id": "YMqiKzZsc3i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def rename_item(old_path):\n",
        "    directory, item_name = os.path.split(old_path)\n",
        "    new_name = item_name\n",
        "    if \"lens_opacity\" in item_name:\n",
        "        new_name = new_name.replace(\"lens_opacity\", \"lens-opacity\")\n",
        "    if \"non_infection\" in item_name:\n",
        "        new_name = new_name.replace(\"non_infection\", \"non-infection\")\n",
        "\n",
        "    if new_name != item_name:\n",
        "        new_path = os.path.join(directory, new_name)\n",
        "        os.rename(old_path, new_path)\n",
        "        print(f\"Renamed: {item_name} -> {new_name}\")\n",
        "        return new_path\n",
        "    return old_path\n",
        "\n",
        "def rename_files_and_directories(folder_path):\n",
        "    for root, dirs, files in os.walk(folder_path, topdown=False):\n",
        "        # まずファイルの名前を変更\n",
        "        for file in files:\n",
        "            old_path = os.path.join(root, file)\n",
        "            rename_item(old_path)\n",
        "\n",
        "        # 次にディレクトリの名前を変更\n",
        "        for dir in dirs:\n",
        "            old_path = os.path.join(root, dir)\n",
        "            new_path = rename_item(old_path)\n",
        "\n",
        "            # ディレクトリ名が変更された場合、親ディレクトリのパスも更新\n",
        "            if new_path != old_path:\n",
        "                index = dirs.index(dir)\n",
        "                dirs[index] = os.path.basename(new_path)\n",
        "\n",
        "# 使用例\n",
        "folder_path = \"/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img\"  # ここに実際のフォルダパスを指定してください\n",
        "folder_path = \"/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea\"\n",
        "rename_files_and_directories(folder_path)"
      ],
      "metadata": {
        "id": "LeinskbYdKyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6bf6b5c-fe92-4414-a5a7-89633b5fa12b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed: non_infection_195.png -> non-infection_195.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**inference YOLOv5**"
      ],
      "metadata": {
        "id": "T4ZqYUcWUWIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXyDmJVDZgpB",
        "outputId": "29cf9cd5-bd74-4f0d-c75e-f3c331ce9d63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 12.93 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMj4HMy4aK2K",
        "outputId": "8c1cf955-cf10-4ef5-ca53-83d2f95f94ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/yolov5-gradcam')"
      ],
      "metadata": {
        "id": "QaT7J5-8aWIX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "# 以下の関数は変更なし\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "# YOLOV5GradCAMクラスを更新\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []  # クラス名を格納するリストを初期化\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        return alpha.view(b, k, 1, 1)\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)\n",
        "        weights = (positive_gradients * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        return eigen_cam.unsqueeze(1)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Apply finite constraint\n",
        "            # if not torch.isfinite(x).all():\n",
        "            #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0"
      ],
      "metadata": {
        "id": "yDA2QCgoaO5t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_GradCAM**"
      ],
      "metadata": {
        "id": "V0rW1tHUbFv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = os.listdir(folder_path)\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2.imwrite(output_path, final_image)\n",
        "        #cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "    img_dir = os.path.join(folder_path, os.listdir(folder_path)[0])\n",
        "\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = img_dir\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/content/outputs'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "    #for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ],
      "metadata": {
        "id": "AMp_rx-yjfal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAMなしバージョン**"
      ],
      "metadata": {
        "id": "qM6lGDVP6NcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Apply finite constraint\n",
        "            # if not torch.isfinite(x).all():\n",
        "            #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    for subdir in ['back_scar', 'back_bullous', 'back_infection', 'back_APAC', 'back_deposit', 'back_tumor', 'back_lens-opacity', 'back_non-infection']:\n",
        "        subdir_path = os.path.join(folder_path, subdir)\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            print(f\"Warning: Subdirectory {subdir} not found in {folder_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing subdirectory: {subdir}\")\n",
        "        file_list = os.listdir(subdir_path)\n",
        "\n",
        "        for item in file_list:\n",
        "            img_path = os.path.join(subdir_path, item)\n",
        "            img_basename = os.path.basename(img_path)\n",
        "            print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "            top_class, top_prob = process_image(img_path, model)\n",
        "            print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)  # 入力画像サイズ\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"])\n",
        "    folder_main(folder_path, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xd0WYkcXZgcC",
        "outputId": "3a317199-1914-48e5-cfb9-3efa7cddb778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Processing folder: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subdirectory: back_scar\n",
            "Processing image: apac_5_scar.png\n",
            "Top class: bullous, Probability: 0.8800\n",
            "Processing image: apac_124_scar.png\n",
            "Top class: APAC, Probability: 0.9600\n",
            "Processing image: apac_203_scar.png\n",
            "Top class: APAC, Probability: 0.9700\n",
            "Processing image: apac_221_scar.png\n",
            "Top class: APAC, Probability: 0.9700\n",
            "Processing image: bullous_132_scar.png\n",
            "Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_139_scar.png\n",
            "Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_scar.png\n",
            "Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_191_scar.png\n",
            "Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_scar.png\n",
            "Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_scar.png\n",
            "Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_scar.png\n",
            "Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_scar.png\n",
            "Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_scar.png\n",
            "Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_scar.png\n",
            "Top class: scar, Probability: 0.9200\n",
            "Processing image: infection_29_scar.png\n",
            "Top class: bullous, Probability: 0.9200\n",
            "Processing image: infection_75_scar.png\n",
            "Top class: infection, Probability: 0.7300\n",
            "Processing image: infection_81_scar.png\n",
            "Top class: scar, Probability: 0.7100\n",
            "Processing image: infection_97_scar.png\n",
            "Top class: bullous, Probability: 0.8100\n",
            "Processing image: normal_8_scar.png\n",
            "Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_73_scar.png\n",
            "Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_scar.png\n",
            "Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_159_scar.png\n",
            "Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_189_scar.png\n",
            "Top class: normal, Probability: 0.9700\n",
            "Processing image: scar_89_scar.png\n",
            "Top class: scar, Probability: 0.9100\n",
            "Processing image: scar_82_scar.png\n",
            "Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_114_scar.png\n",
            "Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_127_scar.png\n",
            "Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_scar.png\n",
            "Top class: scar, Probability: 0.8400\n",
            "Processing image: tumor_15_scar.png\n",
            "Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_scar.png\n",
            "Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_98_scar.png\n",
            "Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_183_scar.png\n",
            "Top class: tumor, Probability: 0.8600\n",
            "Processing image: tumor_220_scar.png\n",
            "Top class: tumor, Probability: 0.9300\n",
            "Processing image: deposit_70_scar.png\n",
            "Top class: deposit, Probability: 0.9400\n",
            "Processing image: apac_234_scar.png\n",
            "Top class: deposit, Probability: 0.9400\n",
            "Processing image: lens-opacity_85_scar.png\n",
            "Top class: lens opacity, Probability: 0.9000\n",
            "Processing image: non-infection_99_scar.png\n",
            "Top class: non-infection, Probability: 0.8200\n",
            "Processing image: non-infection_195_scar.png\n",
            "Top class: non-infection, Probability: 0.9100\n",
            "Processing image: lens-opacity_205_scar.png\n",
            "Top class: lens opacity, Probability: 0.9400\n",
            "Processing image: lens-opacity_116_scar.png\n",
            "Top class: lens opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_43_scar.png\n",
            "Top class: lens opacity, Probability: 0.7900\n",
            "Processing image: non-infection128_scar.png\n",
            "Top class: scar, Probability: 0.9100\n",
            "Processing image: non-infection_210_scar.png\n",
            "Top class: non-infection, Probability: 0.8900\n",
            "Processing image: non-infection_223_scar.png\n",
            "Top class: non-infection, Probability: 0.8900\n",
            "Processing image: lens-opacity_196_scar.png\n",
            "Top class: lens opacity, Probability: 0.9000\n",
            "Processing image: non-infection_128_scar.png\n",
            "Top class: scar, Probability: 0.7500\n",
            "Processing subdirectory: back_bullous\n",
            "Processing image: apac_5_bullous.png\n",
            "Top class: deposit, Probability: 0.6900\n",
            "Processing image: apac_124_bullous.png\n",
            "Top class: APAC, Probability: 0.9300\n",
            "Processing image: apac_203_bullous.png\n",
            "Top class: APAC, Probability: 0.9400\n",
            "Processing image: apac_221_bullous.png\n",
            "Top class: APAC, Probability: 0.9600\n",
            "Processing image: bullous_132_bullous.png\n",
            "Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_139_bullous.png\n",
            "Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_143_bullous.png\n",
            "Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_bullous.png\n",
            "Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_bullous.png\n",
            "Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_bullous.png\n",
            "Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_64_bullous.png\n",
            "Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_70_bullous.png\n",
            "Top class: deposit, Probability: 0.8300\n",
            "Processing image: deposit_77_bullous.png\n",
            "Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_bullous.png\n",
            "Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_75_bullous.png\n",
            "Top class: scar, Probability: 0.4900\n",
            "Processing image: infection_81_bullous.png\n",
            "Top class: infection, Probability: 0.6500\n",
            "Processing image: infection_97_bullous.png\n",
            "Top class: non-infection, Probability: 0.8500\n",
            "Processing image: normal_8_bullous.png\n",
            "Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_bullous.png\n",
            "Top class: non-infection, Probability: 0.6400\n",
            "Processing image: normal_110_bullous.png\n",
            "Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_159_bullous.png\n",
            "Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_189_bullous.png\n",
            "Top class: normal, Probability: 0.9500\n",
            "Processing image: scar_69_bullous.png\n",
            "Top class: scar, Probability: 0.7300\n",
            "Processing image: scar_82_bullous.png\n",
            "Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_114_bullous.png\n",
            "Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_bullous.png\n",
            "Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_bullous.png\n",
            "Top class: scar, Probability: 0.9000\n",
            "Processing image: tumor_15_bullous.png\n",
            "Top class: tumor, Probability: 0.8400\n",
            "Processing image: tumor_95_bullous.png\n",
            "Top class: tumor, Probability: 0.8400\n",
            "Processing image: tumor_98_bullous.png\n",
            "Top class: tumor, Probability: 0.8600\n",
            "Processing image: tumor_183_bullous.png\n",
            "Top class: tumor, Probability: 0.8900\n",
            "Processing image: tumor_220_bullous.png\n",
            "Top class: tumor, Probability: 0.8900\n",
            "Processing image: infection_29_bullous.png\n",
            "Top class: scar, Probability: 0.8900\n",
            "Processing image: infection_1_bullous.png\n",
            "Top class: scar, Probability: 0.9400\n",
            "Processing image: apac_234_bullous.png\n",
            "Top class: APAC, Probability: 0.6500\n",
            "Processing image: .DS_Store\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-24872c571598>\u001b[0m in \u001b[0;36m<cell line: 195>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOV5TorchObjectDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"infection\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"non-infection\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"scar\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tumor\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"deposit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"APAC\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lens opacity\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bullous\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mfolder_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-24872c571598>\u001b[0m in \u001b[0;36mfolder_main\u001b[0;34m(folder_path, model)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing image: {img_basename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mtop_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Top class: {top_class}, Probability: {top_prob:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-24872c571598>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(img_path, model)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0mtorch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Apply finite constraint\n",
        "            # if not torch.isfinite(x).all():\n",
        "            #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        print(f\"Processing directory: {root}\")\n",
        "\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "                print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "                print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)  # 入力画像サイズ\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"])\n",
        "    folder_main(folder_path, model)"
      ],
      "metadata": {
        "id": "mai-uKp1ibJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMMRiqfA4e4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "## 画像のpathを指定したinference\n",
        "################################\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torchvision\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch.nn as nn\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        self.names = names if names else self.model.names\n",
        "\n",
        "        # Prevent cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    self.class_names[i].append(self.names[cls])\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def process_single_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    # Top 1 クラスとその確率を取得\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        print(f\"Processing directory: {root}\")\n",
        "\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "                print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "                print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)  # 入力画像サイズ\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"])\n",
        "\n",
        "    # ユーザーに画像のパスを入力してもらう\n",
        "    img_path = \"/content/lens_opacity_43_lens_opacity.png\"\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        top_class, top_prob = process_single_image(img_path, model)\n",
        "        print(f\"Image: {os.path.basename(img_path)}\")\n",
        "        print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "    else:\n",
        "        print(f\"Error: The file {img_path} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j_4xO0a4e6B",
        "outputId": "854ab70a-b7d4-419d-c9fd-e3e4bf39b8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "Image: lens_opacity_43_lens_opacity.png\n",
            "Top class: lens opacity, Probability: 0.3300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**create dataframe to csv**\n",
        "\n",
        "cutmix-images"
      ],
      "metadata": {
        "id": "9FhPI6scaIRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "## create dataframe to csv ##\n",
        "#############################\n",
        "\n",
        "\"\"\"\n",
        "Underbar --> Hyphenを行ってから\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    results = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "\n",
        "                # ファイル名をハイフンでsplitし、1番目と3番目を取る\n",
        "                parts = img_basename.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    cornea = parts[0]\n",
        "                    image_id = f\"{parts[0]}_{parts[1]}\"\n",
        "                    background = parts[2].split('.')[0]\n",
        "                    results.append({\n",
        "                        \"basename\": img_basename,\n",
        "                        \"image_id\": image_id,\n",
        "                        \"cornea\": cornea,\n",
        "                        \"background\": background,\n",
        "                        \"pred\": top_class,\n",
        "                        \"prob\": top_prob\n",
        "                    })\n",
        "\n",
        "                print(f\"Processing image: {img_basename} Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=[\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"apac\",\"lens-opacity\",\"bullous\"])\n",
        "\n",
        "    df = folder_main(folder_path, model)\n",
        "\n",
        "    print(\"\\nDataFrame:\")\n",
        "    print(df)\n",
        "\n",
        "    csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nCSV saved to: {csv_path}\")"
      ],
      "metadata": {
        "id": "B0iUX9rL7Rzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412caaba-d6ed-44fb-e070-b85313bb580d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Processing folder: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "Processing image: apac_5_normal.png Top class: deposit, Probability: 0.8600\n",
            "Processing image: apac_124_normal.png Top class: apac, Probability: 0.8500\n",
            "Processing image: apac_203_normal.png Top class: apac, Probability: 0.9200\n",
            "Processing image: apac_221_normal.png Top class: apac, Probability: 0.6400\n",
            "Processing image: bullous_132_normal.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: bullous_139_normal.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_normal.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_normal.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: bullous_212_normal.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: deposit_57_normal.png Top class: deposit, Probability: 0.9200\n",
            "Processing image: deposit_64_normal.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_70_normal.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_77_normal.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_normal.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_1_normal.png Top class: scar, Probability: 0.9000\n",
            "Processing image: infection_29_normal.png Top class: bullous, Probability: 0.6600\n",
            "Processing image: infection_75_normal.png Top class: infection, Probability: 0.8600\n",
            "Processing image: infection_81_normal.png Top class: infection, Probability: 0.5000\n",
            "Processing image: infection_97_normal.png Top class: scar, Probability: 0.5300\n",
            "Processing image: lens-opacity_43_normal.png Top class: lens-opacity, Probability: 0.6400\n",
            "Processing image: lens-opacity_85_normal.png Top class: lens-opacity, Probability: 0.8900\n",
            "Processing image: lens-opacity_116_normal.png Top class: lens-opacity, Probability: 0.8100\n",
            "Processing image: lens-opacity_196_normal.png Top class: lens-opacity, Probability: 0.7900\n",
            "Processing image: non-infection_99_normal.png Top class: non-infection, Probability: 0.8600\n",
            "Processing image: non-infection_128_normal.png Top class: scar, Probability: 0.9100\n",
            "Processing image: non-infection_195_normal.png Top class: normal, Probability: 0.4100\n",
            "Processing image: non-infection_210_normal.png Top class: non-infection, Probability: 0.8300\n",
            "Processing image: non-infection_223_normal.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: normal_8_normal.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_normal.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_normal.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_159_normal.png Top class: normal, Probability: 0.9800\n",
            "Processing image: normal_189_normal.png Top class: normal, Probability: 0.9600\n",
            "Processing image: scar_69_normal.png Top class: scar, Probability: 0.8600\n",
            "Processing image: scar_82_normal.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_114_normal.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_127_normal.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_normal.png Top class: scar, Probability: 0.9400\n",
            "Processing image: tumor_15_normal.png Top class: tumor, Probability: 0.9200\n",
            "Processing image: tumor_95_normal.png Top class: tumor, Probability: 0.9300\n",
            "Processing image: tumor_98_normal.png Top class: tumor, Probability: 0.9200\n",
            "Processing image: tumor_183_normal.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_220_normal.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: lens-opacity_205_normal.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: apac_234_normal.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: apac_5_bullous.png Top class: deposit, Probability: 0.6900\n",
            "Processing image: apac_124_bullous.png Top class: apac, Probability: 0.9300\n",
            "Processing image: apac_203_bullous.png Top class: apac, Probability: 0.9400\n",
            "Processing image: apac_221_bullous.png Top class: apac, Probability: 0.9600\n",
            "Processing image: bullous_132_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_139_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_143_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_bullous.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_bullous.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_64_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_70_bullous.png Top class: deposit, Probability: 0.8300\n",
            "Processing image: deposit_77_bullous.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_bullous.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_75_bullous.png Top class: scar, Probability: 0.4900\n",
            "Processing image: infection_81_bullous.png Top class: infection, Probability: 0.6500\n",
            "Processing image: infection_97_bullous.png Top class: non-infection, Probability: 0.8500\n",
            "Processing image: normal_8_bullous.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_bullous.png Top class: non-infection, Probability: 0.6400\n",
            "Processing image: normal_110_bullous.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_159_bullous.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_189_bullous.png Top class: normal, Probability: 0.9500\n",
            "Processing image: scar_69_bullous.png Top class: scar, Probability: 0.7300\n",
            "Processing image: scar_82_bullous.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_114_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_bullous.png Top class: scar, Probability: 0.9000\n",
            "Processing image: tumor_15_bullous.png Top class: tumor, Probability: 0.8400\n",
            "Processing image: tumor_95_bullous.png Top class: tumor, Probability: 0.8400\n",
            "Processing image: tumor_98_bullous.png Top class: tumor, Probability: 0.8600\n",
            "Processing image: tumor_183_bullous.png Top class: tumor, Probability: 0.8900\n",
            "Processing image: tumor_220_bullous.png Top class: tumor, Probability: 0.8900\n",
            "Processing image: infection_29_bullous.png Top class: scar, Probability: 0.8900\n",
            "Processing image: infection_1_bullous.png Top class: scar, Probability: 0.9400\n",
            "Processing image: apac_234_bullous.png Top class: apac, Probability: 0.6500\n",
            "Processing image: lens-opacity_43_bullous.png Top class: lens-opacity, Probability: 0.5400\n",
            "Processing image: non-infection_195_bullous.png Top class: non-infection, Probability: 0.9600\n",
            "Processing image: non-infection_223_bullous.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: lens-opacity_196_bullous.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: non-infection_210_bullous.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_99_bullous.png Top class: non-infection, Probability: 0.9600\n",
            "Processing image: lens-opacity_205_bullous.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_128_bullous.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: lens-opacity_116_bullous.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_85_bullous.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: apac_5_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: apac_124_infection.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_203_infection.png Top class: apac, Probability: 0.9800\n",
            "Processing image: apac_221_infection.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_infection.png Top class: infection, Probability: 0.4200\n",
            "Processing image: bullous_139_infection.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: bullous_143_infection.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_infection.png Top class: bullous, Probability: 0.8200\n",
            "Processing image: bullous_212_infection.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: deposit_57_infection.png Top class: infection, Probability: 0.9500\n",
            "Processing image: deposit_64_infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_70_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_77_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: deposit_138_infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: infection_1_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: infection_29_infection.png Top class: infection, Probability: 0.7400\n",
            "Processing image: infection_75_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: infection_81_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: infection_97_infection.png Top class: infection, Probability: 0.9700\n",
            "Processing image: normal_8_infection.png Top class: infection, Probability: 0.9400\n",
            "Processing image: normal_73_infection.png Top class: infection, Probability: 0.9000\n",
            "Processing image: normal_110_infection.png Top class: non-infection, Probability: 0.5900\n",
            "Processing image: normal_159_infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_189_infection.png Top class: infection, Probability: 0.8800\n",
            "Processing image: scar_69_infection.png Top class: infection, Probability: 0.9500\n",
            "Processing image: scar_82_infection.png Top class: non-infection, Probability: 0.7400\n",
            "Processing image: scar_114_infection.png Top class: infection, Probability: 0.9600\n",
            "Processing image: scar_127_infection.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_186_infection.png Top class: infection, Probability: 0.6500\n",
            "Processing image: tumor_15_infection.png Top class: tumor, Probability: 0.8800\n",
            "Processing image: tumor_95_infection.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_98_infection.png Top class: infection, Probability: 0.6100\n",
            "Processing image: tumor_183_infection.png Top class: tumor, Probability: 0.7400\n",
            "Processing image: tumor_220_infection.png Top class: tumor, Probability: 0.9200\n",
            "Processing image: apac_234_infection.png Top class: apac, Probability: 0.9200\n",
            "Processing image: non-infection_99_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_210_infection.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: lens-opacity_205_infection.png Top class: lens-opacity, Probability: 0.4700\n",
            "Processing image: lens-opacity_43_infection.png Top class: non-infection, Probability: 0.4000\n",
            "Processing image: lens-opacity_196_infection.png Top class: non-infection, Probability: 0.7300\n",
            "Processing image: lens-opacity_116_infection.png Top class: lens-opacity, Probability: 0.5200\n",
            "Processing image: lens-opacity_85_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_195_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_128_infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: non-infection_223_infection.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: apac_221_tumor.png Top class: apac, Probability: 0.9000\n",
            "Processing image: apac_5_tumor.png Top class: bullous, Probability: 0.5300\n",
            "Processing image: apac_124_tumor.png Top class: apac, Probability: 0.9500\n",
            "Processing image: apac_203_tumor.png Top class: apac, Probability: 0.9300\n",
            "Processing image: bullous_132_tumor.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_139_tumor.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_143_tumor.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_191_tumor.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_tumor.png Top class: bullous, Probability: 0.9700\n",
            "Processing image: deposit_57_tumor.png Top class: deposit, Probability: 0.9500\n",
            "Processing image: deposit_64_tumor.png Top class: deposit, Probability: 0.9400\n",
            "Processing image: deposit_70_tumor.png Top class: deposit, Probability: 0.9300\n",
            "Processing image: deposit_77_tumor.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_tumor.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_tumor.png Top class: scar, Probability: 0.9000\n",
            "Processing image: infection_29_tumor.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: infection_75_tumor.png Top class: infection, Probability: 0.3700\n",
            "Processing image: infection_81_tumor.png Top class: infection, Probability: 0.6800\n",
            "Processing image: infection_97_tumor.png Top class: non-infection, Probability: 0.2700\n",
            "Processing image: normal_8_tumor.png Top class: normal, Probability: 0.9300\n",
            "Processing image: normal_73_tumor.png Top class: normal, Probability: 0.7100\n",
            "Processing image: normal_110_tumor.png Top class: normal, Probability: 0.8300\n",
            "Processing image: normal_159_tumor.png Top class: normal, Probability: 0.8500\n",
            "Processing image: normal_189_tumor.png Top class: normal, Probability: 0.7900\n",
            "Processing image: scar_69_tumor.png Top class: bullous, Probability: 0.4700\n",
            "Processing image: scar_82_tumor.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_114_tumor.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_tumor.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_186_tumor.png Top class: scar, Probability: 0.6900\n",
            "Processing image: tumor_15_tumor.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_95_tumor.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_tumor.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_183_tumor.png Top class: tumor, Probability: 0.9000\n",
            "Processing image: tumor_220_tumor.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: apac_234_tumor.png Top class: apac, Probability: 0.9600\n",
            "Processing image: non-infection_210_tumor.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: non-infection_223_tumor.png Top class: non-infection, Probability: 0.7500\n",
            "Processing image: lens-opacity_85_tumor.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: non-infection_195_tumor.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: lens-opacity_43_tumor.png Top class: lens-opacity, Probability: 0.5000\n",
            "Processing image: non-infection_99_tumor.png Top class: non-infection, Probability: 0.8200\n",
            "Processing image: non-infection_128_tumor.png Top class: scar, Probability: 0.9300\n",
            "Processing image: lens-opacity_196_tumor.png Top class: lens-opacity, Probability: 0.7600\n",
            "Processing image: lens-opacity_205_tumor.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: lens-opacity_116_tumor.png Top class: lens-opacity, Probability: 0.5900\n",
            "Processing image: apac_5_deposit.png Top class: bullous, Probability: 0.3000\n",
            "Processing image: apac_124_deposit.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_203_deposit.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_221_deposit.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_deposit.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: bullous_139_deposit.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_deposit.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_deposit.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_deposit.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: deposit_57_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_64_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_70_deposit.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_138_deposit.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: infection_1_deposit.png Top class: scar, Probability: 0.9400\n",
            "Processing image: infection_29_deposit.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: infection_75_deposit.png Top class: infection, Probability: 0.8300\n",
            "Processing image: infection_81_deposit.png Top class: deposit, Probability: 0.7600\n",
            "Processing image: infection_97_deposit.png Top class: scar, Probability: 0.4600\n",
            "Processing image: normal_8_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_73_deposit.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_159_deposit.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_189_deposit.png Top class: normal, Probability: 0.9600\n",
            "Processing image: scar_69_deposit.png Top class: scar, Probability: 0.7800\n",
            "Processing image: scar_82_deposit.png Top class: scar, Probability: 0.9100\n",
            "Processing image: scar_114_deposit.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_127_deposit.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_186_deposit.png Top class: scar, Probability: 0.8700\n",
            "Processing image: tumor_15_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_deposit.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_183_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_220_deposit.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: apac_234_deposit.png Top class: deposit, Probability: 0.9100\n",
            "Processing image: lens-opacity_85_deposit.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: lens-opacity_43_deposit.png Top class: lens-opacity, Probability: 0.8500\n",
            "Processing image: non-infection_210_deposit.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: lens-opacity_205_deposit.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_99_deposit.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_128_deposit.png Top class: scar, Probability: 0.4800\n",
            "Processing image: lens-opacity_116_deposit.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: non-infection_195_deposit.png Top class: non-infection, Probability: 0.8600\n",
            "Processing image: non-infection_223_deposit.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: lens-opacity_196_deposit.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: apac_5_scar.png Top class: bullous, Probability: 0.8800\n",
            "Processing image: apac_124_scar.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_203_scar.png Top class: apac, Probability: 0.9700\n",
            "Processing image: apac_221_scar.png Top class: apac, Probability: 0.9700\n",
            "Processing image: bullous_132_scar.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_139_scar.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_scar.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_191_scar.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_212_scar.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: deposit_57_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_scar.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_1_scar.png Top class: scar, Probability: 0.9200\n",
            "Processing image: infection_29_scar.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: infection_75_scar.png Top class: infection, Probability: 0.7300\n",
            "Processing image: infection_81_scar.png Top class: scar, Probability: 0.7100\n",
            "Processing image: infection_97_scar.png Top class: bullous, Probability: 0.8100\n",
            "Processing image: normal_8_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_73_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_110_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_159_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: normal_189_scar.png Top class: normal, Probability: 0.9700\n",
            "Processing image: scar_89_scar.png Top class: scar, Probability: 0.9100\n",
            "Processing image: scar_82_scar.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_114_scar.png Top class: scar, Probability: 0.9500\n",
            "Processing image: scar_127_scar.png Top class: scar, Probability: 0.9400\n",
            "Processing image: scar_186_scar.png Top class: scar, Probability: 0.8400\n",
            "Processing image: tumor_15_scar.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_scar.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_98_scar.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_183_scar.png Top class: tumor, Probability: 0.8600\n",
            "Processing image: tumor_220_scar.png Top class: tumor, Probability: 0.9300\n",
            "Processing image: deposit_70_scar.png Top class: deposit, Probability: 0.9400\n",
            "Processing image: apac_234_scar.png Top class: deposit, Probability: 0.9400\n",
            "Processing image: lens-opacity_85_scar.png Top class: lens-opacity, Probability: 0.9000\n",
            "Processing image: non-infection_99_scar.png Top class: non-infection, Probability: 0.8200\n",
            "Processing image: non-infection_195_scar.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: lens-opacity_205_scar.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: lens-opacity_116_scar.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: lens-opacity_43_scar.png Top class: lens-opacity, Probability: 0.7900\n",
            "Processing image: non-infection128_scar.png Top class: scar, Probability: 0.9100\n",
            "Processing image: non-infection_210_scar.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: non-infection_223_scar.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: lens-opacity_196_scar.png Top class: lens-opacity, Probability: 0.9000\n",
            "Processing image: non-infection_128_scar.png Top class: scar, Probability: 0.7500\n",
            "Processing image: apac_234_non-infection.png Top class: apac, Probability: 0.9400\n",
            "Processing image: lens-opacity_116_non-infection.png Top class: lens-opacity, Probability: 0.3900\n",
            "Processing image: tumor_183_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: lens-opacity_85_non-infection.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: deposit_77_non-infection.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: scar_127_non-infection.png Top class: scar, Probability: 0.9500\n",
            "Processing image: normal_73_non-infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: non-infection_99_non-infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: scar_82_non-infection.png Top class: scar, Probability: 0.9300\n",
            "Processing image: tumor_98_non-infection.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: infection_81_non-infection.png Top class: infection, Probability: 0.9200\n",
            "Processing image: deposit_70_non-infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: scar_114_non-infection.png Top class: scar, Probability: 0.9400\n",
            "Processing image: apac_221_non-infection.png Top class: apac, Probability: 0.8600\n",
            "Processing image: apac_124_non-infection.png Top class: apac, Probability: 0.8200\n",
            "Processing image: deposit_138_non-infection.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: infection_75_non-infection.png Top class: infection, Probability: 0.9400\n",
            "Processing image: non-infection_223_non-infection.png Top class: non-infection, Probability: 0.7800\n",
            "Processing image: tumor_95_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: scar_186_non-infection.png Top class: scar, Probability: 0.8700\n",
            "Processing image: bullous_132_non-infection.png Top class: bullous, Probability: 0.8900\n",
            "Processing image: lens-opacity_43_non-infection.png Top class: lens-opacity, Probability: 0.5300\n",
            "Processing image: bullous_212_non-infection.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: normal_110_non-infection.png Top class: normal, Probability: 0.9500\n",
            "Processing image: deposit_57_non-infection.png Top class: deposit, Probability: 0.9600\n",
            "Processing image: tumor_15_non-infection.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: normal_159_non-infection.png Top class: normal, Probability: 0.9700\n",
            "Processing image: apac_203_non-infection.png Top class: apac, Probability: 0.9600\n",
            "Processing image: normal_189_non-infection.png Top class: normal, Probability: 0.8700\n",
            "Processing image: lens-opacity_205_non-infection.png Top class: lens-opacity, Probability: 0.9500\n",
            "Processing image: non-infection_195_non-infection.png Top class: non-infection, Probability: 0.9100\n",
            "Processing image: deposit_64_non-infection.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: normal_8_non-infection.png Top class: normal, Probability: 0.9700\n",
            "Processing image: infection_1_non-infection.png Top class: infection, Probability: 0.7900\n",
            "Processing image: bullous_143_non-infection.png Top class: bullous, Probability: 0.9600\n",
            "Processing image: bullous_139_non-infection.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: tumor_220_non-infection.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: infection_97_non-infection.png Top class: bullous, Probability: 0.4400\n",
            "Processing image: non-infection_210_non-infection.png Top class: non-infection, Probability: 0.8900\n",
            "Processing image: apac_5_non-infection.png Top class: infection, Probability: 0.3600\n",
            "Processing image: lens-opacity_196_non-infection.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: infection_29_non-infection.png Top class: bullous, Probability: 0.8900\n",
            "Processing image: bullous_101_non-infection.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: scar_69_non-infection.png Top class: bullous, Probability: 0.4500\n",
            "Processing image: non-infection_128_non-infection.png Top class: non-infection, Probability: 0.7300\n",
            "Processing image: apac_5_apac.png Top class: apac, Probability: 0.9300\n",
            "Processing image: apac_124_apac.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_203_apac.png Top class: apac, Probability: 0.9600\n",
            "Processing image: apac_221_apac.png Top class: apac, Probability: 0.9400\n",
            "Processing image: apac_234_apac.png Top class: apac, Probability: 0.9600\n",
            "Processing image: bullous_132_apac.png Top class: bullous, Probability: 0.4300\n",
            "Processing image: bullous_139_apac.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_143_apac.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: bullous_191_apac.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_212_apac.png Top class: bullous, Probability: 0.8800\n",
            "Processing image: deposit_57_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_70_apac.png Top class: deposit, Probability: 0.8800\n",
            "Processing image: deposit_77_apac.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_apac.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: infection_1_apac.png Top class: scar, Probability: 0.9100\n",
            "Processing image: infection_29_apac.png Top class: infection, Probability: 0.8600\n",
            "Processing image: infection_75_apac.png Top class: infection, Probability: 0.7800\n",
            "Processing image: infection_81_apac.png Top class: infection, Probability: 0.9400\n",
            "Processing image: infection_97_apac.png Top class: infection, Probability: 0.9300\n",
            "Processing image: lens-opacity_43_apac.png Top class: tumor, Probability: 0.6500\n",
            "Processing image: lens-opacity_85_apac.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: lens-opacity_116_apac.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: lens-opacity_196_apac.png Top class: lens-opacity, Probability: 0.8900\n",
            "Processing image: lens-opacity_205_apac.png Top class: lens-opacity, Probability: 0.9400\n",
            "Processing image: non-infection_99_apac.png Top class: non-infection, Probability: 0.9500\n",
            "Processing image: non-infection_128_apac.png Top class: non-infection, Probability: 0.9000\n",
            "Processing image: non-infection_195_apac.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: non-infection_210_apac.png Top class: non-infection, Probability: 0.9400\n",
            "Processing image: non-infection_223_apac.png Top class: non-infection, Probability: 0.7000\n",
            "Processing image: normal_8_apac.png Top class: normal, Probability: 0.5400\n",
            "Processing image: normal_73_apac.png Top class: non-infection, Probability: 0.7600\n",
            "Processing image: normal_110_apac.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_159_apac.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_189_apac.png Top class: infection, Probability: 0.6100\n",
            "Processing image: scar_69_apac.png Top class: infection, Probability: 0.8100\n",
            "Processing image: scar_82_apac.png Top class: scar, Probability: 0.8500\n",
            "Processing image: scar_114_apac.png Top class: scar, Probability: 0.9100\n",
            "Processing image: scar_127_apac.png Top class: scar, Probability: 0.8900\n",
            "Processing image: scar_186_apac.png Top class: scar, Probability: 0.9100\n",
            "Processing image: tumor_15_apac.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: tumor_95_apac.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_98_apac.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_183_apac.png Top class: tumor, Probability: 0.9400\n",
            "Processing image: tumor_220_apac.png Top class: tumor, Probability: 0.9500\n",
            "Processing image: deposit_57_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_64_lens-opacity.png Top class: deposit, Probability: 0.9800\n",
            "Processing image: deposit_70_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_77_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: deposit_138_lens-opacity.png Top class: deposit, Probability: 0.9700\n",
            "Processing image: apac_5_lens-opacity.png Top class: bullous, Probability: 0.6800\n",
            "Processing image: apac_124_lens-opacity.png Top class: apac, Probability: 0.7300\n",
            "Processing image: apac_203_lens-opacity.png Top class: apac, Probability: 0.8800\n",
            "Processing image: apac_221_lens-opacity.png Top class: apac, Probability: 0.7800\n",
            "Processing image: apac_234_lens-opacity.png Top class: deposit, Probability: 0.8900\n",
            "Processing image: bullous_132_lens-opacity.png Top class: bullous, Probability: 0.9000\n",
            "Processing image: bullous_139_lens-opacity.png Top class: bullous, Probability: 0.9200\n",
            "Processing image: bullous_143_lens-opacity.png Top class: bullous, Probability: 0.9500\n",
            "Processing image: bullous_191_lens-opacity.png Top class: bullous, Probability: 0.9300\n",
            "Processing image: bullous_212_lens-opacity.png Top class: bullous, Probability: 0.9400\n",
            "Processing image: infection_1_lens-opacity.png Top class: scar, Probability: 0.7100\n",
            "Processing image: infection_29_lens-opacity.png Top class: bullous, Probability: 0.9100\n",
            "Processing image: infection_75_lens-opacity.png Top class: deposit, Probability: 0.5300\n",
            "Processing image: infection_81_lens-opacity.png Top class: deposit, Probability: 0.5700\n",
            "Processing image: infection_97_lens-opacity.png Top class: bullous, Probability: 0.3800\n",
            "Processing image: lens-opacity_85_lens-opacity.png Top class: lens-opacity, Probability: 0.9200\n",
            "Processing image: lens-opacity_116_lens-opacity.png Top class: lens-opacity, Probability: 0.8700\n",
            "Processing image: lens-opacity_196_lens-opacity.png Top class: lens-opacity, Probability: 0.4500\n",
            "Processing image: lens-opacity_205_lens-opacity.png Top class: lens-opacity, Probability: 0.9300\n",
            "Processing image: non-infection_99_lens-opacity.png Top class: non-infection, Probability: 0.8300\n",
            "Processing image: non-infection_128_lens-opacity.png Top class: scar, Probability: 0.9100\n",
            "Processing image: non-infection_195_lens-opacity.png Top class: non-infection, Probability: 0.7800\n",
            "Processing image: non-infection_210_lens-opacity.png Top class: tumor, Probability: 0.8200\n",
            "Processing image: non-infection_223_lens-opacity.png Top class: non-infection, Probability: 0.6400\n",
            "Processing image: normal_8_lens-opacity.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_73_lens-opacity.png Top class: normal, Probability: 0.9400\n",
            "Processing image: normal_110_lens-opacity.png Top class: normal, Probability: 0.9600\n",
            "Processing image: normal_159_lens-opacity.png Top class: normal, Probability: 0.9500\n",
            "Processing image: normal_189_lens-opacity.png Top class: normal, Probability: 0.9200\n",
            "Processing image: scar_69_lens-opacity.png Top class: scar, Probability: 0.5700\n",
            "Processing image: scar_82_lens-opacity.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_114_lens-opacity.png Top class: scar, Probability: 0.9300\n",
            "Processing image: scar_127_lens-opacity.png Top class: scar, Probability: 0.9200\n",
            "Processing image: scar_186_lens-opacity.png Top class: scar, Probability: 0.8800\n",
            "Processing image: tumor_15_lens-opacity.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_95_lens-opacity.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: tumor_98_lens-opacity.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_183_lens-opacity.png Top class: tumor, Probability: 0.9600\n",
            "Processing image: tumor_220_lens-opacity.png Top class: tumor, Probability: 0.9700\n",
            "Processing image: lens-opacity_43_lens-opacity.png Top class: lens-opacity, Probability: 0.3300\n",
            "\n",
            "DataFrame:\n",
            "                             basename         image_id        cornea  \\\n",
            "0                   apac_5_normal.png           apac_5          apac   \n",
            "1                 apac_124_normal.png         apac_124          apac   \n",
            "2                 apac_203_normal.png         apac_203          apac   \n",
            "3                 apac_221_normal.png         apac_221          apac   \n",
            "4              bullous_132_normal.png      bullous_132       bullous   \n",
            "..                                ...              ...           ...   \n",
            "400         tumor_95_lens-opacity.png         tumor_95         tumor   \n",
            "401         tumor_98_lens-opacity.png         tumor_98         tumor   \n",
            "402        tumor_183_lens-opacity.png        tumor_183         tumor   \n",
            "403        tumor_220_lens-opacity.png        tumor_220         tumor   \n",
            "404  lens-opacity_43_lens-opacity.png  lens-opacity_43  lens-opacity   \n",
            "\n",
            "       background          pred  prob  \n",
            "0          normal       deposit  0.86  \n",
            "1          normal          apac  0.85  \n",
            "2          normal          apac  0.92  \n",
            "3          normal          apac  0.64  \n",
            "4          normal       bullous  0.91  \n",
            "..            ...           ...   ...  \n",
            "400  lens-opacity         tumor  0.97  \n",
            "401  lens-opacity         tumor  0.96  \n",
            "402  lens-opacity         tumor  0.96  \n",
            "403  lens-opacity         tumor  0.97  \n",
            "404  lens-opacity  lens-opacity  0.33  \n",
            "\n",
            "[405 rows x 6 columns]\n",
            "\n",
            "CSV saved to: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw matrix"
      ],
      "metadata": {
        "id": "jEmlfJ91BMAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "file_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cutmix.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 正しい判定の割合\n",
        "correct_predictions = df[df['cornea'] == df['pred']]\n",
        "accuracy = len(correct_predictions) / len(df)\n",
        "\n",
        "# 各状態に対する正しい判定の割合\n",
        "correct_by_cornea = df[df['cornea'] == df['pred']].groupby('cornea').size() / df.groupby('cornea').size()\n",
        "\n",
        "# 確率の分布\n",
        "correct_probs = correct_predictions['prob']\n",
        "incorrect_probs = df[df['cornea'] != df['pred']]['prob']\n",
        "\n",
        "# ヒストグラムをプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(correct_probs, bins=10, alpha=0.5, label='Correct Predictions', color='blue')\n",
        "plt.hist(incorrect_probs, bins=10, alpha=0.5, label='Incorrect Predictions', color='red')\n",
        "plt.xlabel('Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Probability Distribution of Correct and Incorrect Predictions')\n",
        "plt.show()\n",
        "\n",
        "# グラウンドトゥルースごとの背景に対する正解率マトリックスを作成\n",
        "matrix = df.pivot_table(index='cornea', columns='background', values='pred', aggfunc=lambda x: (x == df.loc[x.index, 'cornea']).mean())\n",
        "\n",
        "# マトリックス図をプロット\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title('Accuracy Matrix: Cornea vs Background')\n",
        "plt.xlabel('Background')\n",
        "plt.ylabel('Cornea (Ground Truth)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QOWWO0dKBQ1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkMbYsOGc1KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SWPhfox_BbpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aWI7LQJpDz0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference cornea vs original images**"
      ],
      "metadata": {
        "id": "fWXmIziTD2JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def folder_main(folder_path, model):\n",
        "    results = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for item in files:\n",
        "            if item.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                img_path = os.path.join(root, item)\n",
        "                img_basename = os.path.basename(img_path)\n",
        "\n",
        "                top_class, top_prob = process_image(img_path, model)\n",
        "\n",
        "                # Split filename to get groundtruth\n",
        "                parts = img_basename.split('_')\n",
        "                if len(parts) >= 2:\n",
        "                    groundtruth = parts[0]\n",
        "                    image_id = f\"{parts[0]}_{parts[1]}\"\n",
        "                    results.append({\n",
        "                        \"basename\": img_basename,\n",
        "                        \"image_id\": f\"{parts[0]}_{parts[1].split('.')[0]}\",\n",
        "                        \"cornea_groundtruth\": groundtruth,\n",
        "                        \"pred\": top_class,\n",
        "                        \"prob\": top_prob\n",
        "                    })\n",
        "\n",
        "                print(f\"Processing image: {img_basename} Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea'\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    # Updated class names\n",
        "    classes = names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"apac\", \"lens-opacity\", \"bullous\"]\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=classes)\n",
        "\n",
        "    df = folder_main(folder_path, model)\n",
        "\n",
        "    print(\"\\nDataFrame:\")\n",
        "    print(df)\n",
        "    csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nCSV saved to: {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq_uhTR-Pfky",
        "outputId": "9d39f7e3-b718-47b9-b02e-b049726bde71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Processing folder: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "Processing image: tumor_95.png Top class: tumor, Probability: 0.8900\n",
            "Processing image: infection_1.png Top class: infection, Probability: 0.6600\n",
            "Processing image: tumor_15.png Top class: tumor, Probability: 0.8500\n",
            "Processing image: deposit_64.png Top class: deposit, Probability: 0.9100\n",
            "Processing image: infection_81.png Top class: infection, Probability: 0.7600\n",
            "Processing image: tumor_98.png Top class: tumor, Probability: 0.8200\n",
            "Processing image: scar_127.png Top class: scar, Probability: 0.8100\n",
            "Processing image: deposit_138.png Top class: deposit, Probability: 0.9200\n",
            "Processing image: bullous_139.png Top class: bullous, Probability: 0.7600\n",
            "Processing image: tumor_220.png Top class: tumor, Probability: 0.9300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "def process_image(img_path, model):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    torch_img = model.preprocessing(img)\n",
        "    with torch.no_grad():\n",
        "        prediction, logits = model(torch_img)\n",
        "    if len(prediction[2][0]) > 0:\n",
        "        top_class = prediction[2][0][0]\n",
        "        top_prob = prediction[3][0][0]\n",
        "        return top_class, top_prob\n",
        "    else:\n",
        "        return \"No detection\", 0.0\n",
        "\n",
        "def process_folders(folder_path_cornea, folder_path_original, model):\n",
        "    results = []\n",
        "    for cornea_img_name in os.listdir(folder_path_cornea):\n",
        "        if cornea_img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "            cornea_img_path = os.path.join(folder_path_cornea, cornea_img_name)\n",
        "\n",
        "            # Construct the corresponding original image name (always .jpg)\n",
        "            original_img_name = os.path.splitext(cornea_img_name)[0] + '.jpg'\n",
        "            original_img_path = os.path.join(folder_path_original, original_img_name)\n",
        "\n",
        "            if not os.path.exists(original_img_path):\n",
        "                print(f\"Warning: Corresponding original image not found for {cornea_img_name}\")\n",
        "                continue\n",
        "\n",
        "            # Process cornea image\n",
        "            cornea_top_class, cornea_top_prob = process_image(cornea_img_path, model)\n",
        "\n",
        "            # Process original image\n",
        "            original_top_class, original_top_prob = process_image(original_img_path, model)\n",
        "\n",
        "            # Split filename to get groundtruth\n",
        "            parts = cornea_img_name.split('_')\n",
        "            groundtruth = parts[0] if len(parts) >= 2 else \"Unknown\"\n",
        "\n",
        "            results.append({\n",
        "                \"basename\": cornea_img_name,\n",
        "                \"image_id\": f\"{parts[0]}_{parts[1].split('.')[0]}\",\n",
        "                \"groundtruth\": groundtruth,\n",
        "                \"original_pred\": original_top_class,\n",
        "                \"original_prob\": original_top_prob,\n",
        "                \"cornea_pred\": cornea_top_class,\n",
        "                \"cornea_prob\": cornea_top_prob\n",
        "            })\n",
        "\n",
        "            print(f\"Processing image: {cornea_img_name}\")\n",
        "            print(f\"  Original - Top class: {original_top_class}, Probability: {original_top_prob:.4f}\")\n",
        "            print(f\"  Cornea   - Top class: {cornea_top_class}, Probability: {cornea_top_prob:.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    img_size = (640, 640)\n",
        "\n",
        "    folder_path_cornea = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea'\n",
        "    folder_path_original = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/original'\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Processing cornea folder: {folder_path_cornea}\")\n",
        "    print(f\"Processing original folder: {folder_path_original}\")\n",
        "\n",
        "    # Updated class names\n",
        "    classes = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"apac\", \"lens-opacity\", \"bullous\"]\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=img_size, names=classes)\n",
        "\n",
        "    df = process_folders(folder_path_cornea, folder_path_original, model)\n",
        "\n",
        "    print(\"\\nDataFrame:\")\n",
        "    print(df)\n",
        "\n",
        "    csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/comparison_results.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nCSV saved to: {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_SwpatajD3P",
        "outputId": "80b70340-9610-4521-c3fe-02e9f3a43ca8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Processing cornea folder: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/cornea\n",
            "Processing original folder: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/original\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "Processing image: tumor_95.png\n",
            "  Original - Top class: tumor, Probability: 0.8400\n",
            "  Cornea   - Top class: tumor, Probability: 0.8900\n",
            "Processing image: infection_1.png\n",
            "  Original - Top class: infection, Probability: 0.9600\n",
            "  Cornea   - Top class: infection, Probability: 0.6600\n",
            "Processing image: tumor_15.png\n",
            "  Original - Top class: tumor, Probability: 0.6600\n",
            "  Cornea   - Top class: tumor, Probability: 0.8500\n",
            "Processing image: deposit_64.png\n",
            "  Original - Top class: deposit, Probability: 0.9700\n",
            "  Cornea   - Top class: deposit, Probability: 0.9100\n",
            "Processing image: infection_81.png\n",
            "  Original - Top class: infection, Probability: 0.9700\n",
            "  Cornea   - Top class: infection, Probability: 0.7600\n",
            "Processing image: tumor_98.png\n",
            "  Original - Top class: tumor, Probability: 0.9400\n",
            "  Cornea   - Top class: tumor, Probability: 0.8200\n",
            "Processing image: scar_127.png\n",
            "  Original - Top class: scar, Probability: 0.9200\n",
            "  Cornea   - Top class: scar, Probability: 0.8100\n",
            "Processing image: deposit_138.png\n",
            "  Original - Top class: deposit, Probability: 0.9500\n",
            "  Cornea   - Top class: deposit, Probability: 0.9200\n",
            "Processing image: bullous_139.png\n",
            "  Original - Top class: bullous, Probability: 0.9400\n",
            "  Cornea   - Top class: bullous, Probability: 0.7600\n",
            "Processing image: tumor_220.png\n",
            "  Original - Top class: tumor, Probability: 0.9400\n",
            "  Cornea   - Top class: tumor, Probability: 0.9300\n",
            "Processing image: normal_189.png\n",
            "  Original - Top class: normal, Probability: 0.9700\n",
            "  Cornea   - Top class: normal, Probability: 0.6900\n",
            "Processing image: bullous_143.png\n",
            "  Original - Top class: bullous, Probability: 0.9200\n",
            "  Cornea   - Top class: bullous, Probability: 0.7600\n",
            "Processing image: infection_97.png\n",
            "  Original - Top class: infection, Probability: 0.9600\n",
            "  Cornea   - Top class: infection, Probability: 0.2800\n",
            "Processing image: bullous_191.png\n",
            "  Original - Top class: bullous, Probability: 0.9500\n",
            "  Cornea   - Top class: bullous, Probability: 0.7900\n",
            "Processing image: bullous_212.png\n",
            "  Original - Top class: bullous, Probability: 0.9000\n",
            "  Cornea   - Top class: bullous, Probability: 0.8600\n",
            "Processing image: bullous_132.png\n",
            "  Original - Top class: bullous, Probability: 0.9300\n",
            "  Cornea   - Top class: bullous, Probability: 0.8300\n",
            "Processing image: deposit_70.png\n",
            "  Original - Top class: deposit, Probability: 0.9700\n",
            "  Cornea   - Top class: deposit, Probability: 0.9100\n",
            "Processing image: deposit_77.png\n",
            "  Original - Top class: deposit, Probability: 0.9800\n",
            "  Cornea   - Top class: deposit, Probability: 0.9100\n",
            "Processing image: deposit_57.png\n",
            "  Original - Top class: deposit, Probability: 0.9700\n",
            "  Cornea   - Top class: deposit, Probability: 0.7900\n",
            "Processing image: infection_75.png\n",
            "  Original - Top class: infection, Probability: 0.9700\n",
            "  Cornea   - Top class: infection, Probability: 0.8400\n",
            "Processing image: normal_8.png\n",
            "  Original - Top class: normal, Probability: 0.9600\n",
            "  Cornea   - Top class: normal, Probability: 0.7000\n",
            "Processing image: normal_73.png\n",
            "  Original - Top class: normal, Probability: 0.9700\n",
            "  Cornea   - Top class: normal, Probability: 0.7000\n",
            "Processing image: normal_110.png\n",
            "  Original - Top class: normal, Probability: 0.9700\n",
            "  Cornea   - Top class: normal, Probability: 0.7600\n",
            "Processing image: normal_159.png\n",
            "  Original - Top class: normal, Probability: 0.9800\n",
            "  Cornea   - Top class: normal, Probability: 0.9200\n",
            "Processing image: scar_69.png\n",
            "  Original - Top class: scar, Probability: 0.8800\n",
            "  Cornea   - Top class: bullous, Probability: 0.4600\n",
            "Processing image: scar_82.png\n",
            "  Original - Top class: scar, Probability: 0.9400\n",
            "  Cornea   - Top class: scar, Probability: 0.8700\n",
            "Processing image: scar_114.png\n",
            "  Original - Top class: scar, Probability: 0.9400\n",
            "  Cornea   - Top class: scar, Probability: 0.7400\n",
            "Processing image: scar_186.png\n",
            "  Original - Top class: scar, Probability: 0.9000\n",
            "  Cornea   - Top class: scar, Probability: 0.8000\n",
            "Processing image: tumor_183.png\n",
            "  Original - Top class: tumor, Probability: 0.9400\n",
            "  Cornea   - Top class: tumor, Probability: 0.9300\n",
            "Processing image: infection_29.png\n",
            "  Original - Top class: infection, Probability: 0.9300\n",
            "  Cornea   - Top class: bullous, Probability: 0.7000\n",
            "Warning: Corresponding original image not found for apac_5.png\n",
            "Warning: Corresponding original image not found for apac_124.png\n",
            "Warning: Corresponding original image not found for apac_203.png\n",
            "Warning: Corresponding original image not found for apac_221.png\n",
            "Warning: Corresponding original image not found for apac_234.png\n",
            "Processing image: non-infection_99.png\n",
            "  Original - Top class: non-infection, Probability: 0.8300\n",
            "  Cornea   - Top class: normal, Probability: 0.8500\n",
            "Processing image: lens-opacity_196.png\n",
            "  Original - Top class: lens-opacity, Probability: 0.9200\n",
            "  Cornea   - Top class: scar, Probability: 0.3800\n",
            "Processing image: non-infection_210.png\n",
            "  Original - Top class: non-infection, Probability: 0.8000\n",
            "  Cornea   - Top class: non-infection, Probability: 0.7700\n",
            "Processing image: non-infection_223.png\n",
            "  Original - Top class: non-infection, Probability: 0.9000\n",
            "  Cornea   - Top class: non-infection, Probability: 0.7100\n",
            "Processing image: non-infection_128.png\n",
            "  Original - Top class: non-infection, Probability: 0.9200\n",
            "  Cornea   - Top class: scar, Probability: 0.6200\n",
            "Warning: Corresponding original image not found for lens-opacity_85.png\n",
            "Processing image: lens-opacity_43.png\n",
            "  Original - Top class: lens-opacity, Probability: 0.7800\n",
            "  Cornea   - Top class: normal, Probability: 0.6700\n",
            "Processing image: lens-opacity_116.png\n",
            "  Original - Top class: lens-opacity, Probability: 0.7700\n",
            "  Cornea   - Top class: lens-opacity, Probability: 0.5400\n",
            "Processing image: lens-opacity_205.png\n",
            "  Original - Top class: lens-opacity, Probability: 0.9600\n",
            "  Cornea   - Top class: lens-opacity, Probability: 0.8100\n",
            "Processing image: non-infection_195.png\n",
            "  Original - Top class: non-infection, Probability: 0.9100\n",
            "  Cornea   - Top class: No detection, Probability: 0.0000\n",
            "\n",
            "DataFrame:\n",
            "                 basename           image_id    groundtruth  original_pred  \\\n",
            "0            tumor_95.png           tumor_95          tumor          tumor   \n",
            "1         infection_1.png        infection_1      infection      infection   \n",
            "2            tumor_15.png           tumor_15          tumor          tumor   \n",
            "3          deposit_64.png         deposit_64        deposit        deposit   \n",
            "4        infection_81.png       infection_81      infection      infection   \n",
            "5            tumor_98.png           tumor_98          tumor          tumor   \n",
            "6            scar_127.png           scar_127           scar           scar   \n",
            "7         deposit_138.png        deposit_138        deposit        deposit   \n",
            "8         bullous_139.png        bullous_139        bullous        bullous   \n",
            "9           tumor_220.png          tumor_220          tumor          tumor   \n",
            "10         normal_189.png         normal_189         normal         normal   \n",
            "11        bullous_143.png        bullous_143        bullous        bullous   \n",
            "12       infection_97.png       infection_97      infection      infection   \n",
            "13        bullous_191.png        bullous_191        bullous        bullous   \n",
            "14        bullous_212.png        bullous_212        bullous        bullous   \n",
            "15        bullous_132.png        bullous_132        bullous        bullous   \n",
            "16         deposit_70.png         deposit_70        deposit        deposit   \n",
            "17         deposit_77.png         deposit_77        deposit        deposit   \n",
            "18         deposit_57.png         deposit_57        deposit        deposit   \n",
            "19       infection_75.png       infection_75      infection      infection   \n",
            "20           normal_8.png           normal_8         normal         normal   \n",
            "21          normal_73.png          normal_73         normal         normal   \n",
            "22         normal_110.png         normal_110         normal         normal   \n",
            "23         normal_159.png         normal_159         normal         normal   \n",
            "24            scar_69.png            scar_69           scar           scar   \n",
            "25            scar_82.png            scar_82           scar           scar   \n",
            "26           scar_114.png           scar_114           scar           scar   \n",
            "27           scar_186.png           scar_186           scar           scar   \n",
            "28          tumor_183.png          tumor_183          tumor          tumor   \n",
            "29       infection_29.png       infection_29      infection      infection   \n",
            "30   non-infection_99.png   non-infection_99  non-infection  non-infection   \n",
            "31   lens-opacity_196.png   lens-opacity_196   lens-opacity   lens-opacity   \n",
            "32  non-infection_210.png  non-infection_210  non-infection  non-infection   \n",
            "33  non-infection_223.png  non-infection_223  non-infection  non-infection   \n",
            "34  non-infection_128.png  non-infection_128  non-infection  non-infection   \n",
            "35    lens-opacity_43.png    lens-opacity_43   lens-opacity   lens-opacity   \n",
            "36   lens-opacity_116.png   lens-opacity_116   lens-opacity   lens-opacity   \n",
            "37   lens-opacity_205.png   lens-opacity_205   lens-opacity   lens-opacity   \n",
            "38  non-infection_195.png  non-infection_195  non-infection  non-infection   \n",
            "\n",
            "    original_prob    cornea_pred  cornea_prob  \n",
            "0            0.84          tumor         0.89  \n",
            "1            0.96      infection         0.66  \n",
            "2            0.66          tumor         0.85  \n",
            "3            0.97        deposit         0.91  \n",
            "4            0.97      infection         0.76  \n",
            "5            0.94          tumor         0.82  \n",
            "6            0.92           scar         0.81  \n",
            "7            0.95        deposit         0.92  \n",
            "8            0.94        bullous         0.76  \n",
            "9            0.94          tumor         0.93  \n",
            "10           0.97         normal         0.69  \n",
            "11           0.92        bullous         0.76  \n",
            "12           0.96      infection         0.28  \n",
            "13           0.95        bullous         0.79  \n",
            "14           0.90        bullous         0.86  \n",
            "15           0.93        bullous         0.83  \n",
            "16           0.97        deposit         0.91  \n",
            "17           0.98        deposit         0.91  \n",
            "18           0.97        deposit         0.79  \n",
            "19           0.97      infection         0.84  \n",
            "20           0.96         normal         0.70  \n",
            "21           0.97         normal         0.70  \n",
            "22           0.97         normal         0.76  \n",
            "23           0.98         normal         0.92  \n",
            "24           0.88        bullous         0.46  \n",
            "25           0.94           scar         0.87  \n",
            "26           0.94           scar         0.74  \n",
            "27           0.90           scar         0.80  \n",
            "28           0.94          tumor         0.93  \n",
            "29           0.93        bullous         0.70  \n",
            "30           0.83         normal         0.85  \n",
            "31           0.92           scar         0.38  \n",
            "32           0.80  non-infection         0.77  \n",
            "33           0.90  non-infection         0.71  \n",
            "34           0.92           scar         0.62  \n",
            "35           0.78         normal         0.67  \n",
            "36           0.77   lens-opacity         0.54  \n",
            "37           0.96   lens-opacity         0.81  \n",
            "38           0.91   No detection         0.00  \n",
            "\n",
            "CSV saved to: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/comparison_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/comparison_results.csv'\n",
        "# を開く\n",
        "\n",
        "import pandas as pd\n",
        "csv_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/comparison_results.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8j_hww8F1CZl",
        "outputId": "34ce2e35-511e-4e72-82f1-db22c6080ef5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 basename    groundtruth  original_pred  original_prob  \\\n",
              "0            tumor_95.png          tumor          tumor           0.84   \n",
              "1         infection_1.png      infection      infection           0.96   \n",
              "2            tumor_15.png          tumor          tumor           0.66   \n",
              "3          deposit_64.png        deposit        deposit           0.97   \n",
              "4        infection_81.png      infection      infection           0.97   \n",
              "5            tumor_98.png          tumor          tumor           0.94   \n",
              "6    non-infection_99.png  non-infection  non-infection           0.83   \n",
              "7            scar_127.png           scar           scar           0.92   \n",
              "8         deposit_138.png        deposit        deposit           0.95   \n",
              "9         bullous_139.png        bullous        bullous           0.94   \n",
              "10          tumor_220.png          tumor          tumor           0.94   \n",
              "11   lens-opacity_196.png   lens-opacity   lens-opacity           0.92   \n",
              "12         normal_189.png         normal         normal           0.97   \n",
              "13        bullous_143.png        bullous        bullous           0.92   \n",
              "14       infection_97.png      infection      infection           0.96   \n",
              "15        bullous_191.png        bullous        bullous           0.95   \n",
              "16        bullous_212.png        bullous        bullous           0.90   \n",
              "17        bullous_132.png        bullous        bullous           0.93   \n",
              "18         deposit_70.png        deposit        deposit           0.97   \n",
              "19         deposit_77.png        deposit        deposit           0.98   \n",
              "20         deposit_57.png        deposit        deposit           0.97   \n",
              "21       infection_75.png      infection      infection           0.97   \n",
              "22  non-infection_210.png  non-infection  non-infection           0.80   \n",
              "23  non-infection_223.png  non-infection  non-infection           0.90   \n",
              "24  non-infection_128.png  non-infection  non-infection           0.92   \n",
              "25           normal_8.png         normal         normal           0.96   \n",
              "26          normal_73.png         normal         normal           0.97   \n",
              "27         normal_110.png         normal         normal           0.97   \n",
              "28         normal_159.png         normal         normal           0.98   \n",
              "29            scar_69.png           scar           scar           0.88   \n",
              "30            scar_82.png           scar           scar           0.94   \n",
              "31           scar_114.png           scar           scar           0.94   \n",
              "32           scar_186.png           scar           scar           0.90   \n",
              "33          tumor_183.png          tumor          tumor           0.94   \n",
              "34    lens-opacity_43.png   lens-opacity   lens-opacity           0.78   \n",
              "35   lens-opacity_116.png   lens-opacity   lens-opacity           0.77   \n",
              "36   lens-opacity_205.png   lens-opacity   lens-opacity           0.96   \n",
              "37       infection_29.png      infection      infection           0.93   \n",
              "38  non-infection_195.png  non-infection  non-infection           0.91   \n",
              "\n",
              "      cornea_pred  cornea_prob  \n",
              "0           tumor         0.89  \n",
              "1       infection         0.66  \n",
              "2           tumor         0.85  \n",
              "3         deposit         0.91  \n",
              "4       infection         0.76  \n",
              "5           tumor         0.82  \n",
              "6          normal         0.85  \n",
              "7            scar         0.81  \n",
              "8         deposit         0.92  \n",
              "9         bullous         0.76  \n",
              "10          tumor         0.93  \n",
              "11           scar         0.38  \n",
              "12         normal         0.69  \n",
              "13        bullous         0.76  \n",
              "14      infection         0.28  \n",
              "15        bullous         0.79  \n",
              "16        bullous         0.86  \n",
              "17        bullous         0.83  \n",
              "18        deposit         0.91  \n",
              "19        deposit         0.91  \n",
              "20        deposit         0.79  \n",
              "21      infection         0.84  \n",
              "22  non-infection         0.77  \n",
              "23  non-infection         0.71  \n",
              "24           scar         0.62  \n",
              "25         normal         0.70  \n",
              "26         normal         0.70  \n",
              "27         normal         0.76  \n",
              "28         normal         0.92  \n",
              "29        bullous         0.46  \n",
              "30           scar         0.87  \n",
              "31           scar         0.74  \n",
              "32           scar         0.80  \n",
              "33          tumor         0.93  \n",
              "34         normal         0.67  \n",
              "35   lens-opacity         0.54  \n",
              "36   lens-opacity         0.81  \n",
              "37        bullous         0.70  \n",
              "38   No detection         0.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e3af08d-86f8-41ab-a416-6ab27688408a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>basename</th>\n",
              "      <th>groundtruth</th>\n",
              "      <th>original_pred</th>\n",
              "      <th>original_prob</th>\n",
              "      <th>cornea_pred</th>\n",
              "      <th>cornea_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tumor_95.png</td>\n",
              "      <td>tumor</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.84</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>infection_1.png</td>\n",
              "      <td>infection</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.96</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tumor_15.png</td>\n",
              "      <td>tumor</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.66</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>deposit_64.png</td>\n",
              "      <td>deposit</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.97</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>infection_81.png</td>\n",
              "      <td>infection</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.97</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tumor_98.png</td>\n",
              "      <td>tumor</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.94</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>non-infection_99.png</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.83</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>scar_127.png</td>\n",
              "      <td>scar</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.92</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>deposit_138.png</td>\n",
              "      <td>deposit</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.95</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bullous_139.png</td>\n",
              "      <td>bullous</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.94</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>tumor_220.png</td>\n",
              "      <td>tumor</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.94</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>lens-opacity_196.png</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>0.92</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>normal_189.png</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.97</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>bullous_143.png</td>\n",
              "      <td>bullous</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.92</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>infection_97.png</td>\n",
              "      <td>infection</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.96</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>bullous_191.png</td>\n",
              "      <td>bullous</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.95</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>bullous_212.png</td>\n",
              "      <td>bullous</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.90</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>bullous_132.png</td>\n",
              "      <td>bullous</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.93</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>deposit_70.png</td>\n",
              "      <td>deposit</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.97</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>deposit_77.png</td>\n",
              "      <td>deposit</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.98</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>deposit_57.png</td>\n",
              "      <td>deposit</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.97</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>infection_75.png</td>\n",
              "      <td>infection</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.97</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>non-infection_210.png</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.80</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>non-infection_223.png</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.90</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>non-infection_128.png</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.92</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>normal_8.png</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.96</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>normal_73.png</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.97</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>normal_110.png</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.97</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>normal_159.png</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.98</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>scar_69.png</td>\n",
              "      <td>scar</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.88</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>scar_82.png</td>\n",
              "      <td>scar</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.94</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>scar_114.png</td>\n",
              "      <td>scar</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.94</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>scar_186.png</td>\n",
              "      <td>scar</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.90</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>tumor_183.png</td>\n",
              "      <td>tumor</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.94</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>lens-opacity_43.png</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>0.78</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>lens-opacity_116.png</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>0.77</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>lens-opacity_205.png</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>0.96</td>\n",
              "      <td>lens-opacity</td>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>infection_29.png</td>\n",
              "      <td>infection</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.93</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>non-infection_195.png</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.91</td>\n",
              "      <td>No detection</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e3af08d-86f8-41ab-a416-6ab27688408a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9e3af08d-86f8-41ab-a416-6ab27688408a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9e3af08d-86f8-41ab-a416-6ab27688408a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-718d6845-0adf-4d1b-8b8c-bae99032bf61\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-718d6845-0adf-4d1b-8b8c-bae99032bf61')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-718d6845-0adf-4d1b-8b8c-bae99032bf61 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_779b4745-9c9b-4e42-8633-76ea72cac928\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_779b4745-9c9b-4e42-8633-76ea72cac928 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 39,\n  \"fields\": [\n    {\n      \"column\": \"basename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39,\n        \"samples\": [\n          \"tumor_183.png\",\n          \"lens-opacity_205.png\",\n          \"infection_81.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groundtruth\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"infection\",\n          \"bullous\",\n          \"tumor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"infection\",\n          \"bullous\",\n          \"tumor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_prob\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.068705006133078,\n        \"min\": 0.66,\n        \"max\": 0.98,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.84,\n          0.96,\n          0.83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cornea_pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"lens-opacity\",\n          \"infection\",\n          \"bullous\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cornea_prob\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1911295354602113,\n        \"min\": 0.0,\n        \"max\": 0.93,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          0.93,\n          0.86,\n          0.38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zAJWB1Eb1bEl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}