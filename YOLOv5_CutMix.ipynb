{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ5qdBjkM20WlAZJCw2Quf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_CutMix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_CutMix**"
      ],
      "metadata": {
        "id": "ewqwD__x-0fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iZzkoFNN-RlM",
        "outputId": "b233f3f9-feb4-4d82-e003-ca4a0a200135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# prompt: gdriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "os.listdir(folder_path)\n"
      ],
      "metadata": {
        "id": "XoZXOidNATKL",
        "outputId": "29509d3a-1812-4623-df73-7fec5ff84952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['back_scar',\n",
              " 'back_bullous',\n",
              " 'back_infection',\n",
              " 'back_APAC',\n",
              " 'back_deposit',\n",
              " 'back_tumor',\n",
              " 'back_lens-opacity',\n",
              " 'back_non-infection']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**inference YOLOv5**"
      ],
      "metadata": {
        "id": "T4ZqYUcWUWIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXyDmJVDZgpB",
        "outputId": "bf7549c0-8a6d-4867-fd6c-b6e7a8e93b49"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Found existing installation: deep-utils 1.3.33\n",
            "Uninstalling deep-utils-1.3.33:\n",
            "  Successfully uninstalled deep-utils-1.3.33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "fatal: destination path 'yolov5-gradcam' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMj4HMy4aK2K",
        "outputId": "4634a943-27f7-4979-e1b1-f50668278a12"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3917"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/yolov5-gradcam')"
      ],
      "metadata": {
        "id": "QaT7J5-8aWIX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "yDA2QCgoaO5t"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "img_dir = os.path.join(folder_path, os.listdir(folder_path)[0])\n",
        "img_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "qxAxzq_Ya6K9",
        "outputId": "a64803be-b5ed-4b39-85d5-7ffb643c37ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img/back_scar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "# 以下の関数は変更なし\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "# YOLOV5GradCAMクラスを更新\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []  # クラス名を格納するリストを初期化\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        return alpha.view(b, k, 1, 1)\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)\n",
        "        weights = (positive_gradients * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        return eigen_cam.unsqueeze(1)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)"
      ],
      "metadata": {
        "id": "LqyqTc0ibAoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = os.listdir(folder_path)\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2.imwrite(output_path, final_image)\n",
        "        #cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = img_dir\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/content/outputs'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "    #for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ],
      "metadata": {
        "id": "AMp_rx-yjfal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAMなしバージョン**"
      ],
      "metadata": {
        "id": "qM6lGDVP6NcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from deep_utils import split_extension\n",
        "\n",
        "# ... (前のYOLOV5TorchObjectDetectorクラスとpredict_image関数はそのまま) ...\n",
        "\n",
        "def process_folder(folder_path, model):\n",
        "    print(f\"Inspecting folder contents of: {folder_path}\")\n",
        "    all_files = os.listdir(folder_path)\n",
        "    print(f\"Total files in folder: {len(all_files)}\")\n",
        "    print(f\"First 10 files: {all_files[:10]}\")\n",
        "\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')\n",
        "    file_list = [f for f in all_files if f.lower().endswith(image_extensions)]\n",
        "    print(f\"Found {len(file_list)} image files in the folder.\")\n",
        "\n",
        "    if len(file_list) == 0:\n",
        "        print(\"No image files found. Checking all file extensions:\")\n",
        "        for file in all_files:\n",
        "            _, ext = os.path.splitext(file)\n",
        "            print(f\"File: {file}, Extension: {ext}\")\n",
        "\n",
        "    results = []\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "\n",
        "        top_class, top_prob = predict_image(img_path, model)\n",
        "        if top_class is not None and top_prob is not None:\n",
        "            print(f\"Top class: {top_class}, Probability: {top_prob:.4f}\")\n",
        "            results.append((img_basename, top_class, top_prob))\n",
        "        else:\n",
        "            print(f\"Failed to process image: {img_basename}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    model_path = \"/content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "    img_size = 640\n",
        "    names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "    folder_path = '/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img'\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    try:\n",
        "        model = YOLOV5TorchObjectDetector(model_path, device, img_size=(img_size, img_size), names=names)\n",
        "        results = process_folder(folder_path, model)\n",
        "\n",
        "        # Print summary of results\n",
        "        print(\"\\nPrediction Results Summary:\")\n",
        "        if results:\n",
        "            for img_name, top_class, top_prob in results:\n",
        "                print(f\"{img_name}: {top_class} ({top_prob:.4f})\")\n",
        "        else:\n",
        "            print(\"No valid predictions were made.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "mgsTvlCHlEdC",
        "outputId": "9ea79cd6-f398-4dca-ce92-ace1256edf89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Processing folder: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img\n",
            "Model loaded successfully from /content/drive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\n",
            "Class names: ['infection', 'normal', 'non-infection', 'scar', 'tumor', 'deposit', 'APAC', 'lens opacity', 'bullous']\n",
            "Inspecting folder contents of: /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_CutMix/CutMix_images/Cutmix_mixed_img\n",
            "Total files in folder: 8\n",
            "First 10 files: ['back_scar', 'back_bullous', 'back_infection', 'back_APAC', 'back_deposit', 'back_tumor', 'back_lens-opacity', 'back_non-infection']\n",
            "Found 0 image files in the folder.\n",
            "No image files found. Checking all file extensions:\n",
            "File: back_scar, Extension: \n",
            "File: back_bullous, Extension: \n",
            "File: back_infection, Extension: \n",
            "File: back_APAC, Extension: \n",
            "File: back_deposit, Extension: \n",
            "File: back_tumor, Extension: \n",
            "File: back_lens-opacity, Extension: \n",
            "File: back_non-infection, Extension: \n",
            "\n",
            "Prediction Results Summary:\n",
            "No valid predictions were made.\n"
          ]
        }
      ]
    }
  ]
}