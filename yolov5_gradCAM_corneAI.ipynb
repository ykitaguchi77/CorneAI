{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNS7WCNF9YdYwHe8YxCfbNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/yolov5_gradCAM_corneAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d42M6k9QpvSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_GradCAM_CorneAI**\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "id": "2cakhs2BZLRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "ytBOWsuXZQzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7oJE1rLrE_4",
        "outputId": "c26718d2-5309-4122-ff23-f2cbc7a6828e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p48tU-_wYVHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1c193c-a6ae-4735-da78-eca73ddf6c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 8.76 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ],
      "metadata": {
        "id": "wkvc2KijYes5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python main.py --model-path $model_path --img-path $img_path --output-dir out"
      ],
      "metadata": {
        "id": "cjgRkuSuYinZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Top3 analysis**"
      ],
      "metadata": {
        "id": "wvBLe9pbpCwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit\"\n",
        "print(len(os.listdir(img_dir)))"
      ],
      "metadata": {
        "id": "GWizQkPAjczd",
        "outputId": "4b491509-f628-447c-f61c-fd18f4e8e736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN98dpHRuyNV",
        "outputId": "6a174612-c5a4-470e-a7ec-577534a8946b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rMIlXO_mIun8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "#img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "#output_dir = 'out'  # 出力ディレクトリ\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "img_size = 640  # 入力画像サイズ\n",
        "target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_23_cv3_act'  # デフォルト\n",
        "#target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # Set the font size and other parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    # Calculate the text size and position\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10  # Adjust the horizontal position to place the text near the left edge\n",
        "    text_y = text_size[1] + 10  # Adjust the vertical position to place the text near the top edge\n",
        "\n",
        "    # Put the text on the image\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "        # print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        tic = time.time()\n",
        "        preds, logits = self.model(input_img)\n",
        "        # print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "        else:\n",
        "            self.cls_names = []  # top3_indicesが空の場合は空のリストを代入\n",
        "\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "            self.model.zero_grad()\n",
        "            tic = time.time()\n",
        "            score.backward(retain_graph=True)\n",
        "            # print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            weights = alpha.view(b, k, 1, 1)\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path):\n",
        "   input_size = (img_size, img_size)\n",
        "   img = cv2.imread(img_path)\n",
        "  #  print('[INFO] Loading the model')\n",
        "   model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                     names=names)\n",
        "   torch_img = model.preprocessing(img[..., ::-1])\n",
        "   if method == 'gradcam':\n",
        "       saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "   tic = time.time()\n",
        "   masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "   print(\"total time:\", round(time.time() - tic, 4))\n",
        "   result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "   result = result[..., ::-1]  # convert to bgr\n",
        "   images = [result]\n",
        "\n",
        "   for i in range(len(masks)):\n",
        "       res_img = result.copy()\n",
        "       for j, mask in enumerate(masks[i]):\n",
        "           bbox = boxes[0][j]\n",
        "           res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "           res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "       images.append(res_img)\n",
        "\n",
        "   final_image = concat_images(images)\n",
        "   img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "   output_path = f'{output_dir}/{img_name}'\n",
        "   os.makedirs(output_dir, exist_ok=True)\n",
        "   print(f'[INFO] Saving the final image at {output_path}')\n",
        "   #cv2.imwrite(output_path, final_image)\n",
        "   cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #################################################################\n",
        "    # ファイル名を数字でソート\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[6:10]\n",
        "    #################################################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        if method == 'gradcam':\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        #cv2.imwrite(output_path, final_image)\n",
        "        cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "\n",
        "        # オブジェクトを明示的に削除\n",
        "        del saliency_method\n",
        "        del masks\n",
        "        del logits\n",
        "        del boxes\n",
        "        del images\n",
        "        del final_image\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)"
      ],
      "metadata": {
        "id": "6hBb7eKP986c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.exit()"
      ],
      "metadata": {
        "id": "SFDwQ_bfIYaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LayerごとTop1の可視化**"
      ],
      "metadata": {
        "id": "MDjAHX8v7tS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_size = 640  # 入力画像サイズ\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # Set the font size and other parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    # Calculate the text size and position\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10  # Adjust the horizontal position to place the text near the left edge\n",
        "    text_y = text_size[1] + 10  # Adjust the vertical position to place the text near the top edge\n",
        "\n",
        "    # Put the text on the image\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "        # print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        tic = time.time()\n",
        "        preds, logits = self.model(input_img)\n",
        "        # print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "        else:\n",
        "            self.cls_names = []  # top3_indicesが空の場合は空のリストを代入\n",
        "\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "            self.model.zero_grad()\n",
        "            tic = time.time()\n",
        "            score.backward(retain_graph=True)\n",
        "            # print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            weights = alpha.view(b, k, 1, 1)\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                      names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[0], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    #os.makedirs(output_dir, exist_ok=True)\n",
        "    #print(f'[INFO] Saving the final image at {output_path}')\n",
        "    #cv2.imwrite(output_path, final_image)\n",
        "    cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #################################################################\n",
        "    # ファイル名を数字でソート\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[0:5]\n",
        "    #################################################################\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            print(f\"boxes: {boxes}\")\n",
        "\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, cls_names[0], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "        #cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "        ## 画像を保存comment out if not save\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2.imwrite(output_path, final_image)\n",
        "        ##\n",
        "\n",
        "        # オブジェクトを明示的に削除\n",
        "        del saliency_method\n",
        "        del masks\n",
        "        del logits\n",
        "        del boxes\n",
        "        del images\n",
        "        del final_image\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit_layers'  # 出力ディレクトリ\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)"
      ],
      "metadata": {
        "id": "4xA9qfVzajFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa94451c-86fe-4af6-f63f-975db0175f90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "Processing image 1: 1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[168, 83, 513, 470]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[168, 83, 513, 470]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-02d42864930d>:236: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[168, 83, 513, 470]]]\n",
            "[INFO] Saving the final image at /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit_layers/1-res.jpg\n",
            "Processing image 2: 2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[143, 135, 399, 456]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[143, 135, 399, 456]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-02d42864930d>:236: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[143, 135, 399, 456]]]\n",
            "[INFO] Saving the final image at /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit_layers/2-res.jpg\n",
            "Processing image 3: 3.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[219, 197, 456, 495]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[219, 197, 456, 495]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-02d42864930d>:236: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[219, 197, 456, 495]]]\n",
            "[INFO] Saving the final image at /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit_layers/3-res.jpg\n",
            "Processing image 4: 4.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[115, 88, 454, 522]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[115, 88, 454, 522]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-02d42864930d>:236: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[115, 88, 454, 522]]]\n",
            "[INFO] Saving the final image at /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit_layers/4-res.jpg\n",
            "Processing image 5: 5.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[205, 159, 381, 366]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[205, 159, 381, 366]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-02d42864930d>:236: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[205, 159, 381, 366]]]\n",
            "[INFO] Saving the final image at /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit_layers/5-res.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sort GradCam Images**\n",
        "\n",
        "GradCAM画像をクラスごとにフォルダ分け"
      ],
      "metadata": {
        "id": "YHLMn6kZIlSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara234.csv\"\n",
        "\n",
        "# CSVファイルを読み込み\n",
        "df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
        "\n",
        "# データフレームの最初の数行を表示\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZaYBsjL7NTcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dirs = [\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit',\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_sumaho'\n",
        "]\n",
        "destination_base_dirs = [\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_slit_sorted',\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_sumaho_sorted'\n",
        "]\n",
        "\n",
        "# Iterate over the source and destination directory pairs\n",
        "for source_dir, destination_base_dir in zip(source_dirs, destination_base_dirs):\n",
        "    print(f'Processing files from {source_dir} to {destination_base_dir}')\n",
        "    # Iterate over the dataframe and copy files to the appropriate class folder\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        serial_number = row['serial_number']\n",
        "        class_num = row['class_num']\n",
        "        source_file = os.path.join(source_dir, f'{serial_number}-res.jpg')\n",
        "        destination_dir = os.path.join(destination_base_dir, str(class_num))\n",
        "\n",
        "        # Create the destination directory if it does not exist\n",
        "        os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "        # Copy the file to the destination directory\n",
        "        destination_file = os.path.join(destination_dir, f'{serial_number}-res.jpg')\n",
        "        if os.path.exists(source_file):\n",
        "            shutil.copy(source_file, destination_file)\n",
        "        else:\n",
        "            print(f'File {source_file} does not exist.')\n",
        "\n",
        "print('Files have been copied successfully.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBll2vlDM26D",
        "outputId": "f238c4e9-9e95-44f6-cffc-3b90ec61e326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Processing files from /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit to /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_slit_sorted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 228/228 [00:02<00:00, 88.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing files from /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_sumaho to /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_sumaho_sorted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 228/228 [00:02<00:00, 89.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been copied successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Compare area of interest**\n",
        "\n",
        "注目度n%以上の画像を"
      ],
      "metadata": {
        "id": "dxtwlXulSgO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ニ値化して表示**"
      ],
      "metadata": {
        "id": "2lQctr8O90Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "6iDm_dPKM28s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "#img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "#output_dir = 'out'  # 出力ディレクトリ\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "img_size = 640  # 入力画像サイズ\n",
        "target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_res_img(bbox, masks, res_img, threshold):\n",
        "    total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        mask[mask < threshold*255] = 0  # 追加: 128未満の値を0にする\n",
        "        mask[mask >= threshold*255] = 255  # 追加: 128未満の値を0にする\n",
        "        binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "\n",
        "        # bboxの範囲内のマスクの部分を取得\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "        # 閾値を超える共通部分のピクセル数をカウント\n",
        "        intersect_pixels = np.sum(mask_bbox)\n",
        "        total_intersect_pixels += intersect_pixels\n",
        "\n",
        "        # mask_bbox のピクセル数を取得\n",
        "        mask_bbox_area = mask_bbox.size\n",
        "\n",
        "        # AOI を計算\n",
        "        AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "        print(f\"Area of Interest (AOI): {AOI}\")\n",
        "\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # Set the font size and other parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    # Calculate the text size and position\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10  # Adjust the horizontal position to place the text near the left edge\n",
        "    text_y = text_size[1] + 10  # Adjust the vertical position to place the text near the top edge\n",
        "\n",
        "    # Put the text on the image\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "        # print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        tic = time.time()\n",
        "        preds, logits = self.model(input_img)\n",
        "        # print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "        else:\n",
        "            self.cls_names = []  # top3_indicesが空の場合は空のリストを代入\n",
        "\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "            self.model.zero_grad()\n",
        "            tic = time.time()\n",
        "            score.backward(retain_graph=True)\n",
        "            # print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            weights = alpha.view(b, k, 1, 1)\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                      names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    if len(masks) > 0:\n",
        "        mask = masks[0][0]  # top1のマスクのみ使用\n",
        "        bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "        cls_name = cls_names[0]  # top1のクラス名のみ使用\n",
        "        res_img = result.copy()\n",
        "        res_img, _ = get_res_img(bbox, [mask], res_img, GradCAM_threshold)\n",
        "        res_img = put_text_box(bbox, cls_name, res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    ##############################################################################################\n",
        "    ######## ファイル名を数字でソート#############################################################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[0:5]\n",
        "    ##############################################################################################\n",
        "    ##############################################################################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        if method == 'gradcam':\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        if len(masks) > 0:\n",
        "            mask = masks[0][0]  # top1のマスクのみ使用\n",
        "            bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "            cls_name = cls_names[0]  # top1のクラス名のみ使用\n",
        "            res_img = result.copy()\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img, GradCAM_threshold)\n",
        "            res_img = put_text_box(bbox, cls_name, res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "        # オブジェクトを明示的に削除\n",
        "        del saliency_method\n",
        "        del masks\n",
        "        del logits\n",
        "        del boxes\n",
        "        del images\n",
        "        del final_image\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "\n",
        "    GradCAM_threshold = 0.5 #GradCAMの閾値設定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)"
      ],
      "metadata": {
        "id": "HV90u_Z9Urn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Area of interestの計算**\n",
        "\n",
        "結果をcsvに保存する"
      ],
      "metadata": {
        "id": "iEUdHWgh79uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルをDataFrameとして読み込む\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 'Unnamed'が含まれる列を削除する\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# 例としてthresholdの値を設定\n",
        "threshold = 0.5\n",
        "\n",
        "# 新しい列 'AOI_{threshold}' を作成\n",
        "df[f'AOI_{threshold}'] = None  # 初期値を設定\n",
        "\n",
        "# 修正後のDataFrameを表示する\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "Up7ykhgI5l3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                      in_source=Box.BoxSource.Torch,\n",
        "                                      to_source=Box.BoxSource.Numpy,\n",
        "                                      return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "wj6fl4uVCCji"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS3iA_8QKWZC",
        "outputId": "c43c0f4c-00c0-4a78-d6fa-78f6b99f5034"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #################\n",
        "# # 最終（23）layerのみの解析\n",
        "# #################\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# from deep_utils import Box, split_extension\n",
        "# import os\n",
        "# from tqdm import tqdm\n",
        "# #from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# # from models.gradcam import YOLOV5GradCAM\n",
        "# import time\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import gc\n",
        "\n",
        "# class YOLOV5GradCAM:\n",
        "#     def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "#         self.model = model\n",
        "#         self.gradients = dict()\n",
        "#         self.activations = dict()\n",
        "#         self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "#         def backward_hook(module, grad_input, grad_output):\n",
        "#             self.gradients['value'] = grad_output[0]\n",
        "#             return None\n",
        "\n",
        "#         def forward_hook(module, input, output):\n",
        "#             self.activations['value'] = output\n",
        "#             return None\n",
        "\n",
        "#         target_layer = find_yolo_layer(self.model, layer_name)\n",
        "#         target_layer.register_forward_hook(forward_hook)\n",
        "#         target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "#         device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "#         self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "#         # print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "#     def forward(self, input_img, class_idx=True):\n",
        "#         saliency_maps = []\n",
        "#         b, c, h, w = input_img.size()\n",
        "#         tic = time.time()\n",
        "#         preds, logits = self.model(input_img)\n",
        "#         # print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "\n",
        "#         _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "#         if top3_indices.numel() > 0:\n",
        "#             preds[1][0] = top3_indices.tolist()[0]\n",
        "#             preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "#             self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "#         else:\n",
        "#             self.cls_names = []  # top3_indicesが空の場合は空のリストを代入\n",
        "\n",
        "#         for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "#             if class_idx:\n",
        "#                 score = logits[0][0][cls]\n",
        "#             else:\n",
        "#                 score = logits[0][0].max()\n",
        "#             self.model.zero_grad()\n",
        "#             tic = time.time()\n",
        "#             score.backward(retain_graph=True)\n",
        "#             # print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "#             gradients = self.gradients['value']\n",
        "#             activations = self.activations['value']\n",
        "#             b, k, u, v = gradients.size()\n",
        "#             alpha = gradients.view(b, k, -1).mean(2)\n",
        "#             weights = alpha.view(b, k, 1, 1)\n",
        "#             saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "#             saliency_map = F.relu(saliency_map)\n",
        "#             saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "#             saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "#             saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "#             saliency_maps.append(saliency_map)\n",
        "\n",
        "#         return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "\n",
        "#     def __call__(self, input_img):\n",
        "#         return self.forward(input_img)\n",
        "\n",
        "# def find_yolo_layer(model, layer_name):\n",
        "#     \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "#     Args:\n",
        "#         model: yolov5 model.\n",
        "#         layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "#     Return:\n",
        "#         target_layer: found layer\n",
        "#     \"\"\"\n",
        "#     hierarchy = layer_name.split('_')\n",
        "#     target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "#     for h in hierarchy[1:]:\n",
        "#         target_layer = target_layer._modules[h]\n",
        "#     return target_layer\n",
        "\n",
        "# def get_aoi(bbox, masks, threshold):\n",
        "#     total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "#     for mask in masks:\n",
        "#         # マスクの前処理\n",
        "#         mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "#         mask[mask < threshold * 255] = 0\n",
        "#         mask[mask >= threshold * 255] = 255\n",
        "#         binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "#         # bboxの範囲内のマスク部分を取得\n",
        "#         x1, y1, x2, y2 = bbox\n",
        "#         mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "#         # 閾値を超える共通部分のピクセル数をカウント\n",
        "#         intersect_pixels = np.sum(mask_bbox)\n",
        "#         total_intersect_pixels += intersect_pixels\n",
        "\n",
        "#         # mask_bbox のピクセル数を取得\n",
        "#         mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "#         # AOI を計算\n",
        "#         AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "#     # if len(bbox) == 0 or len(masks) == 0:\n",
        "#     #     return 0.0\n",
        "\n",
        "#     # for mask in masks:\n",
        "#     #     mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "#     #     mask[mask < threshold*255] = 0\n",
        "#     #     mask[mask >= threshold*255] = 255\n",
        "#     #     heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "#     #     n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "#     #     aoi = np.sum(n_heatmat > 0) / (n_heatmat.shape[0] * n_heatmat.shape[1])\n",
        "#     #     print(f\"Area of Interest (original_AOI): {aoi}\")\n",
        "\n",
        "#     return AOI\n",
        "\n",
        "# def calculate_aoi(folder_path, csv_path, threshold, model, saliency_method, start_index=0, end_index=None):\n",
        "#     df = pd.read_csv(csv_path)\n",
        "#     df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "#     if f'AOI_{threshold}' not in df.columns:\n",
        "#         df[f'AOI_{threshold}'] = None\n",
        "\n",
        "#     if end_index is None:\n",
        "#         end_index = len(df)\n",
        "\n",
        "#     for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "#         if index < start_index or index >= end_index:\n",
        "#             continue\n",
        "\n",
        "#         serial_number = row['serial_number']\n",
        "\n",
        "#         if pd.isna(serial_number):\n",
        "#             print(f\"Skipping row {index} due to NaN serial_number\")\n",
        "#             continue\n",
        "\n",
        "#         img_name = f\"{int(serial_number)}.jpg\"\n",
        "#         img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "#         # Rest of the code remains the same\n",
        "#         if os.path.exists(img_path):\n",
        "#             img = cv2.imread(img_path)\n",
        "#             torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "#             try:\n",
        "#                 masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "#             except ValueError as e:\n",
        "#                 print(f\"Error processing image {img_name}: {str(e)}\")\n",
        "#                 continue\n",
        "\n",
        "#             if len(masks) > 0 and len(boxes) > 0:\n",
        "#                 mask = masks[0][0]  # top1のマスクのみ使用\n",
        "#                 bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "\n",
        "#                 aoi = get_aoi(bbox, [mask], threshold)\n",
        "#                 df.at[index, f'AOI_{threshold}'] = aoi\n",
        "\n",
        "#                 print(f\"Image: {img_name}, AOI: {aoi}\")\n",
        "#         else:\n",
        "#             print(f\"Image not found: {img_name}\")\n",
        "\n",
        "#     df.to_csv(csv_path, index=False)\n",
        "\n",
        "# # 使用例\n",
        "# folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "# csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv\"\n",
        "# #folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "# #csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "# threshold = 0.5\n",
        "# start_index = 51 # 開始するインデックスを指定\n",
        "# end_index = 100 # 終了するインデックスを指定\n",
        "\n",
        "# # モデルとsaliency_methodの定義\n",
        "# model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "# device = 'cpu'\n",
        "# input_size = (640, 640)\n",
        "# names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "# model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "# target_layer = 'model_23_cv3_conv'\n",
        "# saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "\n",
        "# calculate_aoi(folder_path, csv_path, threshold, model, saliency_method, start_index, end_index)"
      ],
      "metadata": {
        "id": "7rq_tvQf7zfW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxS9jH2fO59C",
        "outputId": "2ddf344f-a355-40fa-9943-a7617a7eb8cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "# 3-layerで解析\n",
        "########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "   def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "       self.model = model\n",
        "       self.gradients = dict()\n",
        "       self.activations = dict()\n",
        "       self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "       def backward_hook(module, grad_input, grad_output):\n",
        "           self.gradients['value'] = grad_output[0]\n",
        "           return None\n",
        "\n",
        "       def forward_hook(module, input, output):\n",
        "           self.activations['value'] = output\n",
        "           return None\n",
        "\n",
        "       target_layer = find_yolo_layer(self.model, layer_name)\n",
        "       target_layer.register_forward_hook(forward_hook)\n",
        "       target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "       device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "       self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "       # print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "   def forward(self, input_img, class_idx=True):\n",
        "       saliency_maps = []\n",
        "       b, c, h, w = input_img.size()\n",
        "       tic = time.time()\n",
        "       preds, logits = self.model(input_img)\n",
        "       # print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "\n",
        "       _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "       if top3_indices.numel() > 0:\n",
        "           preds[1][0] = top3_indices.tolist()[0]\n",
        "           preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "           self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "       else:\n",
        "           self.cls_names = []  # top3_indicesが空の場合は空のリストを代入\n",
        "\n",
        "       for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "           if class_idx:\n",
        "               score = logits[0][0][cls]\n",
        "           else:\n",
        "               score = logits[0][0].max()\n",
        "           self.model.zero_grad()\n",
        "           tic = time.time()\n",
        "           score.backward(retain_graph=True)\n",
        "           # print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "           gradients = self.gradients['value']\n",
        "           activations = self.activations['value']\n",
        "           b, k, u, v = gradients.size()\n",
        "           alpha = gradients.view(b, k, -1).mean(2)\n",
        "           weights = alpha.view(b, k, 1, 1)\n",
        "           saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "           saliency_map = F.relu(saliency_map)\n",
        "           saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "           saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "           saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "           saliency_maps.append(saliency_map)\n",
        "\n",
        "       return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "\n",
        "   def __call__(self, input_img):\n",
        "       return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "   for mask in masks:\n",
        "       # マスクの前処理\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "       # bboxの範囲内のマスク部分を取得\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # 閾値を超える共通部分のピクセル数をカウント\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox のピクセル数を取得\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI を計算\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   # if len(bbox) == 0 or len(masks) == 0:\n",
        "   #     return 0.0\n",
        "\n",
        "   # for mask in masks:\n",
        "   #     mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "   #     mask[mask < threshold*255] = 0\n",
        "   #     mask[mask >= threshold*255] = 255\n",
        "   #     heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "   #     n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "   #     aoi = np.sum(n_heatmat > 0) / (n_heatmat.shape[0] * n_heatmat.shape[1])\n",
        "   #     print(f\"Area of Interest (original_AOI): {aoi}\")\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "   df = pd.read_csv(csv_path)\n",
        "   df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "   for layer_num in [17, 20, 23]:\n",
        "       if f'AOI_{threshold}_layer{layer_num}' not in df.columns:\n",
        "           df[f'AOI_{threshold}_layer{layer_num}'] = None\n",
        "\n",
        "   if end_index is None:\n",
        "       end_index = len(df)\n",
        "\n",
        "   for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "       if index < start_index or index >= end_index:\n",
        "           continue\n",
        "\n",
        "       serial_number = row['serial_number']\n",
        "\n",
        "       if pd.isna(serial_number):\n",
        "           print(f\"Skipping row {index} due to NaN serial_number\")\n",
        "           continue\n",
        "\n",
        "       img_name = f\"{int(serial_number)}.jpg\"\n",
        "       img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "       # Rest of the code remains the same\n",
        "       if os.path.exists(img_path):\n",
        "           img = cv2.imread(img_path)\n",
        "           torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "           for layer_num, saliency_method in zip([17, 20, 23], saliency_methods):\n",
        "               try:\n",
        "                   masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "               except ValueError as e:\n",
        "                   print(f\"Error processing image {img_name}: {str(e)}\")\n",
        "                   continue\n",
        "\n",
        "               if len(masks) > 0 and len(boxes) > 0:\n",
        "                   mask = masks[0][0]  # top1のマスクのみ使用\n",
        "                   bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "\n",
        "                   aoi = get_aoi(bbox, [mask], threshold)\n",
        "                   df.at[index, f'AOI_{threshold}_layer{layer_num}'] = aoi\n",
        "\n",
        "                   print(f\"Image: {img_name}, Layer: {layer_num}, AOI: {aoi}\")\n",
        "       else:\n",
        "           print(f\"Image not found: {img_name}\")\n",
        "\n",
        "   df.to_csv(csv_path, index=False)\n",
        "\n",
        "# 使用例\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv\"\n",
        "#folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "#csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "threshold = 0.5\n",
        "start_index =17 # 開始するインデックスを指定\n",
        "end_index = 19 # 終了するインデックスを指定\n",
        "\n",
        "# モデルとsaliency_methodの定義\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cpu'\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "saliency_methods = [YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size) for layer in target_layers]\n",
        "\n",
        "calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)"
      ],
      "metadata": {
        "id": "OrjCYpUtJlyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LE7ZYlphCpBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show_result\n",
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# DataFrameとしてCSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# DataFrameの最初の数行を表示する\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "gVNlV1lh_rRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**注目点色塗り**"
      ],
      "metadata": {
        "id": "fmGNyvTdM3c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#注目点に色を塗る（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            color = np.array([0,0,0], dtype=np.uint8) #マスクの色：白は[0,0,0]\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + (1 - binary_mask[..., np.newaxis]) * color\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + (1 - black_mask[..., np.newaxis]) * color\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/8.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.1  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ],
      "metadata": {
        "id": "sx9KdyNZNzz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 注目点をblurする（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            # ぼかし効果を適用\n",
        "            blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + blurred_result * (1 - binary_mask[..., np.newaxis])\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + black_blurred_result * (1 - black_mask[..., np.newaxis])\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/41.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.3  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ],
      "metadata": {
        "id": "VEn_KHC-T8C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Analyze results**"
      ],
      "metadata": {
        "id": "69WoakWi-ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the CSV files\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# Create confusion matrices for Slit and Sumaho datasets\n",
        "conf_matrix_slit = pd.crosstab(df_slit['class_num'], df_slit['top1'])\n",
        "conf_matrix_sumaho = pd.crosstab(df_sumaho['class_num'], df_sumaho['top1'])\n",
        "\n",
        "# Rename the index and columns for better readability\n",
        "conf_matrix_slit.index = class_names\n",
        "conf_matrix_slit.columns = class_names\n",
        "conf_matrix_sumaho.index = class_names\n",
        "conf_matrix_sumaho.columns = class_names\n",
        "\n",
        "# Plotting confusion matrices using seaborn\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Slit Lamp Data Confusion Matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_slit, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Slit Lamp Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "\n",
        "# Smartphone Data Confusion Matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_sumaho, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Smartphone Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h03rgBUoWi_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for Slit and Sumaho datasets\n",
        "accuracy_slit = (conf_matrix_slit.values.diagonal().sum() / conf_matrix_slit.values.sum()) * 100\n",
        "accuracy_sumaho = (conf_matrix_sumaho.values.diagonal().sum() / conf_matrix_sumaho.values.sum()) * 100\n",
        "\n",
        "accuracy_slit, accuracy_sumaho\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTt2gTWAAD3",
        "outputId": "52262bea-83f1-4f9a-bdd2-a1ba296355a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.08771929824562, 77.63157894736842)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# AOI_0.5の平均と標準偏差を計算\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')['AOI_0.5'].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')['AOI_0.5'].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuQdE9zzPm68",
        "outputId": "3b438b81-3a8b-404d-ff38-df695b7ece3c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ± SD AOI for Slit Lamp Data\n",
            "                   mean       std\n",
            "infection      0.086643  0.037705\n",
            "normal         0.050446  0.016585\n",
            "non-infection  0.271836  0.145000\n",
            "scar           0.205724  0.104553\n",
            "tumor          0.195092  0.097643\n",
            "deposit        0.072846  0.041480\n",
            "APAC           0.068622  0.036099\n",
            "lens opacity   0.137005  0.033883\n",
            "bullous        0.126359  0.041344\n",
            "\n",
            "Mean ± SD AOI for Smartphone Data\n",
            "                   mean       std\n",
            "infection      0.145351  0.055593\n",
            "normal         0.114119  0.097931\n",
            "non-infection  0.421066  0.225339\n",
            "scar           0.358542  0.173204\n",
            "tumor          0.247933  0.077559\n",
            "deposit        0.172425  0.121422\n",
            "APAC           0.108903  0.041468\n",
            "lens opacity   0.241247  0.129700\n",
            "bullous        0.191174  0.048210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23']\n",
        "\n",
        "# AOIの平均と標準偏差を計算\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ],
      "metadata": {
        "id": "7v9DUXYHkJ5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Layerごとに解析**"
      ],
      "metadata": {
        "id": "Z2rBLnp6lbjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "####################\n",
        "layer = 17\n",
        "#17, 20, 23\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形してSeabornでプロットできるようにする\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# グラフのサイズを設定\n",
        "plt.figure(figsize=(18, 8))\n",
        "\n",
        "# Seabornのboxplotを使用してプロット\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined)\n",
        "\n",
        "# ラベルとタイトルを設定\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel(layer_name)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp and Smartphone Data')\n",
        "\n",
        "# グラフを表示\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "jkZ4ZbaoUEMX",
        "outputId": "3a8e8395-185d-4573-bae3-81bae69917ef"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 各クラスごとに対応のあるt検定を行う\n",
        "results = []\n",
        "for class_name in class_names:\n",
        "    slit_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "    sumaho_data = df_sumaho[df_sumaho['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "    # 対応のあるデータを取るため、最小の長さに合わせる\n",
        "    min_length = min(len(slit_data), len(sumaho_data))\n",
        "    slit_data = slit_data[:min_length]\n",
        "    sumaho_data = sumaho_data[:min_length]\n",
        "\n",
        "    t_stat, p_value = stats.ttest_rel(slit_data, sumaho_data)\n",
        "\n",
        "    # スリットランプデータの統計値\n",
        "    slit_mean = slit_data.mean()\n",
        "    slit_std = slit_data.std()\n",
        "\n",
        "    # スマートフォンデータの統計値\n",
        "    sumaho_mean = sumaho_data.mean()\n",
        "    sumaho_std = sumaho_data.std()\n",
        "\n",
        "    results.append({\n",
        "        'class_name': class_name,\n",
        "        't_stat': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'slit_mean': slit_mean,\n",
        "        'slit_std': slit_std,\n",
        "        'sumaho_mean': sumaho_mean,\n",
        "        'sumaho_std': sumaho_std\n",
        "    })\n",
        "\n",
        "# 全クラスまとめた対応のあるt検定\n",
        "all_slit_data = df_slit[layer_name].dropna().reset_index(drop=True)\n",
        "all_sumaho_data = df_sumaho[layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "# 対応のあるデータを取るため、最小の長さに合わせる\n",
        "min_length_all = min(len(all_slit_data), len(all_sumaho_data))\n",
        "all_slit_data = all_slit_data[:min_length_all]\n",
        "all_sumaho_data = all_sumaho_data[:min_length_all]\n",
        "\n",
        "t_stat_all, p_value_all = stats.ttest_rel(all_slit_data, all_sumaho_data)\n",
        "\n",
        "# 全クラスまとめた統計値\n",
        "all_slit_mean = all_slit_data.mean()\n",
        "all_slit_std = all_slit_data.std()\n",
        "all_sumaho_mean = all_sumaho_data.mean()\n",
        "all_sumaho_std = all_sumaho_data.std()\n",
        "\n",
        "# 結果を追加\n",
        "results.append({\n",
        "    'class_name': 'All Classes',\n",
        "    't_stat': t_stat_all,\n",
        "    'p_value': p_value_all,\n",
        "    'slit_mean': all_slit_mean,\n",
        "    'slit_std': all_slit_std,\n",
        "    'sumaho_mean': all_sumaho_mean,\n",
        "    'sumaho_std': all_sumaho_std\n",
        "})\n",
        "\n",
        "# 結果をデータフレームに変換して表示\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results['p_value'] = df_results['p_value'].map(lambda x: f'{x:.3f}')\n",
        "\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "fgFUt3AFWwvA",
        "outputId": "80a47dcc-d59a-454d-eda6-dea247192399"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      class_name    t_stat p_value  slit_mean  slit_std  sumaho_mean  \\\n",
              "0      infection  0.538977   0.594   0.084249  0.067905     0.074083   \n",
              "1         normal  4.106644   0.000   0.181992  0.127307     0.073866   \n",
              "2  non-infection -0.342441   0.738   0.138709  0.141636     0.169776   \n",
              "3           scar  3.530070   0.001   0.164444  0.123306     0.082617   \n",
              "4          tumor -3.784626   0.001   0.140423  0.102705     0.287111   \n",
              "5        deposit  0.776070   0.444   0.176102  0.104585     0.154578   \n",
              "6           APAC -0.077923   0.941   0.142654  0.159529     0.148437   \n",
              "7   lens opacity  0.898269   0.382   0.114988  0.097340     0.084550   \n",
              "8        bullous -1.065212   0.299   0.129807  0.067976     0.163363   \n",
              "9    All Classes  0.563571   0.574   0.143497  0.109576     0.136555   \n",
              "\n",
              "   sumaho_std  \n",
              "0    0.089991  \n",
              "1    0.110189  \n",
              "2    0.242774  \n",
              "3    0.081827  \n",
              "4    0.175874  \n",
              "5    0.121643  \n",
              "6    0.114314  \n",
              "7    0.125398  \n",
              "8    0.145041  \n",
              "9    0.146311  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a0fd037-2798-4b2d-ac59-d6d382f2477d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class_name</th>\n",
              "      <th>t_stat</th>\n",
              "      <th>p_value</th>\n",
              "      <th>slit_mean</th>\n",
              "      <th>slit_std</th>\n",
              "      <th>sumaho_mean</th>\n",
              "      <th>sumaho_std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>infection</td>\n",
              "      <td>0.538977</td>\n",
              "      <td>0.594</td>\n",
              "      <td>0.084249</td>\n",
              "      <td>0.067905</td>\n",
              "      <td>0.074083</td>\n",
              "      <td>0.089991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>normal</td>\n",
              "      <td>4.106644</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.181992</td>\n",
              "      <td>0.127307</td>\n",
              "      <td>0.073866</td>\n",
              "      <td>0.110189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>non-infection</td>\n",
              "      <td>-0.342441</td>\n",
              "      <td>0.738</td>\n",
              "      <td>0.138709</td>\n",
              "      <td>0.141636</td>\n",
              "      <td>0.169776</td>\n",
              "      <td>0.242774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>scar</td>\n",
              "      <td>3.530070</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.164444</td>\n",
              "      <td>0.123306</td>\n",
              "      <td>0.082617</td>\n",
              "      <td>0.081827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tumor</td>\n",
              "      <td>-3.784626</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.140423</td>\n",
              "      <td>0.102705</td>\n",
              "      <td>0.287111</td>\n",
              "      <td>0.175874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>deposit</td>\n",
              "      <td>0.776070</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.176102</td>\n",
              "      <td>0.104585</td>\n",
              "      <td>0.154578</td>\n",
              "      <td>0.121643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>APAC</td>\n",
              "      <td>-0.077923</td>\n",
              "      <td>0.941</td>\n",
              "      <td>0.142654</td>\n",
              "      <td>0.159529</td>\n",
              "      <td>0.148437</td>\n",
              "      <td>0.114314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>lens opacity</td>\n",
              "      <td>0.898269</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.114988</td>\n",
              "      <td>0.097340</td>\n",
              "      <td>0.084550</td>\n",
              "      <td>0.125398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bullous</td>\n",
              "      <td>-1.065212</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.129807</td>\n",
              "      <td>0.067976</td>\n",
              "      <td>0.163363</td>\n",
              "      <td>0.145041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>All Classes</td>\n",
              "      <td>0.563571</td>\n",
              "      <td>0.574</td>\n",
              "      <td>0.143497</td>\n",
              "      <td>0.109576</td>\n",
              "      <td>0.136555</td>\n",
              "      <td>0.146311</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a0fd037-2798-4b2d-ac59-d6d382f2477d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a0fd037-2798-4b2d-ac59-d6d382f2477d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a0fd037-2798-4b2d-ac59-d6d382f2477d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-274145d5-14b0-420c-9c63-38dd800abd3a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-274145d5-14b0-420c-9c63-38dd800abd3a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-274145d5-14b0-420c-9c63-38dd800abd3a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_be9addcd-980b-4178-94a6-222092c37c22\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_be9addcd-980b-4178-94a6-222092c37c22 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"class_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"bullous\",\n          \"normal\",\n          \"deposit\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"t_stat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.218086084539315,\n        \"min\": -3.784625740148133,\n        \"max\": 4.106644107587545,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          -1.0652123681513856,\n          4.106644107587545,\n          0.7760695466399565\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_value\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"0.299\",\n          \"0.000\",\n          \"0.941\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slit_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.028856993115778225,\n        \"min\": 0.0842488211307921,\n        \"max\": 0.18199186492652655,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.12980731231668086,\n          0.18199186492652655,\n          0.17610238014509555\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slit_std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029271190306711402,\n        \"min\": 0.06790482861653908,\n        \"max\": 0.15952938565968372,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.06797646215087792,\n          0.12730701706693787,\n          0.10458495526391483\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sumaho_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06531376990474773,\n        \"min\": 0.07386616799035182,\n        \"max\": 0.28711066793175105,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.16336325307205643,\n          0.07386616799035182,\n          0.15457763533266047\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sumaho_std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.046762711552110384,\n        \"min\": 0.08182675223395117,\n        \"max\": 0.24277384997588608,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.14504053279340573,\n          0.11018912960235533,\n          0.12164331883330962\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ANOVAを実行\n",
        "anova_result = stats.f_oneway(\n",
        "    df_combined[df_combined['class_name'] == 'infection'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'normal'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'non-infection'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'scar'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'tumor'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'deposit'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'APAC'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'lens opacity'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'bullous'][layer_name]\n",
        ")\n",
        "\n",
        "print(f\"ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(df_combined[layer_name], df_combined['class_name'])\n",
        "\n",
        "# 結果を表示\n",
        "print(tukey_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjAiQjnMYHjN",
        "outputId": "8bb20bdc-7fd9-4b00-90d9-f8f2eb87dda0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANOVA result: F=5.279073563921913, p=2.4769766159597473e-06\n",
            "       Multiple Comparison of Means - Tukey HSD, FWER=0.05        \n",
            "==================================================================\n",
            "    group1        group2    meandiff p-adj   lower   upper  reject\n",
            "------------------------------------------------------------------\n",
            "         APAC       bullous   0.0077    1.0  -0.107  0.1223  False\n",
            "         APAC       deposit   0.0318 0.9933 -0.0792  0.1429  False\n",
            "         APAC     infection  -0.0569 0.8163 -0.1691  0.0554  False\n",
            "         APAC  lens opacity  -0.0332 0.9934 -0.1495   0.083  False\n",
            "         APAC non-infection   0.0071    1.0 -0.1154  0.1297  False\n",
            "         APAC        normal  -0.0029    1.0 -0.1154  0.1096  False\n",
            "         APAC          scar  -0.0116    1.0 -0.1213   0.098  False\n",
            "         APAC         tumor   0.0767 0.4546 -0.0356   0.189  False\n",
            "      bullous       deposit   0.0242 0.9838 -0.0496  0.0979  False\n",
            "      bullous     infection  -0.0645 0.1658 -0.1402  0.0111  False\n",
            "      bullous  lens opacity  -0.0409 0.8223 -0.1223  0.0405  False\n",
            "      bullous non-infection  -0.0005    1.0 -0.0907  0.0897  False\n",
            "      bullous        normal  -0.0106    1.0 -0.0865  0.0654  False\n",
            "      bullous          scar  -0.0193 0.9956  -0.091  0.0523  False\n",
            "      bullous         tumor    0.069 0.1056 -0.0066  0.1447  False\n",
            "      deposit     infection  -0.0887 0.0029 -0.1587 -0.0187   True\n",
            "      deposit  lens opacity  -0.0651 0.1648 -0.1413  0.0111  False\n",
            "      deposit non-infection  -0.0247  0.993 -0.1102  0.0609  False\n",
            "      deposit        normal  -0.0347 0.8364 -0.1051  0.0356  False\n",
            "      deposit          scar  -0.0435 0.5005 -0.1092  0.0222  False\n",
            "      deposit         tumor   0.0449 0.5457 -0.0251  0.1149  False\n",
            "    infection  lens opacity   0.0236 0.9903 -0.0544  0.1016  False\n",
            "    infection non-infection    0.064 0.3512 -0.0232  0.1512  False\n",
            "    infection        normal   0.0539 0.3287 -0.0184  0.1262  False\n",
            "    infection          scar   0.0452 0.4889 -0.0226   0.113  False\n",
            "    infection         tumor   0.1336    0.0  0.0616  0.2055   True\n",
            " lens opacity non-infection   0.0404 0.9101 -0.0518  0.1326  False\n",
            " lens opacity        normal   0.0303 0.9546  -0.048  0.1086  False\n",
            " lens opacity          scar   0.0216 0.9925 -0.0525  0.0957  False\n",
            " lens opacity         tumor   0.1099 0.0005  0.0319  0.1879   True\n",
            "non-infection        normal  -0.0101    1.0 -0.0975  0.0774  False\n",
            "non-infection          scar  -0.0188 0.9988 -0.1025  0.0649  False\n",
            "non-infection         tumor   0.0695 0.2411 -0.0176  0.1567  False\n",
            "       normal          scar  -0.0087    1.0 -0.0768  0.0594  False\n",
            "       normal         tumor   0.0796 0.0187  0.0073  0.1519   True\n",
            "         scar         tumor   0.0883 0.0019  0.0206  0.1561   True\n",
            "------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ANOVA_heatmap**"
      ],
      "metadata": {
        "id": "59zc1fuTqBK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'})\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "exil8PT2ZzsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tukey_sumaho/slit別**"
      ],
      "metadata": {
        "id": "XbxW3P2OqFTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "def perform_anova_and_tukey(df, title):\n",
        "    # 欠損値を削除\n",
        "    df = df.dropna(subset=[layer_name])\n",
        "\n",
        "    # ANOVAを実行\n",
        "    anova_result = stats.f_oneway(\n",
        "        df[df['class_name'] == 'infection'][layer_name],\n",
        "        df[df['class_name'] == 'normal'][layer_name],\n",
        "        df[df['class_name'] == 'non-infection'][layer_name],\n",
        "        df[df['class_name'] == 'scar'][layer_name],\n",
        "        df[df['class_name'] == 'tumor'][layer_name],\n",
        "        df[df['class_name'] == 'deposit'][layer_name],\n",
        "        df[df['class_name'] == 'APAC'][layer_name],\n",
        "        df[df['class_name'] == 'lens opacity'][layer_name],\n",
        "        df[df['class_name'] == 'bullous'][layer_name]\n",
        "    )\n",
        "\n",
        "    print(f\"{title} ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "    # 事後検定（TukeyのHSD検定）を実行\n",
        "    tukey_result = pairwise_tukeyhsd(endog=df[layer_name], groups=df['class_name'])\n",
        "\n",
        "    # 結果をDataFrameに変換\n",
        "    tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "    # マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "    matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "    # マトリックスにp値を入力\n",
        "    for i in range(len(tukey_result_df)):\n",
        "        group1 = tukey_result_df.loc[i, 'group1']\n",
        "        group2 = tukey_result_df.loc[i, 'group2']\n",
        "        p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "        matrix.loc[group1, group2] = p_value\n",
        "        matrix.loc[group2, group1] = p_value\n",
        "\n",
        "    # 同じクラス間のセルをNaNに設定\n",
        "    np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "    # ヒートマップを描画\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'}, mask=matrix.isnull())\n",
        "    plt.title(f'Pairwise Comparison P-Values (Tukey HSD) - {title}')\n",
        "    plt.show()\n",
        "\n",
        "# スリットランプデータの解析\n",
        "perform_anova_and_tukey(df_slit, \"Slit Lamp Data\")\n",
        "\n",
        "# スマートフォンデータの解析\n",
        "perform_anova_and_tukey(df_sumaho, \"Smartphone Data\")"
      ],
      "metadata": {
        "id": "8m7Z9FCzbSmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 両方のデータを結合\n",
        "df_combined = pd.concat([df_slit, df_sumaho]).reset_index(drop=True)\n",
        "\n",
        "# \"scar\" + \"non-infection\" グループとその他のクラスに分類\n",
        "df_combined['group'] = df_combined['class_name'].apply(lambda x: 'scar + non-infection' if x in ['scar', 'non-infection'] else 'others')\n",
        "\n",
        "# グループごとにデータを抽出\n",
        "scar_non_infection_group = df_combined[df_combined['group'] == 'scar + non-infection'][layer_name].dropna()\n",
        "others_group = df_combined[df_combined['group'] == 'others'][layer_name].dropna()\n",
        "\n",
        "# t検定を実行\n",
        "t_stat, p_value = stats.ttest_ind(scar_non_infection_group, others_group)\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
        "\n",
        "# 統計値を計算\n",
        "scar_non_infection_mean = scar_non_infection_group.mean()\n",
        "scar_non_infection_std = scar_non_infection_group.std()\n",
        "others_mean = others_group.mean()\n",
        "others_std = others_group.std()\n",
        "\n",
        "print(f\"scar + non-infection mean: {scar_non_infection_mean}, std: {scar_non_infection_std}\")\n",
        "print(f\"others mean: {others_mean}, std: {others_std}\")\n"
      ],
      "metadata": {
        "id": "-ubPn9Erqnid",
        "outputId": "922599df-4a69-4e3c-884b-4b52363ccbaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t-statistic: 12.32446646508546, p-value: 3.0054565569187417e-30\n",
            "scar + non-infection mean: 0.3084456893416098, std: 0.17867467036662105\n",
            "others mean: 0.1427152324390296, std: 0.09632144010269082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcxqbJ_GrbPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}