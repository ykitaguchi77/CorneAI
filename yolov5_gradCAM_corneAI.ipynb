{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPDqCjmTdsF4sd3cExoBwdk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/yolov5_gradCAM_corneAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d42M6k9QpvSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_GradCAM_CorneAI**\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "id": "2cakhs2BZLRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "ytBOWsuXZQzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "P7oJE1rLrE_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0835b9b-b62d-4ec0-9407-fd9db4245f87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p48tU-_wYVHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7336e061-4df6-4290-a78f-1af44a413c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 17.92 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ],
      "metadata": {
        "id": "wkvc2KijYes5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python main.py --model-path $model_path --img-path $img_path --output-dir out"
      ],
      "metadata": {
        "id": "cjgRkuSuYinZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Top3 analysis**"
      ],
      "metadata": {
        "id": "wvBLe9pbpCwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit\"\n",
        "print(len(os.listdir(img_dir)))"
      ],
      "metadata": {
        "id": "GWizQkPAjczd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ebdef3-95b8-489c-ef56-927f327cf4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "FN98dpHRuyNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c32ecff-f6bc-493e-d360-42793dcbd660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rMIlXO_mIun8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAM**"
      ],
      "metadata": {
        "id": "uoTET4OvDiRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GradCAM通常バージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "# 以下の関数は変更なし\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "# YOLOV5GradCAMクラスを更新\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []  # クラス名を格納するリストを初期化\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        return alpha.view(b, k, 1, 1)\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)\n",
        "        weights = (positive_gradients * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        return eigen_cam.unsqueeze(1)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[200:234]\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        #cv2.imwrite(output_path, final_image)\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー →Best\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    ##target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    ##target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "    #for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ],
      "metadata": {
        "id": "6hBb7eKP986c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GradCAM\n",
        "広く見るバージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[210:211] ###########画像を指定\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img = apply_heatmap_to_entire_image(masks[i], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "ojsVa-JDefA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def print_model_structure(model):\n",
        "    \"\"\"\n",
        "    モデルの構造を表示する関数\n",
        "    \"\"\"\n",
        "    print(\"Model structure:\")\n",
        "    for i, (name, module) in enumerate(model.model.named_modules()):\n",
        "        if not isinstance(module, torch.nn.Sequential) and not isinstance(module, torch.nn.ModuleList):\n",
        "            print(f\"{i}: {name} - {module.__class__.__name__}\")\n",
        "\n",
        "def find_yolo_layer(model, layer_index):\n",
        "    \"\"\"\n",
        "    YOLOv5モデルの後ろから指定されたインデックスのレイヤーを返す\n",
        "    \"\"\"\n",
        "    module_list = list(model.model.modules())\n",
        "    module_list = [m for m in module_list if not isinstance(m, torch.nn.Sequential) and not isinstance(m, torch.nn.ModuleList)]\n",
        "    target_layer = module_list[-layer_index]\n",
        "    return target_layer, f\"{target_layer.__class__.__name__} (index: -{layer_index})\"\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_index, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer, layer_name = find_yolo_layer(self.model, layer_index)\n",
        "        self.layer_name = layer_name\n",
        "        print(f\"Selected layer: {self.layer_name}\")\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method, saliency_method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    print(f\"Using layer: {saliency_method.layer_name}\")\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[200:234]\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img = apply_heatmap_to_entire_image(masks[i], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    layer_index = 1  # モデルの後ろから1番目のレイヤーを選択\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=(img_size, img_size), names=names)\n",
        "\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_index=layer_index, img_size=(img_size, img_size), method=method)\n",
        "            folder_main(img_path, method, saliency_method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "uS-LWXlVgFpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3RmQue4SefDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**EigenCAM**"
      ],
      "metadata": {
        "id": "RiAwrM4475X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        # GradCAMの重み計算\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        # GradCAM++の重み計算\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[6:15]\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        #cv2.imwrite(output_path, final_image)\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_0'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['eigencam']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ],
      "metadata": {
        "id": "_KXqjIkf1yg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3G5YQn4dBm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EigenCAM\n",
        "広く可視化するバージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        res_img = apply_heatmap_to_entire_image(mask, res_img)\n",
        "    return res_img, None\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[6:15]\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img, _ = get_res_img(None, [masks[i]], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "    for method in ['eigencam']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "kRvM0njcdBov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2RWDTh3dBqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o_y_jlLBdBsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsuMkl6BdBus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=(img_size, img_size), names=names)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "7-Ydkg4tdBwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.exit()"
      ],
      "metadata": {
        "id": "SFDwQ_bfIYaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LayerごとTop1の可視化**"
      ],
      "metadata": {
        "id": "MDjAHX8v7tS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                          'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "                          'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
        "                          'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "                          'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "                          'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img): ##マージしているレイヤー名\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, f\"{cls_names[0]} - {method} - {layer_name}\", res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    #cv2_imshow(final_image)\n",
        "    cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path, method, file_list):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv'] ##\n",
        "        target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "        ##target_layers = ['model_17_m_0_cv2_conv', 'model_20_m_0_cv2_conv', 'model_23_m_0_cv2_conv']\n",
        "        #target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            print(f\"boxes: {boxes}\")\n",
        "\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, f\"{cls_names[0]}-{'layer'+layer_name.replace('model_', '')}\", res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image) ##画像を表示\n",
        "        #cv2.imwrite(output_path, final_image) ##画像を保存\n",
        "\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer24'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho_layer24'  # 出力ディレクトリ\n",
        "\n",
        "    # ファイル名を数字でソート\n",
        "    if os.path.isdir(img_path):\n",
        "        file_list = sorted(os.listdir(img_path), key=lambda x: int(x.split('.')[0]))\n",
        "        ##########必要に応じて、処理するファイルの範囲を指定########\n",
        "        file_list = file_list[210:211]\n",
        "        ############################################################\n",
        "\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method, file_list)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "Xwry3nj9B9ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbQBtqIHTaCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yvJ9wVXLUSov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0HgW2xNUSrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "## 画像全体（bbox外）も表示\n",
        "#################################\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                          'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "                          'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
        "                          'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "                          'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "                          'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.5, n_heatmap, 0.5, 0)  ###ここでヒートマップと元画像のマージの比率を決める\n",
        "    return res_img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        res_img = apply_heatmap_to_entire_image(mask, res_img)\n",
        "    return res_img, None\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img): ##マージしているレイヤー名\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "\n",
        "    # バウンディングボックスを太く描画\n",
        "    thickness = 3  # 線の太さを増やす（必要に応じて調整してください）\n",
        "    res_img = Box.put_box(res_img, bbox, thickness=thickness)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    text_thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, text_thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, text_thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, f\"{cls_names[0]} - {method} - {layer_name}\", res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    #cv2_imshow(final_image)\n",
        "    cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path, method, file_list):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv'] ##\n",
        "        target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "        ##target_layers = ['model_17_m_0_cv2_conv', 'model_20_m_0_cv2_conv', 'model_23_m_0_cv2_conv']\n",
        "        #target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            print(f\"boxes: {boxes}\")\n",
        "\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, f\"{cls_names[0]}-{'layer'+layer_name.replace('model_', '')}\", res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image) ##画像を表示\n",
        "        #cv2.imwrite(output_path, final_image) ##画像を保存\n",
        "\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer24'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho_layer24'  # 出力ディレクトリ\n",
        "\n",
        "    # ファイル名を数字でソート\n",
        "    if os.path.isdir(img_path):\n",
        "        file_list = sorted(os.listdir(img_path), key=lambda x: int(x.split('.')[0]))\n",
        "        ##########必要に応じて、処理するファイルの範囲を指定########\n",
        "        file_list = file_list[210:211]\n",
        "        ############################################################\n",
        "\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method, file_list)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "0FyVE0SKUStt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaJLDNoaA8D1",
        "outputId": "e533cfd2-1039-4839-9b12-dbe2c4a1739e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPJ1d31SasiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (640,640)\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "HQ0eRLpgoi_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q81i6ptncfDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hauGiazwcfG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXvgJcOytU51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sort GradCam Images**\n",
        "\n",
        "GradCAM画像をクラスごとにフォルダ分け"
      ],
      "metadata": {
        "id": "YHLMn6kZIlSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara234.csv\"\n",
        "\n",
        "# CSVファイルを読み込み\n",
        "df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
        "\n",
        "# データフレームの最初の数行を表示\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZaYBsjL7NTcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dirs = [\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit',\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_sumaho'\n",
        "]\n",
        "destination_base_dirs = [\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_slit_sorted',\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_sumaho_sorted'\n",
        "]\n",
        "\n",
        "# Iterate over the source and destination directory pairs\n",
        "for source_dir, destination_base_dir in zip(source_dirs, destination_base_dirs):\n",
        "    print(f'Processing files from {source_dir} to {destination_base_dir}')\n",
        "    # Iterate over the dataframe and copy files to the appropriate class folder\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        serial_number = row['serial_number']\n",
        "        class_num = row['class_num']\n",
        "        source_file = os.path.join(source_dir, f'{serial_number}-res.jpg')\n",
        "        destination_dir = os.path.join(destination_base_dir, str(class_num))\n",
        "\n",
        "        # Create the destination directory if it does not exist\n",
        "        os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "        # Copy the file to the destination directory\n",
        "        destination_file = os.path.join(destination_dir, f'{serial_number}-res.jpg')\n",
        "        if os.path.exists(source_file):\n",
        "            shutil.copy(source_file, destination_file)\n",
        "        else:\n",
        "            print(f'File {source_file} does not exist.')\n",
        "\n",
        "print('Files have been copied successfully.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBll2vlDM26D",
        "outputId": "f238c4e9-9e95-44f6-cffc-3b90ec61e326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Processing files from /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit to /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_slit_sorted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 228/228 [00:02<00:00, 88.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing files from /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_sumaho to /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_sumaho_sorted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 228/228 [00:02<00:00, 89.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been copied successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Compare area of interest**\n",
        "\n",
        "注目度n%以上の画像を"
      ],
      "metadata": {
        "id": "dxtwlXulSgO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ニ値化して表示**"
      ],
      "metadata": {
        "id": "2lQctr8O90Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###画像として表示"
      ],
      "metadata": {
        "id": "VllpVTyg9zYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "6iDm_dPKM28s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "#img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "#output_dir = 'out'  # 出力ディレクトリ\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "img_size = 640  # 入力画像サイズ\n",
        "target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_res_img(bbox, masks, res_img, threshold):\n",
        "    total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        mask[mask < threshold*255] = 0  # 追加: 128未満の値を0にする\n",
        "        mask[mask >= threshold*255] = 255  # 追加: 128未満の値を0にする\n",
        "        binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "\n",
        "        # bboxの範囲内のマスクの部分を取得\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "        # 閾値を超える共通部分のピクセル数をカウント\n",
        "        intersect_pixels = np.sum(mask_bbox)\n",
        "        total_intersect_pixels += intersect_pixels\n",
        "\n",
        "        # mask_bbox のピクセル数を取得\n",
        "        mask_bbox_area = mask_bbox.size\n",
        "\n",
        "        # AOI を計算\n",
        "        AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "        print(f\"Area of Interest (AOI): {AOI}\")\n",
        "\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # Set the font size and other parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    # Calculate the text size and position\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10  # Adjust the horizontal position to place the text near the left edge\n",
        "    text_y = text_size[1] + 10  # Adjust the vertical position to place the text near the top edge\n",
        "\n",
        "    # Put the text on the image\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "        # print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        tic = time.time()\n",
        "        preds, logits = self.model(input_img)\n",
        "        # print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "        else:\n",
        "            self.cls_names = []  # top3_indicesが空の場合は空のリストを代入\n",
        "\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "            self.model.zero_grad()\n",
        "            tic = time.time()\n",
        "            score.backward(retain_graph=True)\n",
        "            # print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            weights = alpha.view(b, k, 1, 1)\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                      names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    if len(masks) > 0:\n",
        "        mask = masks[0][0]  # top1のマスクのみ使用\n",
        "        bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "        cls_name = cls_names[0]  # top1のクラス名のみ使用\n",
        "        res_img = result.copy()\n",
        "        res_img, _ = get_res_img(bbox, [mask], res_img, GradCAM_threshold)\n",
        "        res_img = put_text_box(bbox, cls_name, res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    #os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    ##############################################################################################\n",
        "    ######## ファイル名を数字でソート#############################################################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[0:5]\n",
        "    ##############################################################################################\n",
        "    ##############################################################################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        if method == 'gradcam':\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        if len(masks) > 0:\n",
        "            mask = masks[0][0]  # top1のマスクのみ使用\n",
        "            bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "            cls_name = cls_names[0]  # top1のクラス名のみ使用\n",
        "            res_img = result.copy()\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img, GradCAM_threshold)\n",
        "            res_img = put_text_box(bbox, cls_name, res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        #os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "        # オブジェクトを明示的に削除\n",
        "        del saliency_method\n",
        "        del masks\n",
        "        del logits\n",
        "        del boxes\n",
        "        del images\n",
        "        del final_image\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_0'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "\n",
        "    GradCAM_threshold = 0.5 #GradCAMの閾値設定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)"
      ],
      "metadata": {
        "id": "HV90u_Z9Urn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Area of interestの計算**\n",
        "\n",
        "結果をcsvに保存する"
      ],
      "metadata": {
        "id": "iEUdHWgh79uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "###これを押すとcsvが更新されてしまうので注意！！！\n",
        "\n",
        "# CSVファイルのパス\n",
        "input_file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルをDataFrameとして読み込む\n",
        "df = pd.read_csv(input_file_path)\n",
        "\n",
        "# 'Unnamed'が含まれる列を削除する\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# 例としてthresholdの値を設定\n",
        "threshold = 0.5\n",
        "\n",
        "# 指定されたレイヤー\n",
        "layers = [\"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "# 各レイヤーに対して新しい列を作成\n",
        "for layer in layers:\n",
        "    df[f'AOI_{threshold}_layer{layer}'] = None  # 初期値を設定\n",
        "\n",
        "# 修正後のDataFrameを表示する\n",
        "print(df.head())\n",
        "\n",
        "# DataFrameをCSVファイルとして保存\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "print(f\"Updated DataFrame has been saved to: {file_path}\")\n"
      ],
      "metadata": {
        "id": "Up7ykhgI5l3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                      in_source=Box.BoxSource.Torch,\n",
        "                                      to_source=Box.BoxSource.Numpy,\n",
        "                                      return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "wj6fl4uVCCji"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS3iA_8QKWZC",
        "outputId": "62cb563e-f0b4-4fa1-fd18-cccd60deda76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxS9jH2fO59C",
        "outputId": "865799b6-b98e-42b0-ad24-259268719ad4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "##################\n",
        "## 3-layerで解析##\n",
        "##################\n",
        "##################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "   def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "       self.model = model\n",
        "       self.gradients = dict()\n",
        "       self.activations = dict()\n",
        "       self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "       def backward_hook(module, grad_input, grad_output):\n",
        "           self.gradients['value'] = grad_output[0]\n",
        "           return None\n",
        "\n",
        "       def forward_hook(module, input, output):\n",
        "           self.activations['value'] = output\n",
        "           return None\n",
        "\n",
        "       target_layer = find_yolo_layer(self.model, layer_name)\n",
        "       target_layer.register_forward_hook(forward_hook)\n",
        "       target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "       device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "       self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "       # print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "   def forward(self, input_img, class_idx=True):\n",
        "       saliency_maps = []\n",
        "       b, c, h, w = input_img.size()\n",
        "       tic = time.time()\n",
        "       preds, logits = self.model(input_img)\n",
        "       # print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "\n",
        "       _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "       if top3_indices.numel() > 0:\n",
        "           preds[1][0] = top3_indices.tolist()[0]\n",
        "           preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "           self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "       else:\n",
        "           self.cls_names = []  # top3_indicesが空の場合は空のリストを代入\n",
        "\n",
        "       for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "           if class_idx:\n",
        "               score = logits[0][0][cls]\n",
        "           else:\n",
        "               score = logits[0][0].max()\n",
        "           self.model.zero_grad()\n",
        "           tic = time.time()\n",
        "           score.backward(retain_graph=True)\n",
        "           # print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "           gradients = self.gradients['value']\n",
        "           activations = self.activations['value']\n",
        "           b, k, u, v = gradients.size()\n",
        "           alpha = gradients.view(b, k, -1).mean(2)\n",
        "           weights = alpha.view(b, k, 1, 1)\n",
        "           saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "           saliency_map = F.relu(saliency_map)\n",
        "           saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "           saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "           saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "           saliency_maps.append(saliency_map)\n",
        "\n",
        "       return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "\n",
        "   def __call__(self, input_img):\n",
        "       return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "   for mask in masks:\n",
        "       # マスクの前処理\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "       # bboxの範囲内のマスク部分を取得\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # 閾値を超える共通部分のピクセル数をカウント\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox のピクセル数を取得\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI を計算\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   # if len(bbox) == 0 or len(masks) == 0:\n",
        "   #     return 0.0\n",
        "\n",
        "   # for mask in masks:\n",
        "   #     mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "   #     mask[mask < threshold*255] = 0\n",
        "   #     mask[mask >= threshold*255] = 255\n",
        "   #     heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "   #     n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "   #     aoi = np.sum(n_heatmat > 0) / (n_heatmat.shape[0] * n_heatmat.shape[1])\n",
        "   #     print(f\"Area of Interest (original_AOI): {aoi}\")\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "   df = pd.read_csv(csv_path)\n",
        "   df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "  #  for layer_num in [17, 20, 23]:\n",
        "  #      if f'AOI_{threshold}_layer{layer_num}' not in df.columns:\n",
        "  #          df[f'AOI_{threshold}_layer{layer_num}'] = None\n",
        "   for layer_num in [0, 1, 2]:\n",
        "       if f'AOI_{threshold}_layer24_m_{layer_num}' not in df.columns:\n",
        "           df[f'AOI_{threshold}_layer24_m_{layer_num}'] = None\n",
        "\n",
        "   if end_index is None:\n",
        "       end_index = len(df)\n",
        "\n",
        "   for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "       if index < start_index or index >= end_index:\n",
        "           continue\n",
        "\n",
        "       serial_number = row['serial_number']\n",
        "\n",
        "       if pd.isna(serial_number):\n",
        "           print(f\"Skipping row {index} due to NaN serial_number\")\n",
        "           continue\n",
        "\n",
        "       img_name = f\"{int(serial_number)}.jpg\"\n",
        "       img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "       # Rest of the code remains the same\n",
        "       if os.path.exists(img_path):\n",
        "           img = cv2.imread(img_path)\n",
        "           torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "           for layer_num, saliency_method in zip([0, 1, 2], saliency_methods):\n",
        "               try:\n",
        "                   masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "               except ValueError as e:\n",
        "                   print(f\"Error processing image {img_name}: {str(e)}\")\n",
        "                   continue\n",
        "\n",
        "               if len(masks) > 0 and len(boxes) > 0:\n",
        "                   mask = masks[0][0]  # top1のマスクのみ使用\n",
        "                   bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "\n",
        "                   aoi = get_aoi(bbox, [mask], threshold)\n",
        "                   #df.at[index, f'AOI_{threshold}_layer{layer_num}'] = aoi\n",
        "                   df.at[index, f'AOI_{threshold}_layer24_m_{layer_num}'] = aoi\n",
        "\n",
        "                   print(f\"Image: {img_name}, Layer: {layer_num}, AOI: {aoi}\")\n",
        "       else:\n",
        "           print(f\"Image not found: {img_name}\")\n",
        "\n",
        "   df.to_csv(csv_path, index=False)\n",
        "\n",
        "# 使用例\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv\"\n",
        "#folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "#csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "threshold = 0.5\n",
        "\n",
        "##############################################\n",
        "start_index = 180 # 開始するインデックスを指定\n",
        "end_index = 240 # 終了するインデックスを指定\n",
        "##############################################\n",
        "\n",
        "# モデルとsaliency_methodの定義\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cpu'\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "#target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2']\n",
        "\n",
        "saliency_methods = [YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size) for layer in target_layers]\n",
        "\n",
        "calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)"
      ],
      "metadata": {
        "id": "OrjCYpUtJlyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LE7ZYlphCpBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show_result\n",
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# DataFrameとしてCSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# DataFrameの最初の数行を表示する\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "gVNlV1lh_rRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**注目点色塗り**"
      ],
      "metadata": {
        "id": "fmGNyvTdM3c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#注目点に色を塗る（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            color = np.array([0,0,0], dtype=np.uint8) #マスクの色：白は[0,0,0]\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + (1 - binary_mask[..., np.newaxis]) * color\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + (1 - black_mask[..., np.newaxis]) * color\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/8.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.1  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ],
      "metadata": {
        "id": "sx9KdyNZNzz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 注目点をblurする（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            # ぼかし効果を適用\n",
        "            blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + blurred_result * (1 - binary_mask[..., np.newaxis])\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + black_blurred_result * (1 - black_mask[..., np.newaxis])\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/41.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.3  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ],
      "metadata": {
        "id": "VEn_KHC-T8C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Analyze results**"
      ],
      "metadata": {
        "id": "69WoakWi-ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the CSV files\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# Create confusion matrices for Slit and Sumaho datasets\n",
        "conf_matrix_slit = pd.crosstab(df_slit['class_num'], df_slit['top1'])\n",
        "conf_matrix_sumaho = pd.crosstab(df_sumaho['class_num'], df_sumaho['top1'])\n",
        "\n",
        "# Rename the index and columns for better readability\n",
        "conf_matrix_slit.index = class_names\n",
        "conf_matrix_slit.columns = class_names\n",
        "conf_matrix_sumaho.index = class_names\n",
        "conf_matrix_sumaho.columns = class_names\n",
        "\n",
        "# Plotting confusion matrices using seaborn\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Slit Lamp Data Confusion Matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_slit, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Slit Lamp Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "\n",
        "# Smartphone Data Confusion Matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_sumaho, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Smartphone Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h03rgBUoWi_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "ee4e30f4-9648-4092-bd60-d57880c6459b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for Slit and Sumaho datasets\n",
        "accuracy_slit = (conf_matrix_slit.values.diagonal().sum() / conf_matrix_slit.values.sum()) * 100\n",
        "accuracy_sumaho = (conf_matrix_sumaho.values.diagonal().sum() / conf_matrix_sumaho.values.sum()) * 100\n",
        "\n",
        "accuracy_slit, accuracy_sumaho\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTt2gTWAAD3",
        "outputId": "7bf0838c-761c-419e-99df-d41dba6187f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.08771929824562, 77.63157894736842)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# AOI_0.5の平均と標準偏差を計算\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')['AOI_0.5'].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')['AOI_0.5'].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuQdE9zzPm68",
        "outputId": "2972c4e5-fb07-4d92-a74b-dfb60cad016e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ± SD AOI for Slit Lamp Data\n",
            "                   mean       std\n",
            "infection      0.086643  0.037705\n",
            "normal         0.050446  0.016585\n",
            "non-infection  0.271836  0.145000\n",
            "scar           0.205724  0.104553\n",
            "tumor          0.195092  0.097643\n",
            "deposit        0.072846  0.041480\n",
            "APAC           0.068622  0.036099\n",
            "lens opacity   0.137005  0.033883\n",
            "bullous        0.126359  0.041344\n",
            "\n",
            "Mean ± SD AOI for Smartphone Data\n",
            "               mean  std\n",
            "infection       0.0  0.0\n",
            "normal          0.0  0.0\n",
            "non-infection   0.0  0.0\n",
            "scar            0.0  0.0\n",
            "tumor           0.0  0.0\n",
            "deposit         0.0  0.0\n",
            "APAC            0.0  0.0\n",
            "lens opacity    0.0  0.0\n",
            "bullous         0.0  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIの平均と標準偏差を計算\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v9DUXYHkJ5x",
        "outputId": "a30f7720-8b41-4c53-c475-3930434eeabf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ± SD AOI for Slit Lamp Data\n",
            "              AOI_0.5_layer24_m_0      AOI_0.5_layer24_m_1       \\\n",
            "                             mean  std                mean  std   \n",
            "infection                     0.0  0.0                 0.0  0.0   \n",
            "normal                        0.0  0.0                 0.0  0.0   \n",
            "non-infection                 0.0  0.0                 0.0  0.0   \n",
            "scar                          0.0  0.0                 0.0  0.0   \n",
            "tumor                         0.0  0.0                 0.0  0.0   \n",
            "deposit                       0.0  0.0                 0.0  0.0   \n",
            "APAC                          0.0  0.0                 0.0  0.0   \n",
            "lens opacity                  0.0  0.0                 0.0  0.0   \n",
            "bullous                       0.0  0.0                 0.0  0.0   \n",
            "\n",
            "              AOI_0.5_layer24_m_2            \n",
            "                             mean       std  \n",
            "infection                0.047342  0.023897  \n",
            "normal                   0.026101  0.010555  \n",
            "non-infection            0.116942  0.075453  \n",
            "scar                     0.073900  0.030724  \n",
            "tumor                    0.112100  0.069113  \n",
            "deposit                  0.036668  0.025242  \n",
            "APAC                     0.063290  0.037150  \n",
            "lens opacity             0.072591  0.040474  \n",
            "bullous                  0.067484  0.022869  \n",
            "\n",
            "Mean ± SD AOI for Smartphone Data\n",
            "              AOI_0.5_layer24_m_0      AOI_0.5_layer24_m_1            \\\n",
            "                             mean  std                mean       std   \n",
            "infection                     0.0  0.0            0.000000  0.000000   \n",
            "normal                        0.0  0.0            0.001019  0.004992   \n",
            "non-infection                 0.0  0.0            0.000000  0.000000   \n",
            "scar                          0.0  0.0            0.000000  0.000000   \n",
            "tumor                         0.0  0.0            0.000000  0.000000   \n",
            "deposit                       0.0  0.0            0.005983  0.034369   \n",
            "APAC                          0.0  0.0            0.000000  0.000000   \n",
            "lens opacity                  0.0  0.0            0.000000  0.000000   \n",
            "bullous                       0.0  0.0            0.000000  0.000000   \n",
            "\n",
            "              AOI_0.5_layer24_m_2            \n",
            "                             mean       std  \n",
            "infection                0.063889  0.037739  \n",
            "normal                   0.037293  0.024043  \n",
            "non-infection            0.178614  0.139032  \n",
            "scar                     0.125316  0.081693  \n",
            "tumor                    0.165419  0.065204  \n",
            "deposit                  0.083059  0.058240  \n",
            "APAC                     0.079910  0.030570  \n",
            "lens opacity             0.111812  0.069523  \n",
            "bullous                  0.082440  0.040750  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Layerごとに解析**"
      ],
      "metadata": {
        "id": "Z2rBLnp6lbjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "####################\n",
        "layer = \"24_m_2\"\n",
        "#\"24_m_0\", \"24_m_1', \"24_m_2'\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形してSeabornでプロットできるようにする\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# グラフのサイズを設定\n",
        "plt.figure(figsize=(18, 8))\n",
        "\n",
        "# Seabornのboxplotを使用してプロット\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined)\n",
        "\n",
        "# ラベルとタイトルを設定\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel(layer_name)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp and Smartphone Data')\n",
        "\n",
        "# グラフを表示\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jkZ4ZbaoUEMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# 各クラスごとに対応のあるt検定を行う\n",
        "results = []\n",
        "for class_name in class_names:\n",
        "    slit_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "    sumaho_data = df_sumaho[df_sumaho['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "    # 対応のあるデータを取るため、最小の長さに合わせる\n",
        "    min_length = min(len(slit_data), len(sumaho_data))\n",
        "    slit_data = slit_data[:min_length]\n",
        "    sumaho_data = sumaho_data[:min_length]\n",
        "\n",
        "    t_stat, p_value = stats.ttest_rel(slit_data, sumaho_data)\n",
        "\n",
        "    # スリットランプデータの統計値\n",
        "    slit_mean = slit_data.mean()\n",
        "    slit_std = slit_data.std()\n",
        "\n",
        "    # スマートフォンデータの統計値\n",
        "    sumaho_mean = sumaho_data.mean()\n",
        "    sumaho_std = sumaho_data.std()\n",
        "\n",
        "    results.append({\n",
        "        'class_name': class_name,\n",
        "        't_stat': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'slit_mean': slit_mean,\n",
        "        'slit_std': slit_std,\n",
        "        'sumaho_mean': sumaho_mean,\n",
        "        'sumaho_std': sumaho_std\n",
        "    })\n",
        "\n",
        "# 全クラスまとめた対応のあるt検定\n",
        "all_slit_data = df_slit[layer_name].dropna().reset_index(drop=True)\n",
        "all_sumaho_data = df_sumaho[layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "# 対応のあるデータを取るため、最小の長さに合わせる\n",
        "min_length_all = min(len(all_slit_data), len(all_sumaho_data))\n",
        "all_slit_data = all_slit_data[:min_length_all]\n",
        "all_sumaho_data = all_sumaho_data[:min_length_all]\n",
        "\n",
        "t_stat_all, p_value_all = stats.ttest_rel(all_slit_data, all_sumaho_data)\n",
        "\n",
        "# 全クラスまとめた統計値\n",
        "all_slit_mean = all_slit_data.mean()\n",
        "all_slit_std = all_slit_data.std()\n",
        "all_sumaho_mean = all_sumaho_data.mean()\n",
        "all_sumaho_std = all_sumaho_data.std()\n",
        "\n",
        "# 結果を追加\n",
        "results.append({\n",
        "    'class_name': 'All Classes',\n",
        "    't_stat': t_stat_all,\n",
        "    'p_value': p_value_all,\n",
        "    'slit_mean': all_slit_mean,\n",
        "    'slit_std': all_slit_std,\n",
        "    'sumaho_mean': all_sumaho_mean,\n",
        "    'sumaho_std': all_sumaho_std\n",
        "})\n",
        "\n",
        "# 結果をデータフレームに変換して表示\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results['p_value'] = df_results['p_value'].map(lambda x: f'{x:.3f}')\n",
        "\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "fgFUt3AFWwvA",
        "outputId": "9bff106f-2bc6-41df-9ec0-8b355c5e7c1f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      class_name    t_stat p_value  slit_mean  slit_std  sumaho_mean  \\\n",
              "0      infection -2.010170   0.054   0.047514  0.024726     0.063889   \n",
              "1         normal -2.237214   0.035   0.024780  0.011687     0.037293   \n",
              "2  non-infection -0.991773   0.341   0.116942  0.075453     0.165322   \n",
              "3           scar -3.849386   0.001   0.073900  0.030724     0.135584   \n",
              "4          tumor -3.143958   0.004   0.112467  0.070307     0.165419   \n",
              "5        deposit -4.416907   0.000   0.036668  0.025242     0.084672   \n",
              "6           APAC -0.926550   0.397   0.057955  0.041766     0.079910   \n",
              "7   lens opacity -2.979324   0.008   0.072591  0.040474     0.131192   \n",
              "8        bullous -1.443593   0.164   0.066964  0.024595     0.082440   \n",
              "9    All Classes -6.956735   0.000   0.064128  0.048114     0.104198   \n",
              "\n",
              "   sumaho_std  \n",
              "0    0.037739  \n",
              "1    0.024043  \n",
              "2    0.135195  \n",
              "3    0.084415  \n",
              "4    0.065204  \n",
              "5    0.058418  \n",
              "6    0.030570  \n",
              "7    0.067411  \n",
              "8    0.040750  \n",
              "9    0.079396  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d107d54a-6238-41c4-883a-266dde19bdcb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class_name</th>\n",
              "      <th>t_stat</th>\n",
              "      <th>p_value</th>\n",
              "      <th>slit_mean</th>\n",
              "      <th>slit_std</th>\n",
              "      <th>sumaho_mean</th>\n",
              "      <th>sumaho_std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>infection</td>\n",
              "      <td>-2.010170</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.047514</td>\n",
              "      <td>0.024726</td>\n",
              "      <td>0.063889</td>\n",
              "      <td>0.037739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>normal</td>\n",
              "      <td>-2.237214</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.024780</td>\n",
              "      <td>0.011687</td>\n",
              "      <td>0.037293</td>\n",
              "      <td>0.024043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>non-infection</td>\n",
              "      <td>-0.991773</td>\n",
              "      <td>0.341</td>\n",
              "      <td>0.116942</td>\n",
              "      <td>0.075453</td>\n",
              "      <td>0.165322</td>\n",
              "      <td>0.135195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>scar</td>\n",
              "      <td>-3.849386</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.073900</td>\n",
              "      <td>0.030724</td>\n",
              "      <td>0.135584</td>\n",
              "      <td>0.084415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tumor</td>\n",
              "      <td>-3.143958</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.112467</td>\n",
              "      <td>0.070307</td>\n",
              "      <td>0.165419</td>\n",
              "      <td>0.065204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>deposit</td>\n",
              "      <td>-4.416907</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.036668</td>\n",
              "      <td>0.025242</td>\n",
              "      <td>0.084672</td>\n",
              "      <td>0.058418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>APAC</td>\n",
              "      <td>-0.926550</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.057955</td>\n",
              "      <td>0.041766</td>\n",
              "      <td>0.079910</td>\n",
              "      <td>0.030570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>lens opacity</td>\n",
              "      <td>-2.979324</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.072591</td>\n",
              "      <td>0.040474</td>\n",
              "      <td>0.131192</td>\n",
              "      <td>0.067411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bullous</td>\n",
              "      <td>-1.443593</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.066964</td>\n",
              "      <td>0.024595</td>\n",
              "      <td>0.082440</td>\n",
              "      <td>0.040750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>All Classes</td>\n",
              "      <td>-6.956735</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.064128</td>\n",
              "      <td>0.048114</td>\n",
              "      <td>0.104198</td>\n",
              "      <td>0.079396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d107d54a-6238-41c4-883a-266dde19bdcb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d107d54a-6238-41c4-883a-266dde19bdcb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d107d54a-6238-41c4-883a-266dde19bdcb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-69a985ea-33d1-4af4-993f-1a338f043799\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69a985ea-33d1-4af4-993f-1a338f043799')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-69a985ea-33d1-4af4-993f-1a338f043799 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_09498caa-c169-42f6-81e9-b945731909d5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_09498caa-c169-42f6-81e9-b945731909d5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"class_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"bullous\",\n          \"normal\",\n          \"deposit\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"t_stat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8443032979200138,\n        \"min\": -6.956735284813072,\n        \"max\": -0.9265500391021524,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          -1.4435931481564337,\n          -2.2372139729762908,\n          -4.416906938645566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_value\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"0.008\",\n          \"0.035\",\n          \"0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slit_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02946012295598514,\n        \"min\": 0.024780148111130596,\n        \"max\": 0.1169416670533802,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.06696438969431771,\n          0.024780148111130596,\n          0.036667667861288264\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slit_std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.020590952465214165,\n        \"min\": 0.011687291764271645,\n        \"max\": 0.0754529486802198,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.024594968056196008,\n          0.011687291764271645,\n          0.025241872403126413\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sumaho_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.043124223932540375,\n        \"min\": 0.037293498249458594,\n        \"max\": 0.16541944816259196,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.08243997104141557,\n          0.037293498249458594,\n          0.08467166181747428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sumaho_std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03280018295944441,\n        \"min\": 0.024042996876849975,\n        \"max\": 0.1351946759225835,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.040749667728648895,\n          0.024042996876849975,\n          0.05841844058793129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ANOVAを実行\n",
        "anova_result = stats.f_oneway(\n",
        "    df_combined[df_combined['class_name'] == 'infection'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'normal'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'non-infection'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'scar'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'tumor'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'deposit'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'APAC'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'lens opacity'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'bullous'][layer_name]\n",
        ")\n",
        "\n",
        "print(f\"ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(df_combined[layer_name], df_combined['class_name'])\n",
        "\n",
        "# 結果を表示\n",
        "print(tukey_result)"
      ],
      "metadata": {
        "id": "LjAiQjnMYHjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ANOVA_heatmap**"
      ],
      "metadata": {
        "id": "59zc1fuTqBK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'})\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "exil8PT2ZzsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# カスタムカラーマップを作成（オレンジ、グレー、白）\n",
        "colors = ['#FFA500', '#D3D3D3', 'white']  # オレンジ、グレー、白\n",
        "custom_cmap = ListedColormap(colors)\n",
        "\n",
        "# p値に基づいてデータを作成（NaNは2、p<0.05は0、それ以外は1）\n",
        "color_data = np.where(np.isnan(matrix), 2, np.where(matrix < 0.05, 0, 1))\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(color_data,\n",
        "            annot=matrix,  # 元のp値を表示\n",
        "            fmt=\".3f\",\n",
        "            cmap=custom_cmap,\n",
        "            cbar=False,  # カラーバーを非表示に\n",
        "            xticklabels=class_names,  # x軸のラベルをクラス名に設定\n",
        "            yticklabels=class_names)  # y軸のラベルをクラス名に設定\n",
        "\n",
        "# カスタムカラーバーを追加\n",
        "sm = plt.cm.ScalarMappable(cmap=ListedColormap(colors[:2]), norm=plt.Normalize(vmin=0, vmax=1))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, label='p-value', ticks=[0.25, 0.75])\n",
        "cbar.set_ticklabels(['p < 0.05', 'p ≥ 0.05'])\n",
        "\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.xticks(rotation=45, ha='right')  # x軸のラベルを45度回転\n",
        "plt.yticks(rotation=0)  # y軸のラベルを水平に\n",
        "plt.tight_layout()  # レイアウトを調整\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W6M5cbfdFlv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tukey_sumaho/slit別**"
      ],
      "metadata": {
        "id": "XbxW3P2OqFTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "def perform_anova_and_tukey(df, title):\n",
        "    # 欠損値を削除\n",
        "    df = df.dropna(subset=[layer_name])\n",
        "\n",
        "    # ANOVAを実行\n",
        "    anova_result = stats.f_oneway(\n",
        "        df[df['class_name'] == 'infection'][layer_name],\n",
        "        df[df['class_name'] == 'normal'][layer_name],\n",
        "        df[df['class_name'] == 'non-infection'][layer_name],\n",
        "        df[df['class_name'] == 'scar'][layer_name],\n",
        "        df[df['class_name'] == 'tumor'][layer_name],\n",
        "        df[df['class_name'] == 'deposit'][layer_name],\n",
        "        df[df['class_name'] == 'APAC'][layer_name],\n",
        "        df[df['class_name'] == 'lens opacity'][layer_name],\n",
        "        df[df['class_name'] == 'bullous'][layer_name]\n",
        "    )\n",
        "\n",
        "    print(f\"{title} ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "    # 事後検定（TukeyのHSD検定）を実行\n",
        "    tukey_result = pairwise_tukeyhsd(endog=df[layer_name], groups=df['class_name'])\n",
        "\n",
        "    # 結果をDataFrameに変換\n",
        "    tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "    # マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "    matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "    # マトリックスにp値を入力\n",
        "    for i in range(len(tukey_result_df)):\n",
        "        group1 = tukey_result_df.loc[i, 'group1']\n",
        "        group2 = tukey_result_df.loc[i, 'group2']\n",
        "        p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "        matrix.loc[group1, group2] = p_value\n",
        "        matrix.loc[group2, group1] = p_value\n",
        "\n",
        "    # 同じクラス間のセルをNaNに設定\n",
        "    np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "    # ヒートマップを描画\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'}, mask=matrix.isnull())\n",
        "    plt.title(f'Pairwise Comparison P-Values (Tukey HSD) - {title}')\n",
        "    plt.show()\n",
        "\n",
        "# スリットランプデータの解析\n",
        "perform_anova_and_tukey(df_slit, \"Slit Lamp Data\")\n",
        "\n",
        "# スマートフォンデータの解析\n",
        "perform_anova_and_tukey(df_sumaho, \"Smartphone Data\")"
      ],
      "metadata": {
        "id": "8m7Z9FCzbSmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 両方のデータを結合\n",
        "df_combined = pd.concat([df_slit, df_sumaho]).reset_index(drop=True)\n",
        "\n",
        "# \"scar\" + \"non-infection\" グループとその他のクラスに分類\n",
        "df_combined['group'] = df_combined['class_name'].apply(lambda x: 'scar + non-infection' if x in ['scar', 'non-infection'] else 'others')\n",
        "\n",
        "# グループごとにデータを抽出\n",
        "scar_non_infection_group = df_combined[df_combined['group'] == 'scar + non-infection'][layer_name].dropna()\n",
        "others_group = df_combined[df_combined['group'] == 'others'][layer_name].dropna()\n",
        "\n",
        "# t検定を実行\n",
        "t_stat, p_value = stats.ttest_ind(scar_non_infection_group, others_group)\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
        "\n",
        "# 統計値を計算\n",
        "scar_non_infection_mean = scar_non_infection_group.mean()\n",
        "scar_non_infection_std = scar_non_infection_group.std()\n",
        "others_mean = others_group.mean()\n",
        "others_std = others_group.std()\n",
        "\n",
        "print(f\"scar + non-infection mean: {scar_non_infection_mean}, std: {scar_non_infection_std}\")\n",
        "print(f\"others mean: {others_mean}, std: {others_std}\")\n"
      ],
      "metadata": {
        "id": "-ubPn9Erqnid",
        "outputId": "a4e19c97-a3c2-44cd-9dac-99c4b7acdb75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t-statistic: 5.652411183767835, p-value: 2.8095252129652275e-08\n",
            "scar + non-infection mean: 0.11625931718387256, std: 0.08804314854287512\n",
            "others mean: 0.07454914396969002, std: 0.05818428866497734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcxqbJ_GrbPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}