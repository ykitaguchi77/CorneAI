{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOUEByUKQVfxCQp13fbwjGA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/yolov5_gradCAM_corneAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_GradCAM_CorneAI**\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "id": "2cakhs2BZLRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "ytBOWsuXZQzv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p48tU-_wYVHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530c9ffd-1ee5-48ce-8e2c-8f8ae4e830af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/pooya-mohammadi/deep_utils.git\n",
            "  Cloning https://github.com/pooya-mohammadi/deep_utils.git to /tmp/pip-req-build-7f00sh4q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pooya-mohammadi/deep_utils.git /tmp/pip-req-build-7f00sh4q\n",
            "  Resolved https://github.com/pooya-mohammadi/deep_utils.git to commit 676177f45cab804253103a03cdbc8133f8580ed6\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.30) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.30) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.30) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.30) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.30) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.30) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.30) (2024.2.2)\n",
            "Building wheels for collected packages: deep-utils\n",
            "  Building wheel for deep-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deep-utils: filename=deep_utils-1.3.30-py3-none-any.whl size=534410 sha256=f61d187521263c51f977e24cd31e8e5f39d2735eff87258132eb02d7de00316e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a13c_wyv/wheels/8f/0a/f4/5e2b92d9573699e3e30ce319a4b06218eb281695935d0b8b54\n",
            "Successfully built deep-utils\n",
            "Installing collected packages: deep-utils\n",
            "Successfully installed deep-utils-1.3.30\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.10.0.82-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "Successfully installed opencv-python-4.10.0.82\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 15.17 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install -U opencv-python\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ],
      "metadata": {
        "id": "wkvc2KijYes5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "wSF89NC7aTCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c46b717-6a1d-45e5-d454-dfe69683c350"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --model-path $model_path --img-path $img_path --output-dir out"
      ],
      "metadata": {
        "id": "cjgRkuSuYinZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**単一画像解析**"
      ],
      "metadata": {
        "id": "3xi7jKIs9lLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "        print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_img: input image with shape of (1, 3, H, W)\n",
        "        Return:\n",
        "            mask: saliency map of the same spatial dimension with input\n",
        "            logit: model output\n",
        "            preds: The object predictions\n",
        "        \"\"\"\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        tic = time.time()\n",
        "        preds, logits = self.model(input_img)\n",
        "        print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "        #for logit, cls, cls_name in zip(logits[0], preds[1][0], preds[2][0]):\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            print(cls_name)\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "            self.model.zero_grad()\n",
        "            tic = time.time()\n",
        "            score.backward(retain_graph=True)\n",
        "            print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            weights = alpha.view(b, k, 1, 1)\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.upsample(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "        return saliency_maps, logits, preds\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)"
      ],
      "metadata": {
        "id": "9wgOFlwZqznN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from deep_utils import Box, split_extension\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/32.jpg\"\n",
        "output_dir = 'out'  # 出力ディレクトリ\n",
        "img_size = 640  # 入力画像サイズ\n",
        "target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "def get_res_img(bbox, mask, res_img):\n",
        "    mask = mask.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy().astype(\n",
        "        np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "    res_img = res_img / 255\n",
        "    res_img = cv2.add(res_img, n_heatmat)\n",
        "    res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "    res_img = Box.put_text(res_img, cls_name, (x1, y1))\n",
        "    return res_img\n",
        "\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "\n",
        "def main(img_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    print('[INFO] Loading the model')\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                      names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, class_names, _] = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "    for i, mask in enumerate(masks):\n",
        "        res_img = result.copy()\n",
        "        bbox, cls_name = boxes[0][i], class_names[0][i]\n",
        "        res_img, heat_map = get_res_img(bbox, mask, res_img)\n",
        "        res_img = put_text_box(bbox, cls_name, res_img)\n",
        "        images.append(res_img)\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    print('[INFO] Loading the model')\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                      classes=classes)\n",
        "    for item in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img = cv2.imread(img_path)\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "        if method == 'gradcam':\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, class_names, _] = saliency_method(torch_img)\n",
        "        print(\"total time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "        for i, mask in enumerate(masks):\n",
        "            res_img = result.copy()\n",
        "            bbox, cls_name = boxes[0][i], class_names[0][i]\n",
        "            res_img, heat_map = get_res_img(bbox, mask, res_img)\n",
        "            res_img = put_text_box(bbox, cls_name, res_img)\n",
        "            images.append(res_img)\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCeqKWOxc9as",
        "outputId": "06c57668-eab0-4fdb-dda9-0f99d5f958a9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading the model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] saliency_map size : torch.Size([20, 20])\n",
            "[INFO] model-forward took:  0.1295 seconds\n",
            "normal\n",
            "[INFO] normal, model-backward took:  0.1786 seconds\n",
            "total time: 0.3135\n",
            "[INFO] Saving the final image at out/32-res.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSOlcWq6SqqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Top3 analysis**"
      ],
      "metadata": {
        "id": "wvBLe9pbpCwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "# def put_text_box(bbox, cls_name, res_img):\n",
        "#     x1, y1, x2, y2 = bbox\n",
        "#     # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "#     cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "#     res_img = cv2.imread('temp.jpg')\n",
        "#     res_img = Box.put_box(res_img, bbox)\n",
        "#     res_img = Box.put_text(res_img, cls_name, (x1, y1))\n",
        "#     return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # Set the font size and other parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    # Calculate the text size and position\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10  # Adjust the horizontal position to place the text near the left edge\n",
        "    text_y = text_size[1] + 10  # Adjust the vertical position to place the text near the top edge\n",
        "\n",
        "    # Put the text on the image\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "        print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        tic = time.time()\n",
        "        preds, logits = self.model(input_img)\n",
        "        print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "        print(f\"preds: {preds}\")\n",
        "        print(f\"logits[0]: {logits[0]}\")\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "        preds[1][0] = top3_indices.tolist()[0]\n",
        "        preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "        print(f\"preds[1][0]: {preds[1][0]}\")\n",
        "        print(f\"preds[2][0]: {preds[2][0]}\")\n",
        "\n",
        "        self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            print(f\"cls_name: {cls_name}\")\n",
        "\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "            self.model.zero_grad()\n",
        "            tic = time.time()\n",
        "            score.backward(retain_graph=True)\n",
        "            print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            weights = alpha.view(b, k, 1, 1)\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.upsample(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names  # 修正: cls_names を返り値に追加\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)"
      ],
      "metadata": {
        "id": "FaHa-Huy982m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from deep_utils import Box, split_extension\n",
        "\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/7.jpg\"\n",
        "# output_dir = 'out'  # 出力ディレクトリ\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img'\n",
        "img_size = 640  # 入力画像サイズ\n",
        "target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "#ここからがメイン\n",
        "input_size = (img_size, img_size)\n",
        "img = cv2.imread(img_path)\n",
        "print('[INFO] Loading the model')\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                  names=names)\n",
        "torch_img = model.preprocessing(img[..., ::-1])\n",
        "if method == 'gradcam':\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "tic = time.time()\n",
        "\n",
        "masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "print(F\"cls_names: {cls_names}\")\n",
        "print(\"total time:\", round(time.time() - tic, 4))\n",
        "result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "result = result[..., ::-1]  # convert to bgr\n",
        "images = [result]\n",
        "\n",
        "for i in range(len(masks)):\n",
        "    res_img = result.copy()\n",
        "    h, w, _ = res_img.shape  # 画像の高さと幅を取得\n",
        "    for j, mask in enumerate(masks[i]):\n",
        "        bbox = boxes[0][j]\n",
        "        bbox = [max(0, coord) for coord in bbox]  # negative座標を0に変更\n",
        "        bbox = [min(coord, max_val) for coord, max_val in zip(bbox, [w, h, w, h])]  # 画像サイズを超える座標を画像サイズに合わせる\n",
        "        res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "        res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "    images.append(res_img)\n",
        "\n",
        "final_image = concat_images(images)\n",
        "img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "output_path = f'{output_dir}/{img_name}'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f'[INFO] Saving the final image at {output_path}')\n",
        "cv2.imwrite(output_path, final_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk-7cy_5984W",
        "outputId": "5d5fcd06-c4f7-414f-f74c-5eef8d20b5c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading the model\n",
            "[INFO] Model is loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] saliency_map size : torch.Size([20, 20])\n",
            "[INFO] model-forward took:  0.1345 seconds\n",
            "preds: [[[[58, 1, 470, 575]]], [[1]], [['normal']], [[0.75]]]\n",
            "logits[0]: tensor([[-3.57789,  1.37006, -4.83992, -3.20287, -2.91802, -5.00547, -7.60832, -5.65962, -7.46785]], grad_fn=<IndexBackward0>)\n",
            "preds[1][0]: [1, 4, 3]\n",
            "preds[2][0]: ['normal', 'tumor', 'scar']\n",
            "cls_name: normal\n",
            "[INFO] normal, model-backward took:  0.1932 seconds\n",
            "cls_name: tumor\n",
            "[INFO] tumor, model-backward took:  0.142 seconds\n",
            "cls_name: scar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] scar, model-backward took:  0.1417 seconds\n",
            "cls_names: ['normal', 'tumor', 'scar']\n",
            "total time: 0.6287\n",
            "[INFO] Saving the final image at /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img/7-res.jpg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BADdoz9sjIAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.4,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.6, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img"
      ],
      "metadata": {
        "id": "rMIlXO_mIun8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "output_dir = 'out'  # 出力ディレクトリ\n",
        "img_size = 640  # 入力画像サイズ\n",
        "target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "# def put_text_box(bbox, cls_name, res_img):\n",
        "#     x1, y1, x2, y2 = bbox\n",
        "#     # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "#     cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "#     res_img = cv2.imread('temp.jpg')\n",
        "#     res_img = Box.put_box(res_img, bbox)\n",
        "#     res_img = Box.put_text(res_img, cls_name, (x1, y1))\n",
        "#     return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # Set the font size and other parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    # Calculate the text size and position\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10  # Adjust the horizontal position to place the text near the left edge\n",
        "    text_y = text_size[1] + 10  # Adjust the vertical position to place the text near the top edge\n",
        "\n",
        "    # Put the text on the image\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []  # 追加: Top1~3のクラス名を保存するリスト\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "        # print('[INFO] saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        tic = time.time()\n",
        "        preds, logits = self.model(input_img)\n",
        "        # print(\"[INFO] model-forward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "        preds[1][0] = top3_indices.tolist()[0]\n",
        "        preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "\n",
        "        self.cls_names = preds[2][0]  # 修正: Top1~3のクラス名を保存\n",
        "\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "            self.model.zero_grad()\n",
        "            tic = time.time()\n",
        "            score.backward(retain_graph=True)\n",
        "            # print(f\"[INFO] {cls_name}, model-backward took: \", round(time.time() - tic, 4), 'seconds')\n",
        "            gradients = self.gradients['value']\n",
        "            activations = self.activations['value']\n",
        "            b, k, u, v = gradients.size()\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            weights = alpha.view(b, k, 1, 1)\n",
        "            saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "            saliency_map = F.relu(saliency_map)\n",
        "            saliency_map = F.upsample(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "            saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names  # 修正: cls_names を返り値に追加\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "def main(img_path):\n",
        "   input_size = (img_size, img_size)\n",
        "   img = cv2.imread(img_path)\n",
        "  #  print('[INFO] Loading the model')\n",
        "   model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                     names=names)\n",
        "   torch_img = model.preprocessing(img[..., ::-1])\n",
        "   if method == 'gradcam':\n",
        "       saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "   tic = time.time()\n",
        "   masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "   print(\"total time:\", round(time.time() - tic, 4))\n",
        "   result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "   result = result[..., ::-1]  # convert to bgr\n",
        "   images = [result]\n",
        "\n",
        "   for i in range(len(masks)):\n",
        "       res_img = result.copy()\n",
        "       for j, mask in enumerate(masks[i]):\n",
        "           bbox = boxes[0][j]\n",
        "           res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "           res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "       images.append(res_img)\n",
        "\n",
        "   final_image = concat_images(images)\n",
        "   img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "   output_path = f'{output_dir}/{img_name}'\n",
        "   os.makedirs(output_dir, exist_ok=True)\n",
        "   print(f'[INFO] Saving the final image at {output_path}')\n",
        "   cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "   input_size = (img_size, img_size)\n",
        "  #  print('[INFO] Loading the model')\n",
        "   model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                     names=names)\n",
        "\n",
        "   for item in os.listdir(folder_path):\n",
        "       img_path = os.path.join(folder_path, item)\n",
        "       img_basename = os.path.basename(img_path)\n",
        "       print(f\"Processing image: {img_basename}\")  # 追加: 画像のファイル名を表示\n",
        "\n",
        "       img = cv2.imread(img_path)\n",
        "       torch_img = model.preprocessing(img[..., ::-1])\n",
        "       if method == 'gradcam':\n",
        "           saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "       tic = time.time()\n",
        "       masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "       print(f\"boxes: {boxes}\")\n",
        "      #  # boxes[0][0][0]とboxes[0][0][1]が0未満の場合は0に修正\n",
        "      #  boxes[0][0][0] = max(1, boxes[0][0][0])\n",
        "      #  boxes[0][0][1] = max(1, boxes[0][0][1])\n",
        "\n",
        "      #  # boxes[0][0][2]とboxes[0][0][3]が640より大きい場合は640に修正\n",
        "      #  boxes[0][0][2] = min(639, boxes[0][0][2])\n",
        "      #  boxes[0][0][3] = min(639, boxes[0][0][3])\n",
        "\n",
        "       #  print(\"total time:\", round(time.time() - tic, 4))\n",
        "       result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "       result = result[..., ::-1]  # convert to bgr\n",
        "       images = [result]\n",
        "\n",
        "       for i in range(len(masks)):\n",
        "           res_img = result.copy()\n",
        "           for j, mask in enumerate(masks[i]):\n",
        "               bbox = boxes[0][j]\n",
        "               res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "               res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "           images.append(res_img)\n",
        "\n",
        "       final_image = concat_images(images)\n",
        "       img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "       output_path = f'{output_dir}/{img_name}'\n",
        "       os.makedirs(output_dir, exist_ok=True)\n",
        "       print(f'[INFO] Saving the final image at {output_path}')\n",
        "       cv2.imwrite(output_path, final_image)\n",
        "\n",
        "           # オブジェクトを明示的に削除\n",
        "       del saliency_method\n",
        "       del masks\n",
        "       del logits\n",
        "       del boxes\n",
        "       del images\n",
        "       del final_image\n",
        "       gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   if os.path.isdir(img_path):\n",
        "       folder_main(img_path)\n",
        "   else:\n",
        "       main(img_path)"
      ],
      "metadata": {
        "id": "6hBb7eKP986c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c41c5ce3-fd46-437d-f4ec-5a8f6df723cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n",
            "Processing image: 141.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[60, 223, 315, 550]]]\n",
            "[INFO] Saving the final image at out/141-res.jpg\n",
            "Processing image: 30.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[40, 25, 479, 599]]]\n",
            "[INFO] Saving the final image at out/30-res.jpg\n",
            "Processing image: 26.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[52, 80, 449, 640]]]\n",
            "[INFO] Saving the final image at out/26-res.jpg\n",
            "Processing image: 123.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[92, 95, 367, 612]]]\n",
            "[INFO] Saving the final image at out/123-res.jpg\n",
            "Processing image: 159.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[1, 61, 453, 561]]]\n",
            "[INFO] Saving the final image at out/159-res.jpg\n",
            "Processing image: 62.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[17, 187, 404, 607]]]\n",
            "[INFO] Saving the final image at out/62-res.jpg\n",
            "Processing image: 169.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[60, 93, 397, 633]]]\n",
            "[INFO] Saving the final image at out/169-res.jpg\n",
            "Processing image: 164.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[96, 79, 410, 609]]]\n",
            "[INFO] Saving the final image at out/164-res.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: 39.jpg\n",
            "boxes: [[[0, 0, 475, 598]]]\n",
            "[INFO] Saving the final image at out/39-res.jpg\n",
            "Processing image: 19.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[4, 81, 482, 639], [5, 84, 482, 638]]]\n",
            "[INFO] Saving the final image at out/19-res.jpg\n",
            "Processing image: 135.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[60, 188, 329, 486]]]\n",
            "[INFO] Saving the final image at out/135-res.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: 58.jpg\n",
            "boxes: [[[20, 140, 400, 575]]]\n",
            "[INFO] Saving the final image at out/58-res.jpg\n",
            "Processing image: 157.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[44, 57, 425, 596]]]\n",
            "[INFO] Saving the final image at out/157-res.jpg\n",
            "Processing image: 34.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[39, 91, 455, 629]]]\n",
            "[INFO] Saving the final image at out/34-res.jpg\n",
            "Processing image: 64.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxes: [[[16, 176, 382, 582]]]\n",
            "[INFO] Saving the final image at out/64-res.jpg\n",
            "Processing image: 31.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2f1ce90208c1>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m        \u001b[0mfolder_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m        \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2f1ce90208c1>\u001b[0m in \u001b[0;36mfolder_main\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m    206\u001b[0m            \u001b[0msaliency_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOV5GradCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m        \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m        \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaliency_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"boxes: {boxes}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;31m#  # boxes[0][0][0]とboxes[0][0][1]が0未満の場合は0に修正\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2f1ce90208c1>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_img)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaliency_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_names\u001b[0m  \u001b[0;31m# 修正: cls_names を返り値に追加\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2f1ce90208c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_img, class_idx)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop3_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop3_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do\n",
        "\n",
        "/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/31.jpg\n",
        "の判定がつかない問題"
      ],
      "metadata": {
        "id": "zLCPCYqFSuxu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NaPJiG93BbK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8w4ftJHBbMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OIX5AxMBbOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ItKeg14pBbQe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}