{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/yolov5_gradCAM_corneAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42M6k9QpvSq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cakhs2BZLRA"
      },
      "source": [
        "#**YOLOv5_GradCAM_CorneAI**\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytBOWsuXZQzv"
      },
      "source": [
        "##**Setup YOLOv5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7oJE1rLrE_4",
        "outputId": "b951b52d-3591-4a06-db53-96f237dc3561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p48tU-_wYVHS",
        "outputId": "42ea2e8b-d022-4574-d4e7-5f370276ab8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Found existing installation: deep_utils 1.3.61\n",
            "Uninstalling deep_utils-1.3.61:\n",
            "  Successfully uninstalled deep_utils-1.3.61\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "fatal: destination path 'yolov5-gradcam' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkvc2KijYes5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjgRkuSuYinZ"
      },
      "outputs": [],
      "source": [
        "# !python main.py --model-path $model_path --img-path $img_path --output-dir out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvBLe9pbpCwd"
      },
      "source": [
        "#**Top3 analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWizQkPAjczd",
        "outputId": "98ebdef3-95b8-489c-ef56-927f327cf4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "234\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit\"\n",
        "print(len(os.listdir(img_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN98dpHRuyNV",
        "outputId": "1c32ecff-f6bc-493e-d360-42793dcbd660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMIlXO_mIun8"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoTET4OvDiRo"
      },
      "source": [
        "#**GradCAM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hBb7eKP986c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "GradCAM通常バージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "# 以下の関数は変更なし\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "# YOLOV5GradCAMクラスを更新\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []  # クラス名を格納するリストを初期化\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        return alpha.view(b, k, 1, 1)\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)\n",
        "        weights = (positive_gradients * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        return eigen_cam.unsqueeze(1)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[200:234]\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        #cv2.imwrite(output_path, final_image)\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー →Best\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    ##target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    ##target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "    #for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojsVa-JDefA4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "GradCAM\n",
        "広く見るバージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[210:211] ###########画像を指定\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img = apply_heatmap_to_entire_image(masks[i], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS-LWXlVgFpI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def print_model_structure(model):\n",
        "    \"\"\"\n",
        "    モデルの構造を表示する関数\n",
        "    \"\"\"\n",
        "    print(\"Model structure:\")\n",
        "    for i, (name, module) in enumerate(model.model.named_modules()):\n",
        "        if not isinstance(module, torch.nn.Sequential) and not isinstance(module, torch.nn.ModuleList):\n",
        "            print(f\"{i}: {name} - {module.__class__.__name__}\")\n",
        "\n",
        "def find_yolo_layer(model, layer_index):\n",
        "    \"\"\"\n",
        "    YOLOv5モデルの後ろから指定されたインデックスのレイヤーを返す\n",
        "    \"\"\"\n",
        "    module_list = list(model.model.modules())\n",
        "    module_list = [m for m in module_list if not isinstance(m, torch.nn.Sequential) and not isinstance(m, torch.nn.ModuleList)]\n",
        "    target_layer = module_list[-layer_index]\n",
        "    return target_layer, f\"{target_layer.__class__.__name__} (index: -{layer_index})\"\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_index, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer, layer_name = find_yolo_layer(self.model, layer_index)\n",
        "        self.layer_name = layer_name\n",
        "        print(f\"Selected layer: {self.layer_name}\")\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method, saliency_method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    print(f\"Using layer: {saliency_method.layer_name}\")\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[200:234]\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img = apply_heatmap_to_entire_image(masks[i], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    layer_index = 1  # モデルの後ろから1番目のレイヤーを選択\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=(img_size, img_size), names=names)\n",
        "\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_index=layer_index, img_size=(img_size, img_size), method=method)\n",
        "            folder_main(img_path, method, saliency_method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RmQue4SefDJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiAwrM4475X1"
      },
      "source": [
        "###**EigenCAM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KXqjIkf1yg9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        # GradCAMの重み計算\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        # GradCAM++の重み計算\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[6:15]\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        #cv2.imwrite(output_path, final_image)\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_0'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['eigencam']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3G5YQn4dBm_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRvM0njcdBov"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "EigenCAM\n",
        "広く可視化するバージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        res_img = apply_heatmap_to_entire_image(mask, res_img)\n",
        "    return res_img, None\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[6:15]\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img, _ = get_res_img(None, [masks[i]], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "    for method in ['eigencam']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2RWDTh3dBqq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_y_jlLBdBsp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsuMkl6BdBus"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-Ydkg4tdBwr"
      },
      "outputs": [],
      "source": [
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=(img_size, img_size), names=names)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFDwQ_bfIYaU"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDjAHX8v7tS6"
      },
      "source": [
        "#**LayerごとTop1の可視化**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HR__A8-laC1",
        "outputId": "c721eb98-96ef-47d6-a748-9c20c7c6c9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjy3aEbDlaJi",
        "outputId": "420cb691-2489-4d9c-f367-59fe4ba791fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 17.96 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTGwuv3Ql1xL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwry3nj9B9ub"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                          'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "                          'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
        "                          'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "                          'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "                          'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "# def get_res_img(bbox, masks, res_img):\n",
        "#     for mask in masks:\n",
        "#         # NaNやInfを0に置き換え\n",
        "#         mask = torch.nan_to_num(mask.squeeze(), nan=0.0, posinf=1.0, neginf=0.0)\n",
        "#         # スケーリングと型変換\n",
        "#         mask = mask.mul(255).add_(0.5).clamp_(0, 255)\n",
        "#         mask = mask.detach().cpu().numpy()\n",
        "#         # 明示的に型変換\n",
        "#         mask = np.clip(mask, 0, 255).astype(np.uint8)\n",
        "\n",
        "#         heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "#         n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "#         res_img = res_img / 255\n",
        "#         res_img = cv2.add(res_img, n_heatmat)\n",
        "#         res_img = (res_img / res_img.max())\n",
        "#     return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img): ##マージしているレイヤー名\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, f\"{cls_names[0]} - {method} - {layer_name}\", res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    #cv2_imshow(final_image)\n",
        "    cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path, method, file_list):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        ##target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv'] ##\n",
        "        #target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "        #target_layers = ['model_17_m_0_cv2_conv', 'model_20_m_0_cv2_conv', 'model_23_m_0_cv2_conv']\n",
        "        target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            #print(f\"boxes: {boxes}\")\n",
        "\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, f\"{cls_names[0]}-{'layer'+layer_name.replace('model_', '')}\", res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image) ##画像を表示\n",
        "        #cv2.imwrite(output_path, final_image) ##画像を保存\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer24'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho_layer24'  # 出力ディレクトリ\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer_m_0_2_cv2_conv'  # 出力ディレクトリ\n",
        "\n",
        "    # ファイル名を数字でソート\n",
        "    if os.path.isdir(img_path):\n",
        "        file_list = sorted(os.listdir(img_path), key=lambda x: int(x.split('.')[0]))\n",
        "        ##########必要に応じて、処理するファイルの範囲を指定########\n",
        "        file_list = file_list[240:270]\n",
        "        ############################################################\n",
        "\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method, file_list)\n",
        "        else:\n",
        "            main(img_path, method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "zEGo3RoJi1Xi",
        "outputId": "368e6858-3c18-4da1-96a1-4d2dcf7023fc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-35e5c8a5ab93>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
          ]
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbQBtqIHTaCf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvJ9wVXLUSov"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0HgW2xNUSrQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0FyVE0SKUStt"
      },
      "outputs": [],
      "source": [
        "#################################\n",
        "## 画像全体（bbox外）も表示\n",
        "#################################\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                          'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "                          'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
        "                          'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "                          'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "                          'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.5, n_heatmap, 0.5, 0)  ###ここでヒートマップと元画像のマージの比率を決める\n",
        "    return res_img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        res_img = apply_heatmap_to_entire_image(mask, res_img)\n",
        "    return res_img, None\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img): ##マージしているレイヤー名\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "\n",
        "    # バウンディングボックスを太く描画\n",
        "    thickness = 3  # 線の太さを増やす（必要に応じて調整してください）\n",
        "    res_img = Box.put_box(res_img, bbox, thickness=thickness)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    text_thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, text_thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, text_thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, f\"{cls_names[0]} - {method} - {layer_name}\", res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "    #cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path, method, file_list):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv'] ##\n",
        "        target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "        ##target_layers = ['model_17_m_0_cv2_conv', 'model_20_m_0_cv2_conv', 'model_23_m_0_cv2_conv']\n",
        "        #target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            print(f\"boxes: {boxes}\")\n",
        "\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, f\"{cls_names[0]}-{'layer'+layer_name.replace('model_', '')}\", res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image) ##画像を表示\n",
        "        #cv2.imwrite(output_path, final_image) ##画像を保存\n",
        "\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer24'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho_layer24'  # 出力ディレクトリ\n",
        "\n",
        "    # ファイル名を数字でソート\n",
        "    if os.path.isdir(img_path):\n",
        "        file_list = sorted(os.listdir(img_path), key=lambda x: int(x.split('.')[0]))\n",
        "        ##########必要に応じて、処理するファイルの範囲を指定########\n",
        "        file_list = file_list[205:208]\n",
        "        ############################################################\n",
        "\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method, file_list)\n",
        "        else:\n",
        "            main(img_path, method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaJLDNoaA8D1",
        "outputId": "e533cfd2-1039-4839-9b12-dbe2c4a1739e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "108"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###論文用composit画像書き出し（350dpi）"
      ],
      "metadata": {
        "id": "P-l1NPf1JvwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPJ1d31SasiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76577c40-060f-45aa-f1e1-9e2673a6eb27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5-gradcam/models/experimental.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(attempt_download(w), map_location=device)\n",
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRADCAMAPPを実行中:\n",
            "[INFO] Model is loaded\n",
            "Processing image 1: 35.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Unknown method 'gradcamapp'. Using default 'gradcam' method.\n",
            "Warning: Unknown method 'gradcamapp'. Using default 'gradcam' method.\n",
            "Warning: Unknown method 'gradcamapp'. Using default 'gradcam' method.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Unknown method 'gradcamapp'. Using default 'gradcam' method.\n",
            "Warning: Unknown method 'gradcamapp'. Using default 'gradcam' method.\n",
            "Warning: Unknown method 'gradcamapp'. Using default 'gradcam' method.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Unknown method 'gradcamapp'. Using default 'gradcam' method.\n",
            "Warning: Unknown method 'gradcamapp'. Using default 'gradcam' method.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# ★Pillow（PIL）を使うためのimport\n",
        "from PIL import Image\n",
        "\n",
        "os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                          'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "                          'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
        "                          'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "                          'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "                          'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0] if isinstance(self.img_size, tuple) else self.img_size, xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1] if isinstance(self.img_size, tuple) else self.img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    \"\"\"\n",
        "    Grad-CAM等のマスクをヒートマップ化して対象領域(bbox)に合成する。\n",
        "    \"\"\"\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "\n",
        "        # res_imgは [0,255] 範囲を想定 -> floatにして正規化\n",
        "        float_res = res_img.astype(np.float32) / 255.0\n",
        "        added = cv2.add(float_res, n_heatmat)\n",
        "        if added.max() > 0:  # 0除算回避\n",
        "            res_img = (added / added.max()) * 255.0\n",
        "        else:\n",
        "            res_img = added * 255.0\n",
        "        res_img = np.clip(res_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    \"\"\"\n",
        "    bboxに枠と文字を描画する。\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = bbox\n",
        "\n",
        "    # 枠を描画\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # 文字を描画\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # 緑色 (BGR)\n",
        "    thickness = 2\n",
        "\n",
        "    # テキストサイズ取得\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "    return res_img\n",
        "\n",
        "\n",
        "def concat_images(images):\n",
        "    \"\"\"\n",
        "    縦に画像を連結して1枚の画像にまとめる。\n",
        "    \"\"\"\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method.lower()  # Convert to lowercase to handle different casings\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # Get top 3 classes\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                # Initialize weights\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "                else:  # Default to gradcam if method is not recognized\n",
        "                    print(f\"Warning: Unknown method '{self.method}'. Using default 'gradcam' method.\")\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "\n",
        "                if weights is not None:  # Add safety check\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    if saliency_map_max > saliency_map_min:  # Avoid division by zero\n",
        "                        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        try:\n",
        "            alpha = gradients.view(b, k, -1).mean(2)\n",
        "            weights = alpha.view(b, k, 1, 1)\n",
        "            return weights\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _gradcam_weights: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        try:\n",
        "            alpha_num = gradients.pow(2)\n",
        "            alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "                activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "            alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "            alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "            relu_grad = F.relu(score.exp() * gradients)\n",
        "            weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "            return weights\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _gradcampp_weights: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "    #target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2']  # 対象レイヤー\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to BGR\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, f\"{cls_names[0]} - {method} - {layer_name}\", res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "\n",
        "    # 拡張子を \".png\" に統一\n",
        "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    img_name = f\"{base_name}-res-{method}.png\"\n",
        "    output_path = os.path.join(output_dir, img_name)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "    # OpenCV画像(BGR) → PIL画像(RGB) の変換\n",
        "    rgb_final_image = cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(rgb_final_image)\n",
        "    # PNG形式で保存 + DPI情報を付与\n",
        "    pil_image.save(output_path, dpi=(350, 350), format=\"PNG\")\n",
        "\n",
        "\n",
        "def folder_main(folder_path, method, file_list):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持したままリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングして640×640pxに整形\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(\n",
        "            resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
        "        )\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        #target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2']\n",
        "        #target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "        target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv'] ##\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # BGR\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, f\"{cls_names[0]}-{'layer'+layer_name.replace('model_', '')}\", res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "\n",
        "        # 拡張子を \".png\" に統一\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        img_name = f\"{base_name}-res-{method}.png\"\n",
        "        output_path = os.path.join(output_dir, img_name)\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        # PNG形式で350dpiを付与して保存\n",
        "        rgb_final_image = cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(rgb_final_image)\n",
        "        pil_image.save(output_path, dpi=(350, 350), format=\"PNG\")\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    output_dir = '/content/yolov5-gradcam'\n",
        "\n",
        "    # ファイル名を数字でソート\n",
        "    if os.path.isdir(img_path):\n",
        "        file_list = sorted(os.listdir(img_path), key=lambda x: int(x.split('.')[0]))\n",
        "        # 必要に応じてファイルの範囲を限定\n",
        "        #file_list = [file_list[i] for i in [220, 231, 229, 201, 226, 191, 60, 204, 40]] #番号-1する\n",
        "        file_list = [file_list[i] for i in [34]] #番号-1する\n",
        "\n",
        "\n",
        "    # 実行するCAM手法\n",
        "    for method in ['gradcamapp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method, file_list)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ0eRLpgoi_T"
      },
      "outputs": [],
      "source": [
        "input_size = (640,640)\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q81i6ptncfDC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hauGiazwcfG9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXvgJcOytU51"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHLMn6kZIlSG"
      },
      "source": [
        "#**Sort GradCam Images**\n",
        "\n",
        "GradCAM画像をクラスごとにフォルダ分け"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaYBsjL7NTcS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara234.csv\"\n",
        "\n",
        "# CSVファイルを読み込み\n",
        "df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
        "\n",
        "# データフレームの最初の数行を表示\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBll2vlDM26D",
        "outputId": "f238c4e9-9e95-44f6-cffc-3b90ec61e326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "Processing files from /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit to /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_slit_sorted\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 228/228 [00:02<00:00, 88.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing files from /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_sumaho to /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_sumaho_sorted\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 228/228 [00:02<00:00, 89.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files have been copied successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dirs = [\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit',\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_sumaho'\n",
        "]\n",
        "destination_base_dirs = [\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_slit_sorted',\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_sumaho_sorted'\n",
        "]\n",
        "\n",
        "# Iterate over the source and destination directory pairs\n",
        "for source_dir, destination_base_dir in zip(source_dirs, destination_base_dirs):\n",
        "    print(f'Processing files from {source_dir} to {destination_base_dir}')\n",
        "    # Iterate over the dataframe and copy files to the appropriate class folder\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        serial_number = row['serial_number']\n",
        "        class_num = row['class_num']\n",
        "        source_file = os.path.join(source_dir, f'{serial_number}-res.jpg')\n",
        "        destination_dir = os.path.join(destination_base_dir, str(class_num))\n",
        "\n",
        "        # Create the destination directory if it does not exist\n",
        "        os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "        # Copy the file to the destination directory\n",
        "        destination_file = os.path.join(destination_dir, f'{serial_number}-res.jpg')\n",
        "        if os.path.exists(source_file):\n",
        "            shutil.copy(source_file, destination_file)\n",
        "        else:\n",
        "            print(f'File {source_file} does not exist.')\n",
        "\n",
        "print('Files have been copied successfully.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxtwlXulSgO9"
      },
      "source": [
        "#**Compare area of interest**\n",
        "\n",
        "注目度n%以上の画像を"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lQctr8O90Qp"
      },
      "source": [
        "###**ニ値化して表示**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VllpVTyg9zYN"
      },
      "source": [
        "###画像として表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iDm_dPKM28s"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV90u_Z9Urn0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "#img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "#output_dir = 'out'  # 出力ディレクトリ\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "img_size = 640  # 入力画像サイズ\n",
        "target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_res_img(bbox, masks, res_img, threshold):\n",
        "    total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        mask[mask < threshold*255] = 0  # 追加: 128未満の値を0にする\n",
        "        mask[mask >= threshold*255] = 255  # 追加: 128未満の値を0にする\n",
        "        binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "\n",
        "        # bboxの範囲内のマスクの部分を取得\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "        # 閾値を超える共通部分のピクセル数をカウント\n",
        "        intersect_pixels = np.sum(mask_bbox)\n",
        "        total_intersect_pixels += intersect_pixels\n",
        "\n",
        "        # mask_bbox のピクセル数を取得\n",
        "        mask_bbox_area = mask_bbox.size\n",
        "\n",
        "        # AOI を計算\n",
        "        AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "        print(f\"Area of Interest (AOI): {AOI}\")\n",
        "\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # Set the font size and other parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    # Calculate the text size and position\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10  # Adjust the horizontal position to place the text near the left edge\n",
        "    text_y = text_size[1] + 10  # Adjust the vertical position to place the text near the top edge\n",
        "\n",
        "    # Put the text on the image\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "def main(img_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                      names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    if len(masks) > 0:\n",
        "        mask = masks[0][0]  # top1のマスクのみ使用\n",
        "        bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "        cls_name = cls_names[0]  # top1のクラス名のみ使用\n",
        "        res_img = result.copy()\n",
        "        res_img, _ = get_res_img(bbox, [mask], res_img, GradCAM_threshold)\n",
        "        res_img = put_text_box(bbox, cls_name, res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    #os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    ##############################################################################################\n",
        "    ######## ファイル名を数字でソート#############################################################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[0:5]\n",
        "    ##############################################################################################\n",
        "    ##############################################################################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        # if method == 'gradcam':\n",
        "        #     saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method= \"gradcampp\")\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        if len(masks) > 0:\n",
        "            mask = masks[0][0]  # top1のマスクのみ使用\n",
        "            bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "            cls_name = cls_names[0]  # top1のクラス名のみ使用\n",
        "            res_img = result.copy()\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img, GradCAM_threshold)\n",
        "            res_img = put_text_box(bbox, cls_name, res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        #os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "        # オブジェクトを明示的に削除\n",
        "        del saliency_method\n",
        "        del masks\n",
        "        del logits\n",
        "        del boxes\n",
        "        del images\n",
        "        del final_image\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_0'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "\n",
        "    GradCAM_threshold = 0.5 #GradCAMの閾値設定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUdHWgh79uc"
      },
      "source": [
        "###**Area of interestの計算**\n",
        "\n",
        "結果をcsvに保存する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up7ykhgI5l3n"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# ###これを押すとcsvが更新されてしまうので注意！！！\n",
        "\n",
        "# # CSVファイルのパス\n",
        "# input_file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "\n",
        "# # CSVファイルをDataFrameとして読み込む\n",
        "# df = pd.read_csv(input_file_path)\n",
        "\n",
        "# # 'Unnamed'が含まれる列を削除する\n",
        "# df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# # 例としてthresholdの値を設定\n",
        "# threshold = 0.5\n",
        "\n",
        "# # 指定されたレイヤー\n",
        "# layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act', '24_m_0', '24_m_1', '24_m_2']\n",
        "\n",
        "\n",
        "# # 各レイヤーに対して新しい列を作成\n",
        "# for layer in layers:\n",
        "#     df[f'AOI_{threshold}_layer{layer}'] = None  # 初期値を設定\n",
        "\n",
        "# # 修正後のDataFrameを表示する\n",
        "# print(df.head())\n",
        "\n",
        "# # DataFrameをCSVファイルとして保存\n",
        "# df.to_csv(input_file_path, index=False)\n",
        "\n",
        "# print(f\"Updated DataFrame has been saved to: {input_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNjX_T_cVa-9",
        "outputId": "4a97fdc5-bebf-4e37-afa0-690b6ff7d002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 17.87 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "\n",
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj6fl4uVCCji"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                      in_source=Box.BoxSource.Torch,\n",
        "                                      to_source=Box.BoxSource.Numpy,\n",
        "                                      return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS3iA_8QKWZC",
        "outputId": "7a2525ca-66de-4224-a7c8-cc18d8c68e05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxS9jH2fO59C",
        "outputId": "8fd34572-3d8b-4d01-cf0a-fb7d0207f949"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrjCYpUtJlyR"
      },
      "outputs": [],
      "source": [
        "##################\n",
        "##################\n",
        "## 3-layerで解析##\n",
        "##################\n",
        "##################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "   for mask in masks:\n",
        "       # マスクの前処理\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "       # bboxの範囲内のマスク部分を取得\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # 閾値を超える共通部分のピクセル数をカウント\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox のピクセル数を取得\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI を計算\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   # if len(bbox) == 0 or len(masks) == 0:\n",
        "   #     return 0.0\n",
        "\n",
        "   # for mask in masks:\n",
        "   #     mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "   #     mask[mask < threshold*255] = 0\n",
        "   #     mask[mask >= threshold*255] = 255\n",
        "   #     heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "   #     n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "   #     aoi = np.sum(n_heatmat > 0) / (n_heatmat.shape[0] * n_heatmat.shape[1])\n",
        "   #     print(f\"Area of Interest (original_AOI): {aoi}\")\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "   df = pd.read_csv(csv_path)\n",
        "   df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "   for layer_num in [17, 20, 23]:\n",
        "       if f'AOI_{threshold}_layer{layer_num}' not in df.columns:\n",
        "           df[f'AOI_{threshold}_layer{layer_num}'] = None\n",
        "  #  for layer_num in [0, 1, 2]:\n",
        "  #      if f'AOI_{threshold}_layer24_m_{layer_num}' not in df.columns:\n",
        "  #          df[f'AOI_{threshold}_layer24_m_{layer_num}'] = None\n",
        "\n",
        "   if end_index is None:\n",
        "       end_index = len(df)\n",
        "\n",
        "   for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "       if index < start_index or index >= end_index:\n",
        "           continue\n",
        "\n",
        "       serial_number = row['serial_number']\n",
        "\n",
        "       if pd.isna(serial_number):\n",
        "           print(f\"Skipping row {index} due to NaN serial_number\")\n",
        "           continue\n",
        "\n",
        "       img_name = f\"{int(serial_number)}.jpg\"\n",
        "       img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "       # Rest of the code remains the same\n",
        "       if os.path.exists(img_path):\n",
        "           img = cv2.imread(img_path)\n",
        "           torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "           for layer_num, saliency_method in zip([0, 1, 2], saliency_methods):\n",
        "               try:\n",
        "                   masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "               except ValueError as e:\n",
        "                   print(f\"Error processing image {img_name}: {str(e)}\")\n",
        "                   continue\n",
        "\n",
        "               if len(masks) > 0 and len(boxes) > 0:\n",
        "                   mask = masks[0][0]  # top1のマスクのみ使用\n",
        "                   bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "\n",
        "                   aoi = get_aoi(bbox, [mask], threshold)\n",
        "\n",
        "                   ######################\n",
        "                   df.at[index, f'AOI_{threshold}_layer{layer_num}'] = aoi\n",
        "                   #df.at[index, f'AOI_{threshold}_layer24_m_{layer_num}'] = aoi\n",
        "                   ######################\n",
        "\n",
        "                   print(f\"Image: {img_name}, Layer: {layer_num}, AOI: {aoi}\")\n",
        "       else:\n",
        "           print(f\"Image not found: {img_name}\")\n",
        "\n",
        "   df.to_csv(csv_path, index=False)\n",
        "\n",
        "# 使用例\n",
        "# folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "# csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv\"\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv\"\n",
        "threshold = 0.5\n",
        "\n",
        "##############################################\n",
        "start_index = 5 # 開始するインデックスを指定\n",
        "end_index = 7 # 終了するインデックスを指定\n",
        "##############################################\n",
        "\n",
        "# モデルとsaliency_methodの定義\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cpu'\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "############\n",
        "target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "#target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2']\n",
        "############\n",
        "saliency_methods = [YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\") for layer in target_layers]\n",
        "\n",
        "calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "##################\n",
        "## 6-layerまとめて解析##\n",
        "##################\n",
        "##################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "   for mask in masks:\n",
        "       # マスクの前処理\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "       # bboxの範囲内のマスク部分を取得\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # 閾値を超える共通部分のピクセル数をカウント\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox のピクセル数を取得\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI を計算\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "   df = pd.read_csv(csv_path)\n",
        "   df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "   layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act', '24_m_0', '24_m_1', '24_m_2']\n",
        "   for layer in layers:\n",
        "       if f'AOI_{threshold}_layer{layer}' not in df.columns:\n",
        "           df[f'AOI_{threshold}_layer{layer}'] = None\n",
        "\n",
        "   if end_index is None:\n",
        "       end_index = len(df)\n",
        "\n",
        "   for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "       if index < start_index or index >= end_index:\n",
        "           continue\n",
        "\n",
        "       serial_number = row['serial_number']\n",
        "\n",
        "       if pd.isna(serial_number):\n",
        "           print(f\"Skipping row {index} due to NaN serial_number\")\n",
        "           continue\n",
        "\n",
        "       img_name = f\"{int(serial_number)}.jpg\"\n",
        "       img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "       if os.path.exists(img_path):\n",
        "           img = cv2.imread(img_path)\n",
        "           torch_img = model.preprocessing(img[..., ::-1])\n",
        "           for layer, saliency_method in zip(layers, saliency_methods):\n",
        "               try:\n",
        "                   masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "               except ValueError as e:\n",
        "                   print(f\"Error processing image {img_name}: {str(e)}\")\n",
        "                   continue\n",
        "\n",
        "               if len(masks) > 0 and len(boxes) > 0:\n",
        "                   mask = masks[0][0]\n",
        "                   bbox = boxes[0][0]\n",
        "\n",
        "                   aoi = get_aoi(bbox, [mask], threshold)\n",
        "                   df.at[index, f'AOI_{threshold}_layer{layer}'] = aoi\n",
        "\n",
        "                   print(f\"Image: {img_name}, Layer: {layer}, AOI: {aoi}\")\n",
        "       else:\n",
        "           print(f\"Image not found: {img_name}\")\n",
        "\n",
        "   df.to_csv(csv_path, index=False)\n",
        "\n",
        "# 使用例\n",
        "# folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "# csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv\"\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv\"\n",
        "threshold = 0.5\n",
        "\n",
        "##############################################\n",
        "start_index = 4 # 開始するインデックスを指定\n",
        "end_index = 5 # 終了するインデックスを指定\n",
        "##############################################\n",
        "\n",
        "# モデルとsaliency_methodの定義\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cpu'\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "############\n",
        "target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act', 'model_24_m_0', 'model_24_m_1', 'model_24_m_2']\n",
        "############\n",
        "\n",
        "saliency_methods = [YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcam\") for layer in target_layers]\n",
        "\n",
        "calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)"
      ],
      "metadata": {
        "id": "Za1ESuv9kBu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnSY0YhouefW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Temporary (aoi_50計算出力用)\n",
        "## 6-layerまとめて解析##\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "\n",
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\"\n",
        "\n",
        "\n",
        "\n",
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                      in_source=Box.BoxSource.Torch,\n",
        "                                      to_source=Box.BoxSource.Numpy,\n",
        "                                      return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "##################\n",
        "##################\n",
        "## 6-layerまとめて解析##\n",
        "##################\n",
        "##################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "   for mask in masks:\n",
        "       # マスクの前処理\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "       # bboxの範囲内のマスク部分を取得\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # 閾値を超える共通部分のピクセル数をカウント\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox のピクセル数を取得\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI を計算\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "   df = pd.read_csv(csv_path)\n",
        "   df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "   layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv', '24_m_0', '24_m_1', '24_m_2']\n",
        "   for layer in layers:\n",
        "       if f'AOI_{threshold}_layer{layer}' not in df.columns:\n",
        "           df[f'AOI_{threshold}_layer{layer}'] = None\n",
        "\n",
        "   if end_index is None:\n",
        "       end_index = len(df)\n",
        "\n",
        "   for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "       if index < start_index or index >= end_index:\n",
        "           continue\n",
        "\n",
        "       serial_number = row['serial_number']\n",
        "\n",
        "       if pd.isna(serial_number):\n",
        "           print(f\"Skipping row {index} due to NaN serial_number\")\n",
        "           continue\n",
        "\n",
        "       img_name = f\"{int(serial_number)}.jpg\"\n",
        "       img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "       if os.path.exists(img_path):\n",
        "           img = cv2.imread(img_path)\n",
        "           torch_img = model.preprocessing(img[..., ::-1])\n",
        "           for layer, saliency_method in zip(layers, saliency_methods):\n",
        "               try:\n",
        "                   masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "               except ValueError as e:\n",
        "                   print(f\"Error processing image {img_name}: {str(e)}\")\n",
        "                   continue\n",
        "\n",
        "               if len(masks) > 0 and len(boxes) > 0:\n",
        "                   mask = masks[0][0]\n",
        "                   bbox = boxes[0][0]\n",
        "\n",
        "                   aoi = get_aoi(bbox, [mask], threshold)\n",
        "                   df.at[index, f'AOI_{threshold}_layer{layer}'] = aoi\n",
        "\n",
        "                   print(f\"Image: {img_name}, Layer: {layer}, AOI: {aoi}\")\n",
        "       else:\n",
        "           print(f\"Image not found: {img_name}\")\n",
        "\n",
        "   df.to_csv(csv_path, index=False)\n",
        "\n",
        "# 使用例\n",
        "# folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "# csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv\"\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv\"\n",
        "threshold = 0.5\n",
        "\n",
        "##############################################\n",
        "start_index = 0 #@param {type:\"string\"} # 開始するインデックスを指定\n",
        "end_index = 4 #@param {type:\"string\"} # 終了するインデックスを指定\n",
        "##############################################\n",
        "\n",
        "# モデルとsaliency_methodの定義\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cpu'\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "############\n",
        "#target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act', 'model_24_m_0', 'model_24_m_1', 'model_24_m_2']\n",
        "target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv', 'model_24_m_0', 'model_24_m_1', 'model_24_m_2']\n",
        "############\n",
        "\n",
        "\n",
        "saliency_methods = [YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\") for layer in target_layers]\n",
        "\n",
        "calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLtS0QQjuejB",
        "outputId": "3dfb42fd-efc2-4864-ffc2-595a9575e796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/content\n",
            "Found existing installation: deep_utils 1.3.61\n",
            "Uninstalling deep_utils-1.3.61:\n",
            "  Successfully uninstalled deep_utils-1.3.61\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "fatal: destination path 'yolov5-gradcam' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5-gradcam/models/experimental.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(attempt_download(w), map_location=device)\n",
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 0/228 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: model_17_cv3_conv, AOI: 0.46459046932275705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: model_20_cv3_conv, AOI: 0.26868878072231234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: model_23_cv3_conv, AOI: 0.15128297578270536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "<ipython-input-16-0636d9bee1b8>:386: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: 24_m_0, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0636d9bee1b8>:386: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: 24_m_1, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   0%|          | 1/228 [00:03<12:09,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: 24_m_2, AOI: 0.0685130100354546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: model_17_cv3_conv, AOI: 0.4876628912434539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: model_20_cv3_conv, AOI: 0.13375959079283886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: model_23_cv3_conv, AOI: 0.03662160516380465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "<ipython-input-16-0636d9bee1b8>:386: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: 24_m_0, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0636d9bee1b8>:386: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: 24_m_1, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|          | 2/228 [00:07<13:40,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: 24_m_2, AOI: 0.011691633175009134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 3.jpg, Layer: model_17_cv3_conv, AOI: 0.08420543737913776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 3.jpg, Layer: model_20_cv3_conv, AOI: 0.03725400978273234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 3.jpg, Layer: model_23_cv3_conv, AOI: 0.13919064952792629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "<ipython-input-16-0636d9bee1b8>:386: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 3.jpg, Layer: 24_m_0, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0636d9bee1b8>:386: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 3.jpg, Layer: 24_m_1, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   1%|▏         | 3/228 [00:11<15:20,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 3.jpg, Layer: 24_m_2, AOI: 0.05342111250142191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 4.jpg, Layer: model_17_cv3_conv, AOI: 0.4233584617065443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 4.jpg, Layer: model_20_cv3_conv, AOI: 0.2090025128373211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 4.jpg, Layer: model_23_cv3_conv, AOI: 0.29249289850322296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "<ipython-input-16-0636d9bee1b8>:386: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 4.jpg, Layer: 24_m_0, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0636d9bee1b8>:386: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 4.jpg, Layer: 24_m_1, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 228/228 [00:16<00:00, 14.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 4.jpg, Layer: 24_m_2, AOI: 0.04091554681525183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyUKabVuuek_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNL_j2lkuem5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XxKs84GkBwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE7ZYlphCpBV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVNlV1lh_rRp"
      },
      "outputs": [],
      "source": [
        "# show_result\n",
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# DataFrameとしてCSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# DataFrameの最初の数行を表示する\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmGNyvTdM3c4"
      },
      "source": [
        "#**注目点色塗り**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx9KdyNZNzz8"
      },
      "outputs": [],
      "source": [
        "#注目点に色を塗る（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            color = np.array([0,0,0], dtype=np.uint8) #マスクの色：白は[0,0,0]\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + (1 - binary_mask[..., np.newaxis]) * color\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + (1 - black_mask[..., np.newaxis]) * color\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/8.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.1  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEn_KHC-T8C1"
      },
      "outputs": [],
      "source": [
        "# 注目点をblurする（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            # ぼかし効果を適用\n",
        "            blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + blurred_result * (1 - binary_mask[..., np.newaxis])\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + black_blurred_result * (1 - black_mask[..., np.newaxis])\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/41.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.3  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69WoakWi-ryq"
      },
      "source": [
        "#**Analyze results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h03rgBUoWi_L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV files\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# Define class names as shown in the image\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\", \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# Create confusion matrices for Slit and Sumaho datasets\n",
        "conf_matrix_slit = pd.crosstab(df_slit['class_num'], df_slit['top1'])\n",
        "conf_matrix_sumaho = pd.crosstab(df_sumaho['class_num'], df_sumaho['top1'])\n",
        "\n",
        "# Rename the index and columns for better readability\n",
        "conf_matrix_slit.index = class_names\n",
        "conf_matrix_slit.columns = class_names\n",
        "conf_matrix_sumaho.index = class_names\n",
        "conf_matrix_sumaho.columns = class_names\n",
        "\n",
        "# Create a figure with higher DPI\n",
        "plt.figure(figsize=(15, 6), dpi=350)  # 横幅を少し広げました\n",
        "\n",
        "# Slit Lamp Data Confusion Matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_slit, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Slit Lamp Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')  # 横軸のラベルを45度傾ける\n",
        "\n",
        "# Smartphone Data Confusion Matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_sumaho, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Smartphone Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')  # 横軸のラベルを45度傾ける\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure at 350 DPI\n",
        "plt.savefig('confusion_matrices_350dpi.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTt2gTWAAD3",
        "outputId": "7288996f-0e39-46f7-e4d6-d7808826263b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.08771929824562, 77.63157894736842)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Calculate accuracy for Slit and Sumaho datasets\n",
        "accuracy_slit = (conf_matrix_slit.values.diagonal().sum() / conf_matrix_slit.values.sum()) * 100\n",
        "accuracy_sumaho = (conf_matrix_sumaho.values.diagonal().sum() / conf_matrix_sumaho.values.sum()) * 100\n",
        "\n",
        "accuracy_slit, accuracy_sumaho\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title スマホvsスリット (GradCAM++)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# 両データセットの統計を計算して表示\n",
        "for df, device_name in [(df_slit, \"Slit Lamp\"), (df_sumaho, \"Smartphone\")]:\n",
        "    print(f\"\\n============= {device_name} データの統計値 =============\")\n",
        "\n",
        "    # 各レイヤーの詳細な統計量を計算\n",
        "    for column in columns_of_interest:\n",
        "        stats = df[column].describe()\n",
        "        percentiles = df[column].quantile([0.1, 0.25, 0.75, 0.9])\n",
        "\n",
        "        print(f\"\\n------- {column} の統計値 -------\")\n",
        "        print(f\"データ数: {int(stats['count'])}\")\n",
        "        print(f\"平均値: {stats['mean']:.6f}\")\n",
        "        print(f\"標準偏差: {stats['std']:.6f}\")\n",
        "        print(f\"最小値: {stats['min']:.6f}\")\n",
        "        print(f\"10パーセンタイル: {percentiles[0.1]:.6f}\")\n",
        "        print(f\"25パーセンタイル: {stats['25%']:.6f}\")\n",
        "        print(f\"中央値: {stats['50%']:.6f}\")\n",
        "        print(f\"75パーセンタイル: {stats['75%']:.6f}\")\n",
        "        print(f\"90パーセンタイル: {percentiles[0.9]:.6f}\")\n",
        "        print(f\"最大値: {stats['max']:.6f}\")\n",
        "\n",
        "        # 歪度と尖度も計算\n",
        "        skewness = df[column].skew()\n",
        "        kurtosis = df[column].kurtosis()\n",
        "        print(f\"歪度: {skewness:.6f}\")\n",
        "        print(f\"尖度: {kurtosis:.6f}\")\n",
        "\n",
        "    # 3つのレイヤーの合計値の統計を計算\n",
        "    layer_sum = df[columns_of_interest].sum(axis=1)\n",
        "    sum_stats = layer_sum.describe()\n",
        "    sum_percentiles = layer_sum.quantile([0.1, 0.9])\n",
        "    sum_skewness = layer_sum.skew()\n",
        "    sum_kurtosis = layer_sum.kurtosis()\n",
        "\n",
        "    print(f\"\\n------- 3レイヤーの合計値の統計 -------\")\n",
        "    print(f\"データ数: {int(sum_stats['count'])}\")\n",
        "    print(f\"平均値: {sum_stats['mean']:.6f}\")\n",
        "    print(f\"標準偏差: {sum_stats['std']:.6f}\")\n",
        "    print(f\"最小値: {sum_stats['min']:.6f}\")\n",
        "    print(f\"10パーセンタイル: {sum_percentiles[0.1]:.6f}\")\n",
        "    print(f\"25パーセンタイル: {sum_stats['25%']:.6f}\")\n",
        "    print(f\"中央値: {sum_stats['50%']:.6f}\")\n",
        "    print(f\"75パーセンタイル: {sum_stats['75%']:.6f}\")\n",
        "    print(f\"90パーセンタイル: {sum_percentiles[0.9]:.6f}\")\n",
        "    print(f\"最大値: {sum_stats['max']:.6f}\")\n",
        "\n",
        "    # 相関係数の計算\n",
        "    print(f\"\\n------- レイヤー間の相関係数 -------\")\n",
        "    correlation_matrix = df[columns_of_interest].corr()\n",
        "    print(correlation_matrix.round(6))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)  # セパレータの表示"
      ],
      "metadata": {
        "id": "NDEHCADOprhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v9DUXYHkJ5x",
        "outputId": "0e9b02c0-f9aa-48b8-b46c-6ed8a95f44d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ± SD AOI for Slit Lamp Data\n",
            "              AOI_0.5_layer24_m_0      AOI_0.5_layer24_m_1       \\\n",
            "                             mean  std                mean  std   \n",
            "infection                     0.0  0.0                 0.0  0.0   \n",
            "normal                        0.0  0.0                 0.0  0.0   \n",
            "non-infection                 0.0  0.0                 0.0  0.0   \n",
            "scar                          0.0  0.0                 0.0  0.0   \n",
            "tumor                         0.0  0.0                 0.0  0.0   \n",
            "deposit                       0.0  0.0                 0.0  0.0   \n",
            "APAC                          0.0  0.0                 0.0  0.0   \n",
            "lens opacity                  0.0  0.0                 0.0  0.0   \n",
            "bullous                       0.0  0.0                 0.0  0.0   \n",
            "\n",
            "              AOI_0.5_layer24_m_2            \n",
            "                             mean       std  \n",
            "infection                0.047342  0.023897  \n",
            "normal                   0.026101  0.010555  \n",
            "non-infection            0.116942  0.075453  \n",
            "scar                     0.073900  0.030724  \n",
            "tumor                    0.112100  0.069113  \n",
            "deposit                  0.036668  0.025242  \n",
            "APAC                     0.063290  0.037150  \n",
            "lens opacity             0.072591  0.040474  \n",
            "bullous                  0.067483  0.022869  \n",
            "\n",
            "Mean ± SD AOI for Smartphone Data\n",
            "              AOI_0.5_layer24_m_0      AOI_0.5_layer24_m_1            \\\n",
            "                             mean  std                mean       std   \n",
            "infection                     0.0  0.0            0.000000  0.000000   \n",
            "normal                        0.0  0.0            0.001019  0.004992   \n",
            "non-infection                 0.0  0.0            0.000000  0.000000   \n",
            "scar                          0.0  0.0            0.000000  0.000000   \n",
            "tumor                         0.0  0.0            0.000000  0.000000   \n",
            "deposit                       0.0  0.0            0.005983  0.034369   \n",
            "APAC                          0.0  0.0            0.000000  0.000000   \n",
            "lens opacity                  0.0  0.0            0.000000  0.000000   \n",
            "bullous                       0.0  0.0            0.000000  0.000000   \n",
            "\n",
            "              AOI_0.5_layer24_m_2            \n",
            "                             mean       std  \n",
            "infection                0.063889  0.037739  \n",
            "normal                   0.037293  0.024043  \n",
            "non-infection            0.178614  0.139032  \n",
            "scar                     0.125316  0.081693  \n",
            "tumor                    0.165420  0.065204  \n",
            "deposit                  0.083059  0.058240  \n",
            "APAC                     0.079910  0.030570  \n",
            "lens opacity             0.111812  0.069523  \n",
            "bullous                  0.082440  0.040750  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIの平均と標準偏差を計算\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title スリット（GradCAM vs GradCAM++）\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# 両データセットの統計を計算して表示\n",
        "for df, device_name in [(df_slit, \"GradCAM\"), (df_sumaho, \"GradCAM++\")]:\n",
        "    print(f\"\\n============= {device_name} データの統計値 =============\")\n",
        "\n",
        "    # 各レイヤーの詳細な統計量を計算\n",
        "    for column in columns_of_interest:\n",
        "        stats = df[column].describe()\n",
        "        percentiles = df[column].quantile([0.1, 0.25, 0.75, 0.9])\n",
        "\n",
        "        print(f\"\\n------- {column} の統計値 -------\")\n",
        "        print(f\"データ数: {int(stats['count'])}\")\n",
        "        print(f\"平均値: {stats['mean']:.6f}\")\n",
        "        print(f\"標準偏差: {stats['std']:.6f}\")\n",
        "        print(f\"最小値: {stats['min']:.6f}\")\n",
        "        print(f\"10パーセンタイル: {percentiles[0.1]:.6f}\")\n",
        "        print(f\"25パーセンタイル: {stats['25%']:.6f}\")\n",
        "        print(f\"中央値: {stats['50%']:.6f}\")\n",
        "        print(f\"75パーセンタイル: {stats['75%']:.6f}\")\n",
        "        print(f\"90パーセンタイル: {percentiles[0.9]:.6f}\")\n",
        "        print(f\"最大値: {stats['max']:.6f}\")\n",
        "\n",
        "        # 歪度と尖度も計算\n",
        "        skewness = df[column].skew()\n",
        "        kurtosis = df[column].kurtosis()\n",
        "        print(f\"歪度: {skewness:.6f}\")\n",
        "        print(f\"尖度: {kurtosis:.6f}\")\n",
        "\n",
        "    # 3つのレイヤーの合計値の統計を計算\n",
        "    layer_sum = df[columns_of_interest].sum(axis=1)\n",
        "    sum_stats = layer_sum.describe()\n",
        "    sum_percentiles = layer_sum.quantile([0.1, 0.9])\n",
        "    sum_skewness = layer_sum.skew()\n",
        "    sum_kurtosis = layer_sum.kurtosis()\n",
        "\n",
        "    print(f\"\\n------- 3レイヤーの合計値の統計 -------\")\n",
        "    print(f\"データ数: {int(sum_stats['count'])}\")\n",
        "    print(f\"平均値: {sum_stats['mean']:.6f}\")\n",
        "    print(f\"標準偏差: {sum_stats['std']:.6f}\")\n",
        "    print(f\"最小値: {sum_stats['min']:.6f}\")\n",
        "    print(f\"10パーセンタイル: {sum_percentiles[0.1]:.6f}\")\n",
        "    print(f\"25パーセンタイル: {sum_stats['25%']:.6f}\")\n",
        "    print(f\"中央値: {sum_stats['50%']:.6f}\")\n",
        "    print(f\"75パーセンタイル: {sum_stats['75%']:.6f}\")\n",
        "    print(f\"90パーセンタイル: {sum_percentiles[0.9]:.6f}\")\n",
        "    print(f\"最大値: {sum_stats['max']:.6f}\")\n",
        "\n",
        "    # 相関係数の計算\n",
        "    print(f\"\\n------- レイヤー間の相関係数 -------\")\n",
        "    correlation_matrix = df[columns_of_interest].corr()\n",
        "    print(correlation_matrix.round(6))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)  # セパレータの表示"
      ],
      "metadata": {
        "id": "sMhTeXg39b16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIの平均と標準偏差を計算\n",
        "mean_sd_aoi_gradcam = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_gradcampp = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_gradcam.index = class_names\n",
        "mean_sd_aoi_gradcampp.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_gradcam)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_gradcampp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYWp2le2-Vjg",
        "outputId": "80e144a7-f664-4343-f0a9-3b75c64e4513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ± SD AOI for Slit Lamp Data\n",
            "              AOI_0.5_layer17           AOI_0.5_layer20            \\\n",
            "                         mean       std            mean       std   \n",
            "infection            0.085187  0.067278        0.008063  0.044605   \n",
            "normal               0.178015  0.130984        0.008429  0.033492   \n",
            "non-infection        0.138709  0.141636        0.003888  0.008722   \n",
            "scar                 0.164444  0.123306        0.010008  0.034294   \n",
            "tumor                0.139273  0.101115        0.000224  0.001081   \n",
            "deposit              0.176102  0.104585        0.001737  0.009827   \n",
            "APAC                 0.129491  0.132233        0.000000  0.000000   \n",
            "lens opacity         0.118963  0.096171        0.058011  0.157424   \n",
            "bullous              0.128982  0.064899        0.009988  0.039860   \n",
            "\n",
            "              AOI_0.5_layer23  ... AOI_0.5_layer24_m_0 AOI_0.5_layer24_m_1  \\\n",
            "                         mean  ...                 std                mean   \n",
            "infection            0.086643  ...                 0.0                 0.0   \n",
            "normal               0.050446  ...                 0.0                 0.0   \n",
            "non-infection        0.271836  ...                 0.0                 0.0   \n",
            "scar                 0.205724  ...                 0.0                 0.0   \n",
            "tumor                0.195092  ...                 0.0                 0.0   \n",
            "deposit              0.072846  ...                 0.0                 0.0   \n",
            "APAC                 0.068622  ...                 0.0                 0.0   \n",
            "lens opacity         0.137005  ...                 0.0                 0.0   \n",
            "bullous              0.126359  ...                 0.0                 0.0   \n",
            "\n",
            "                   AOI_0.5_layer24_m_2            \n",
            "               std                mean       std  \n",
            "infection      0.0            0.047342  0.023897  \n",
            "normal         0.0            0.026101  0.010555  \n",
            "non-infection  0.0            0.116942  0.075453  \n",
            "scar           0.0            0.073900  0.030724  \n",
            "tumor          0.0            0.112100  0.069113  \n",
            "deposit        0.0            0.036668  0.025242  \n",
            "APAC           0.0            0.063290  0.037150  \n",
            "lens opacity   0.0            0.072591  0.040474  \n",
            "bullous        0.0            0.067483  0.022869  \n",
            "\n",
            "[9 rows x 12 columns]\n",
            "\n",
            "Mean ± SD AOI for Smartphone Data\n",
            "              AOI_0.5_layer17           AOI_0.5_layer20            \\\n",
            "                         mean       std            mean       std   \n",
            "infection            0.360658  0.242355        0.246655  0.125462   \n",
            "normal               0.477689  0.120818        0.268085  0.154725   \n",
            "non-infection        0.365848  0.234142        0.310207  0.107124   \n",
            "scar                 0.333199  0.209766        0.306752  0.134287   \n",
            "tumor                0.307143  0.201282        0.236082  0.142173   \n",
            "deposit              0.376291  0.197399        0.220849  0.169399   \n",
            "APAC                 0.281960  0.248539        0.168814  0.194398   \n",
            "lens opacity         0.306860  0.172524        0.290219  0.136965   \n",
            "bullous              0.372508  0.164337        0.289707  0.168308   \n",
            "\n",
            "              AOI_0.5_layer23  ... AOI_0.5_layer24_m_0 AOI_0.5_layer24_m_1  \\\n",
            "                         mean  ...                 std                mean   \n",
            "infection            0.153173  ...                 0.0                 0.0   \n",
            "normal               0.109381  ...                 0.0                 0.0   \n",
            "non-infection        0.337387  ...                 0.0                 0.0   \n",
            "scar                 0.268892  ...                 0.0                 0.0   \n",
            "tumor                0.271492  ...                 0.0                 0.0   \n",
            "deposit              0.115536  ...                 0.0                 0.0   \n",
            "APAC                 0.076960  ...                 0.0                 0.0   \n",
            "lens opacity         0.179528  ...                 0.0                 0.0   \n",
            "bullous              0.160595  ...                 0.0                 0.0   \n",
            "\n",
            "                   AOI_0.5_layer24_m_2            \n",
            "               std                mean       std  \n",
            "infection      0.0            0.047342  0.023897  \n",
            "normal         0.0            0.026101  0.010555  \n",
            "non-infection  0.0            0.116942  0.075453  \n",
            "scar           0.0            0.073900  0.030724  \n",
            "tumor          0.0            0.112100  0.069113  \n",
            "deposit        0.0            0.036668  0.025242  \n",
            "APAC           0.0            0.063290  0.037150  \n",
            "lens opacity   0.0            0.072591  0.040474  \n",
            "bullous        0.0            0.067483  0.022869  \n",
            "\n",
            "[9 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2rBLnp6lbjW"
      },
      "source": [
        "##**Layerごとに解析**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkZ4ZbaoUEMX"
      },
      "outputs": [],
      "source": [
        "#@title スリットvsスマホ\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"23\"\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形してSeabornでプロットできるようにする\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 合算データを作成\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# 青とオレンジのカラーパレットを定義\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# グラフ1: スリットランプとスマートフォンのデータ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=colors)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# グラフ2: 合算データ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='#1f77b4')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# グラフ3: slitとsumahoの全体比較\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n",
        "plt.xlabel('Type', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Comparison of AOI_0.5 between Slit Lamp and Smartphone', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GradCAM vs GradCAM++ (スリット)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"23\"\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形してSeabornでプロットできるようにする\n",
        "df_slit['Type'] = 'GradCAM'\n",
        "df_sumaho['Type'] = 'GradCAM++'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 合算データを作成\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# 青とオレンジのカラーパレットを定義\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# グラフ1: スリットランプとスマートフォンのデータ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=colors)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('gradcam_overall_comparison.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# グラフ2: 合算データ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='#1f77b4')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# グラフ3: slitとsumahoの全体比較\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n",
        "plt.xlabel('Type', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Comparison of AOI_0.5 between GradCAM and GradCAM++', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u8ZCGTRoJO7a",
        "outputId": "d9de0470-1e94-49b3-a8e6-130b8b37fa93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-4bca1363b50b>:81: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.labelsize': 18,\n",
        "    'axes.titlesize': 20,\n",
        "    'xtick.labelsize': 14,\n",
        "    'ytick.labelsize': 14,\n",
        "    'legend.fontsize': 14,\n",
        "    'figure.titlesize': 22\n",
        "})\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# クラス名を定義（指定された順序）\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# レイヤーごとのプロットと統計量計算\n",
        "for layer in [17, 20, 23]:\n",
        "    layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    # グラフの作成（縦横比3:4）\n",
        "    plt.figure(figsize=(12, 9))\n",
        "\n",
        "    # boxplotのスタイルをカスタマイズ（先ほどと同じ灰色のスタイル）\n",
        "    sns.boxplot(x='class_name', y=layer_name, data=df_slit, order=class_names,\n",
        "                color='#CCCCCC',\n",
        "                boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "                medianprops={'color':'black', 'linewidth':2.5},\n",
        "                flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "                whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "                capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "    plt.xlabel('Class', fontsize=18, labelpad=10)\n",
        "    plt.ylabel(layer_name, fontsize=18, labelpad=10)\n",
        "    plt.title(f'Box Plot of {layer_name} for Slit Lamp Data', fontsize=20, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # グラフを保存（高解像度）\n",
        "    plt.savefig(f'boxplot_{layer_name}.png', dpi=350, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 基本統計量の計算と表示\n",
        "    stats = df_slit.groupby('class_name')[layer_name].describe()\n",
        "    # クラスの順序を指定して並び替え\n",
        "    stats = stats.reindex(class_names)\n",
        "    print(f\"\\n{layer_name}の基本統計量:\")\n",
        "    print(stats)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "v5jSgs_QuUly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJlHqzTQpuHP"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # フォントサイズとスタイルの設定\n",
        "# plt.rcParams.update({\n",
        "#     'font.size': 12,\n",
        "#     'axes.labelsize': 16,\n",
        "#     'axes.titlesize': 16,\n",
        "#     'xtick.labelsize': 12,\n",
        "#     'ytick.labelsize': 12,\n",
        "#     'legend.fontsize': 12,\n",
        "#     'figure.titlesize': 18\n",
        "# })\n",
        "\n",
        "# # ファイルパスを指定\n",
        "# file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "# file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# # CSVファイルを読み込む\n",
        "# df_slit = pd.read_csv(file_path1)\n",
        "# df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# # クラス名を定義\n",
        "# class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# # クラス名を数字に対応させる\n",
        "# df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "# df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# # データフレームを整形してSeabornでプロットできるようにする\n",
        "# df_slit['Type'] = 'Slit'\n",
        "# df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# # 比較するレイヤーのリスト\n",
        "# layers_group1 = [\"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "# layers_group2 = [\"17\", \"20\", \"23\"]\n",
        "\n",
        "# def create_boxplots(layers, group_name):\n",
        "#     for layer in layers:\n",
        "#         layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "#         # 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "#         df_combined = pd.concat([\n",
        "#             df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "#             df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "#         ]).reset_index(drop=True)\n",
        "\n",
        "#         # グラフ: スリットランプとスマートフォンのデータ\n",
        "#         plt.figure(figsize=(15, 10))\n",
        "#         sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=['#333333', '#999999'])\n",
        "#         plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "#         plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "#         plt.title(f'Box Plot of {layer_name} for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "#         plt.xticks(rotation=45, ha='right')\n",
        "#         legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "#         legend.get_title().set_fontsize(14)\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "\n",
        "#     # レイヤー間の比較（幅を半分に）\n",
        "#     plt.figure(figsize=(7.5, 10))  # 幅を半分に変更\n",
        "#     df_melted = pd.melt(df_slit, id_vars=['class_name'], value_vars=[f'AOI_0.5_layer{layer}' for layer in layers], var_name='Layer', value_name='Value')\n",
        "#     sns.boxplot(x='Layer', y='Value', data=df_melted, palette='Blues')\n",
        "#     plt.xlabel('Layer', fontsize=14, labelpad=10)\n",
        "#     plt.ylabel('AOI_0.5 Value', fontsize=14, labelpad=10)\n",
        "#     plt.title(f'Comparison of AOI_0.5 Values\\nAcross Different Layers\\n({group_name})', fontsize=16, pad=20)\n",
        "#     plt.xticks(rotation=45, ha='right')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # グループ1のグラフを作成\n",
        "# create_boxplots(layers_group1, \"Group 1: 24_m_0, 24_m_1, 24_m_2\")\n",
        "\n",
        "# # グループ2のグラフを作成\n",
        "# create_boxplots(layers_group2, \"Group 2: 17, 20, 23\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#レイヤー毎のAOI値（17, 20, 23）(24-0, 24-1, 24-2)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# フォントサイズとスタイルの設定をさらに大きく\n",
        "plt.rcParams.update({\n",
        "    'font.size': 18,          # 16 → 18\n",
        "    'axes.labelsize': 24,     # 22 → 24\n",
        "    'axes.titlesize': 26,     # 24 → 26\n",
        "    'xtick.labelsize': 18,    # 16 → 18\n",
        "    'ytick.labelsize': 18,    # 16 → 18\n",
        "    'legend.fontsize': 18,    # 16 → 18\n",
        "    'figure.titlesize': 28    # 26 → 28\n",
        "})\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 2つのグループに分けるレイヤーのリスト\n",
        "layers_group1 = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\"]\n",
        "layers_group2 = [\"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# レイヤー名を変更するマッピング\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# グラフ1（3:4のアスペクト比）\n",
        "plt.figure(figsize=(9, 12))  # 3:4の比率\n",
        "\n",
        "df_melted_1 = pd.melt(df[layers_group1], var_name='Layer', value_name='AOI Value')\n",
        "df_melted_1['Layer'] = df_melted_1['Layer'].map(layer_mapping)\n",
        "\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted_1,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values\\nLayers 17, 20, and 23', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# グラフを保存（350dpi）\n",
        "plt.savefig('boxplot_aoi_comparison_group1.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# グラフ2（3:4のアスペクト比）\n",
        "plt.figure(figsize=(9, 12))  # 3:4の比率\n",
        "\n",
        "df_melted_2 = pd.melt(df[layers_group2], var_name='Layer', value_name='AOI Value')\n",
        "df_melted_2['Layer'] = df_melted_2['Layer'].map(layer_mapping)\n",
        "\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted_2,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values\\nLayer 24 Outputs', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# グラフを保存（350dpi）\n",
        "plt.savefig('boxplot_aoi_comparison_group2.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 基本統計量の計算と表示\n",
        "print(\"\\n基本統計量 - グループ1 (Layers 17, 20, 23):\")\n",
        "print(df[layers_group1].describe())\n",
        "print(\"\\n基本統計量 - グループ2 (Layer 24 Outputs):\")\n",
        "print(df[layers_group2].describe())"
      ],
      "metadata": {
        "id": "C92rF-kAx8Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs --q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 18,\n",
        "    'axes.labelsize': 24,\n",
        "    'axes.titlesize': 26,\n",
        "    'xtick.labelsize': 18,\n",
        "    'ytick.labelsize': 18,\n",
        "    'legend.fontsize': 18,\n",
        "    'figure.titlesize': 28\n",
        "})\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# すべてのレイヤーのリスト\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# レイヤー名を変更するマッピング\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# データを整形\n",
        "df_melted = pd.melt(df[layers], var_name='Layer', value_name='AOI Value')\n",
        "df_melted['Layer'] = df_melted['Layer'].map(layer_mapping)\n",
        "\n",
        "# グラフ作成（3:4のアスペクト比）\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# boxplotのスタイルをカスタマイズ\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values Across Different Layers', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# グラフを保存（350dpi）\n",
        "plt.savefig('boxplot_aoi_comparison_all.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 基本統計量の計算と表示\n",
        "print(\"\\n基本統計量:\")\n",
        "print(df[layers].describe())\n",
        "\n",
        "# Kruskal-Wallis 検定を実施\n",
        "h_statistic, p_value = stats.kruskal(*[df[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results:')\n",
        "print(f'H-statistic: {h_statistic:.3f}')\n",
        "print(f'p-value: {p_value:.3e}')\n",
        "\n",
        "# Dunn's testによる多重比較（Bonferroni補正）\n",
        "dunn = posthoc_dunn(df_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (p-values):')\n",
        "print(dunn.round(4))  # p値を4桁に丸める\n",
        "\n",
        "# 有意差のあるペアを抽出（p < 0.05）\n",
        "significant_pairs = []\n",
        "for i in dunn.index:\n",
        "    for j in dunn.columns:\n",
        "        if i < j:  # 重複を避けるため、上三角行列のみを見る\n",
        "            if dunn.loc[i, j] < 0.05:\n",
        "                significant_pairs.append((i, j, dunn.loc[i, j]))\n",
        "\n",
        "print('\\n有意差のあるペア (p < 0.05):')\n",
        "for pair in significant_pairs:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')"
      ],
      "metadata": {
        "id": "31icyQtM1UNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs --q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 22,  # フォントサイズを大きめに設定\n",
        "    'axes.labelsize': 28,  # 軸ラベルのフォントサイズ\n",
        "    'axes.titlesize': 30,  # タイトルのフォントサイズ\n",
        "    'xtick.labelsize': 24,  # x軸の目盛りラベルのフォントサイズ\n",
        "    'ytick.labelsize': 24,  # y軸の目盛りラベルのフォントサイズ\n",
        "    'legend.fontsize': 24,  # 凡例のフォントサイズ\n",
        "    'figure.titlesize': 32  # フィギュアタイトルのフォントサイズ\n",
        "})\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df1 = pd.read_csv(file_path1)\n",
        "df2 = pd.read_csv(file_path2)\n",
        "\n",
        "# 使用するレイヤーのリスト\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# レイヤー名を変更するマッピング\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# データを整形\n",
        "df1_melted = pd.melt(df1[layers], var_name='Layer', value_name='AOI Value')\n",
        "df1_melted['Layer'] = df1_melted['Layer'].map(layer_mapping)\n",
        "df1_melted['Method'] = 'Grad-CAM'\n",
        "\n",
        "df2_melted = pd.melt(df2[layers], var_name='Layer', value_name='AOI Value')\n",
        "df2_melted['Layer'] = df2_melted['Layer'].map(layer_mapping)\n",
        "df2_melted['Method'] = 'Grad-CAM++'\n",
        "\n",
        "# データを結合し、インデックスを振り直す\n",
        "df_combined = pd.concat([df1_melted, df2_melted], ignore_index=True)\n",
        "# あるいは\n",
        "# df_combined = pd.concat([df1_melted, df2_melted])\n",
        "# df_combined = df_combined.reset_index(drop=True)\n",
        "\n",
        "# グラフ作成（4:3のアスペクト比）\n",
        "plt.figure(figsize=(16, 12))  # 4:3の比率になるように調整\n",
        "\n",
        "# boxplotのスタイルをカスタマイズ\n",
        "sns.boxplot(x='Layer', y='AOI Value', hue='Method', data=df_combined,\n",
        "            palette=[\"#1f77b4\", \"#ff7f0e\"],\n",
        "            boxprops={'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=28, labelpad=15)  # 軸ラベルのフォントサイズをさらに大きく\n",
        "plt.ylabel('AOI Value', fontsize=28, labelpad=15)  # 軸ラベルのフォントサイズをさらに大きく\n",
        "plt.title('Comparison of AOI Values Across Different Layers and Methods', fontsize=30, pad=25)  # タイトルのフォントサイズをさらに大きく\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Type')\n",
        "plt.tight_layout()\n",
        "\n",
        "# グラフを保存（350dpi）\n",
        "plt.savefig('boxplot_aoi_comparison_all_combined.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 基本統計量の計算と表示 (Grad-CAM)\n",
        "print(\"\\n基本統計量 (Grad-CAM):\")\n",
        "print(df1[layers].describe())\n",
        "\n",
        "# 基本統計量の計算と表示 (Grad-CAM++)\n",
        "print(\"\\n基本統計量 (Grad-CAM++):\")\n",
        "print(df2[layers].describe())\n",
        "\n",
        "# Kruskal-Wallis 検定を実施 (Grad-CAM)\n",
        "h_statistic1, p_value1 = stats.kruskal(*[df1[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results (Grad-CAM):')\n",
        "print(f'H-statistic: {h_statistic1:.3f}')\n",
        "print(f'p-value: {p_value1:.3e}')\n",
        "\n",
        "# Kruskal-Wallis 検定を実施 (Grad-CAM++)\n",
        "h_statistic2, p_value2 = stats.kruskal(*[df2[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results (Grad-CAM++):')\n",
        "print(f'H-statistic: {h_statistic2:.3f}')\n",
        "print(f'p-value: {p_value2:.3e}')\n",
        "\n",
        "# Dunn's testによる多重比較（Bonferroni補正）(Grad-CAM)\n",
        "dunn1 = posthoc_dunn(df1_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (Grad-CAM) (p-values):')\n",
        "print(dunn1.round(4))  # p値を4桁に丸める\n",
        "\n",
        "# Dunn's testによる多重比較（Bonferroni補正）(Grad-CAM++)\n",
        "dunn2 = posthoc_dunn(df2_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (Grad-CAM++) (p-values):')\n",
        "print(dunn2.round(4))  # p値を4桁に丸める\n",
        "\n",
        "# 有意差のあるペアを抽出（p < 0.05）(Grad-CAM)\n",
        "significant_pairs1 = []\n",
        "for i in dunn1.index:\n",
        "    for j in dunn1.columns:\n",
        "        if i < j:  # 重複を避けるため、上三角行列のみを見る\n",
        "            if dunn1.loc[i, j] < 0.05:\n",
        "                significant_pairs1.append((i, j, dunn1.loc[i, j]))\n",
        "\n",
        "print('\\n有意差のあるペア (Grad-CAM) (p < 0.05):')\n",
        "for pair in significant_pairs1:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')\n",
        "\n",
        "# 有意差のあるペアを抽出（p < 0.05）(Grad-CAM++)\n",
        "significant_pairs2 = []\n",
        "for i in dunn2.index:\n",
        "    for j in dunn2.columns:\n",
        "        if i < j:  # 重複を避けるため、上三角行列のみを見る\n",
        "            if dunn2.loc[i, j] < 0.05:\n",
        "                significant_pairs2.append((i, j, dunn2.loc[i, j]))\n",
        "\n",
        "print('\\n有意差のあるペア (Grad-CAM++) (p < 0.05):')\n",
        "for pair in significant_pairs2:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')"
      ],
      "metadata": {
        "id": "mRmADr6uMAci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mt7gOfde1UPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgFUt3AFWwvA"
      },
      "outputs": [],
      "source": [
        "####\n",
        "#### スリット vs スマホの比較\n",
        "####\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# 各クラスごとに対応のあるt検定を行う\n",
        "results = []\n",
        "for class_name in class_names:\n",
        "    slit_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "    sumaho_data = df_sumaho[df_sumaho['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "    # 対応のあるデータを取るため、最小の長さに合わせる\n",
        "    min_length = min(len(slit_data), len(sumaho_data))\n",
        "    slit_data = slit_data[:min_length]\n",
        "    sumaho_data = sumaho_data[:min_length]\n",
        "\n",
        "    t_stat, p_value = stats.ttest_rel(slit_data, sumaho_data)\n",
        "\n",
        "    # スリットランプデータの統計値\n",
        "    slit_mean = slit_data.mean()\n",
        "    slit_std = slit_data.std()\n",
        "\n",
        "    # スマートフォンデータの統計値\n",
        "    sumaho_mean = sumaho_data.mean()\n",
        "    sumaho_std = sumaho_data.std()\n",
        "\n",
        "    results.append({\n",
        "        'class_name': class_name,\n",
        "        't_stat': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'slit_mean': slit_mean,\n",
        "        'slit_std': slit_std,\n",
        "        'sumaho_mean': sumaho_mean,\n",
        "        'sumaho_std': sumaho_std\n",
        "    })\n",
        "\n",
        "# 全クラスまとめた対応のあるt検定\n",
        "all_slit_data = df_slit[layer_name].dropna().reset_index(drop=True)\n",
        "all_sumaho_data = df_sumaho[layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "# 対応のあるデータを取るため、最小の長さに合わせる\n",
        "min_length_all = min(len(all_slit_data), len(all_sumaho_data))\n",
        "all_slit_data = all_slit_data[:min_length_all]\n",
        "all_sumaho_data = all_sumaho_data[:min_length_all]\n",
        "\n",
        "t_stat_all, p_value_all = stats.ttest_rel(all_slit_data, all_sumaho_data)\n",
        "\n",
        "# 全クラスまとめた統計値\n",
        "all_slit_mean = all_slit_data.mean()\n",
        "all_slit_std = all_slit_data.std()\n",
        "all_sumaho_mean = all_sumaho_data.mean()\n",
        "all_sumaho_std = all_sumaho_data.std()\n",
        "\n",
        "# 結果を追加\n",
        "results.append({\n",
        "    'class_name': 'All Classes',\n",
        "    't_stat': t_stat_all,\n",
        "    'p_value': p_value_all,\n",
        "    'slit_mean': all_slit_mean,\n",
        "    'slit_std': all_slit_std,\n",
        "    'sumaho_mean': all_sumaho_mean,\n",
        "    'sumaho_std': all_sumaho_std\n",
        "})\n",
        "\n",
        "# 結果をデータフレームに変換して表示\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results['p_value'] = df_results['p_value'].map(lambda x: f'{x:.3f}')\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjAiQjnMYHjN"
      },
      "outputs": [],
      "source": [
        "# #クラス毎の差（スマホ＋スリット）\n",
        "# from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# # ANOVAを実行\n",
        "# anova_result = stats.f_oneway(\n",
        "#     df_combined[df_combined['class_name'] == 'infection'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'normal'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'non-infection'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'scar'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'tumor'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'deposit'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'APAC'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'lens opacity'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'bullous'][layer_name]\n",
        "# )\n",
        "\n",
        "# print(f\"ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "# # 事後検定（TukeyのHSD検定）を実行\n",
        "# tukey_result = pairwise_tukeyhsd(df_combined[layer_name], df_combined['class_name'])\n",
        "\n",
        "# # 結果を表示\n",
        "# print(tukey_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JihM5ooC3VCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# クラス名を定義（指定された順序）\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# 解析するレイヤーのリスト\n",
        "layers = [17, 20, 23, \"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "def check_all_values_same(groups):\n",
        "    \"\"\"全ての値が同じかチェック\"\"\"\n",
        "    all_values = np.concatenate([group.values for group in groups])\n",
        "    return np.all(all_values == all_values[0]), all_values[0]\n",
        "\n",
        "def dunn_test(groups, group_names):\n",
        "    \"\"\"\n",
        "    Dunn's testの実装\n",
        "    \"\"\"\n",
        "    # 全データをランク付け\n",
        "    all_data = np.concatenate(groups)\n",
        "    ranks = stats.rankdata(all_data)\n",
        "\n",
        "    # グループごとの平均ランクを計算\n",
        "    start = 0\n",
        "    mean_ranks = []\n",
        "    ns = []\n",
        "    for group in groups:\n",
        "        n = len(group)\n",
        "        group_ranks = ranks[start:start + n]\n",
        "        mean_ranks.append(np.mean(group_ranks))\n",
        "        ns.append(n)\n",
        "        start += n\n",
        "\n",
        "    # 全ペアの組み合わせでDunn's testを実行\n",
        "    N = len(ranks)\n",
        "    k = len(groups)\n",
        "    comparisons = []\n",
        "    p_values = []\n",
        "\n",
        "    for i in range(k):\n",
        "        for j in range(i + 1, k):\n",
        "            # 平均ランクの差\n",
        "            diff = abs(mean_ranks[i] - mean_ranks[j])\n",
        "\n",
        "            # 標準誤差\n",
        "            se = np.sqrt((N * (N + 1) / 12) * (1/ns[i] + 1/ns[j]))\n",
        "\n",
        "            # z統計量\n",
        "            z = diff / se\n",
        "\n",
        "            # p値（両側検定）\n",
        "            p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "\n",
        "            comparisons.append((group_names[i], group_names[j]))\n",
        "            p_values.append(p)\n",
        "\n",
        "    # Bonferroni補正\n",
        "    rejected, p_corrected, _, _ = multipletests(p_values, method='bonferroni')\n",
        "\n",
        "    # 有意な結果を返す\n",
        "    significant_results = []\n",
        "    for (name1, name2), p_corr, is_rej in zip(comparisons, p_corrected, rejected):\n",
        "        if is_rej:  # p < 0.05 after correction\n",
        "            significant_results.append({\n",
        "                'group1': name1,\n",
        "                'group2': name2,\n",
        "                'p_value': p_corr\n",
        "            })\n",
        "\n",
        "    return significant_results\n",
        "\n",
        "for layer in layers:\n",
        "    # レイヤー名を設定\n",
        "    if isinstance(layer, int):\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "    else:\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"\\nAnalysis for {layer_name}\")\n",
        "    print('='*80)\n",
        "\n",
        "    # 欠損値を除外してデータを準備\n",
        "    data_for_analysis = df_slit[[layer_name, 'class_name']].dropna()\n",
        "\n",
        "    if len(data_for_analysis) > 0:\n",
        "        # クラスごとのデータを収集（欠損値を除外）\n",
        "        class_groups = []\n",
        "        valid_class_names = []\n",
        "\n",
        "        for class_name in class_names:\n",
        "            group_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna()\n",
        "            if len(group_data) > 0:\n",
        "                class_groups.append(group_data)\n",
        "                valid_class_names.append(class_name)\n",
        "\n",
        "        if len(class_groups) > 1:  # 少なくとも2つのグループが必要\n",
        "            # 全ての値が同じかチェック\n",
        "            is_all_same, same_value = check_all_values_same(class_groups)\n",
        "\n",
        "            if is_all_same:\n",
        "                print(f\"\\n全てのクラスで同じ値: {same_value:.4f}\")\n",
        "                print(\"統計的検定は不要です。\")\n",
        "                continue\n",
        "\n",
        "            # Kruskal-Wallis H-testを実行\n",
        "            h_statistic, p_value = stats.kruskal(*class_groups)\n",
        "\n",
        "            print(f\"\\nKruskal-Wallis H-test結果:\")\n",
        "            print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                # Dunn's testを実行\n",
        "                significant_pairs = dunn_test(class_groups, valid_class_names)\n",
        "\n",
        "                if significant_pairs:\n",
        "                    print(\"\\n有意差のあるペア (Bonferroni補正後 p < 0.05):\")\n",
        "                    for result in significant_pairs:\n",
        "                        print(f\"{result['group1']:25} vs {result['group2']:25}: \"\n",
        "                              f\"p = {result['p_value']:.4e}\")\n",
        "        else:\n",
        "            print(f\"\\n警告: {layer_name}の解析に十分なデータがありません。\")\n",
        "    else:\n",
        "        print(f\"\\n警告: {layer_name}に有効なデータがありません。\")"
      ],
      "metadata": {
        "id": "9mmq-hNs_4Q4",
        "outputId": "4a586bd1-9068-4ca4-a9dc-0c947708464c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer17\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 17.9227\n",
            "p-value: 2.1813e-02\n",
            "\n",
            "有意差のあるペア (Bonferroni補正後 p < 0.05):\n",
            "Infectious keratitis      vs Tumor                    : p = 1.8849e-02\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer20\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 14.8646\n",
            "p-value: 6.1833e-02\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer23\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 98.2819\n",
            "p-value: 9.5787e-18\n",
            "\n",
            "有意差のあるペア (Bonferroni補正後 p < 0.05):\n",
            "Normal                    vs Non-infection keratitis  : p = 3.3410e-03\n",
            "Normal                    vs Scar                     : p = 1.1779e-03\n",
            "Normal                    vs Tumor                    : p = 3.2597e-03\n",
            "Infectious keratitis      vs Non-infection keratitis  : p = 4.7841e-07\n",
            "Infectious keratitis      vs Scar                     : p = 2.0675e-09\n",
            "Infectious keratitis      vs Tumor                    : p = 1.6733e-08\n",
            "Infectious keratitis      vs Lens opacity             : p = 6.6723e-03\n",
            "Non-infection keratitis   vs Deposit                  : p = 4.2487e-05\n",
            "Non-infection keratitis   vs APAC                     : p = 2.2747e-05\n",
            "Scar                      vs Deposit                  : p = 1.9979e-06\n",
            "Scar                      vs APAC                     : p = 1.4439e-05\n",
            "Tumor                     vs Deposit                  : p = 9.3096e-06\n",
            "Tumor                     vs APAC                     : p = 3.0676e-05\n",
            "APAC                      vs Lens opacity             : p = 2.4251e-02\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_0\n",
            "================================================================================\n",
            "\n",
            "全てのクラスで同じ値: 0.0000\n",
            "統計的検定は不要です。\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_1\n",
            "================================================================================\n",
            "\n",
            "全てのクラスで同じ値: 0.0000\n",
            "統計的検定は不要です。\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_2\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 84.7932\n",
            "p-value: 5.2761e-15\n",
            "\n",
            "有意差のあるペア (Bonferroni補正後 p < 0.05):\n",
            "Normal                    vs Tumor                    : p = 1.5855e-03\n",
            "Infectious keratitis      vs Non-infection keratitis  : p = 3.5267e-06\n",
            "Infectious keratitis      vs Scar                     : p = 1.1690e-07\n",
            "Infectious keratitis      vs Tumor                    : p = 3.8577e-11\n",
            "Infectious keratitis      vs Lens opacity             : p = 1.5340e-04\n",
            "Infectious keratitis      vs Bullous keratopathy      : p = 1.2106e-05\n",
            "Non-infection keratitis   vs Deposit                  : p = 1.3774e-03\n",
            "Scar                      vs Deposit                  : p = 6.6084e-04\n",
            "Tumor                     vs Deposit                  : p = 1.1572e-06\n",
            "Deposit                   vs Lens opacity             : p = 4.5894e-02\n",
            "Deposit                   vs Bullous keratopathy      : p = 1.2108e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# クラス名を定義（指定された順序）\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# 解析するレイヤーのリスト\n",
        "layers = [17, 20, 23, \"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "def check_all_values_same(groups):\n",
        "    \"\"\"全ての値が同じかチェック\"\"\"\n",
        "    all_values = np.concatenate([group.values for group in groups])\n",
        "    return np.all(all_values == all_values[0]), all_values[0]\n",
        "\n",
        "def perform_pairwise_mannwhitney(groups, group_names, alpha=0.05):\n",
        "    \"\"\"ペアワイズMann-Whitney U検定を実行\"\"\"\n",
        "    n_groups = len(groups)\n",
        "    results = []\n",
        "\n",
        "    for i in range(n_groups):\n",
        "        for j in range(i+1, n_groups):\n",
        "            try:\n",
        "                stat, p_value = mannwhitneyu(groups[i], groups[j], alternative='two-sided')\n",
        "                n1, n2 = len(groups[i]), len(groups[j])\n",
        "                effect_size = 1 - (2 * stat) / (n1 * n2)  # Common language effect size\n",
        "\n",
        "                if p_value < alpha:\n",
        "                    results.append({\n",
        "                        'group1': group_names[i],\n",
        "                        'group2': group_names[j],\n",
        "                        'p_value': p_value,\n",
        "                        'effect_size': effect_size\n",
        "                    })\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    return results\n",
        "\n",
        "for layer in layers:\n",
        "    # レイヤー名を設定\n",
        "    if isinstance(layer, int):\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "    else:\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"\\nAnalysis for {layer_name}\")\n",
        "    print('='*80)\n",
        "\n",
        "    # 欠損値を除外してデータを準備\n",
        "    data_for_analysis = df_slit[[layer_name, 'class_name']].dropna()\n",
        "\n",
        "    if len(data_for_analysis) > 0:\n",
        "        # クラスごとのデータを収集（欠損値を除外）\n",
        "        class_groups = []\n",
        "        valid_class_names = []\n",
        "\n",
        "        for class_name in class_names:\n",
        "            group_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna()\n",
        "            if len(group_data) > 0:\n",
        "                class_groups.append(group_data)\n",
        "                valid_class_names.append(class_name)\n",
        "\n",
        "        if len(class_groups) > 1:  # 少なくとも2つのグループが必要\n",
        "            # 全ての値が同じかチェック\n",
        "            is_all_same, same_value = check_all_values_same(class_groups)\n",
        "\n",
        "            if is_all_same:\n",
        "                print(f\"\\n全てのクラスで同じ値: {same_value:.4f}\")\n",
        "                print(\"統計的検定は不要です。\")\n",
        "                continue\n",
        "\n",
        "            # Kruskal-Wallis H-testを実行\n",
        "            h_statistic, p_value = stats.kruskal(*class_groups)\n",
        "\n",
        "            print(f\"\\nKruskal-Wallis H-test結果:\")\n",
        "            print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                # ペアワイズMann-Whitney U検定を実行\n",
        "                significant_pairs = perform_pairwise_mannwhitney(\n",
        "                    class_groups,\n",
        "                    valid_class_names\n",
        "                )\n",
        "\n",
        "                if significant_pairs:\n",
        "                    print(\"\\n有意差のあるペア (p < 0.05):\")\n",
        "                    for result in significant_pairs:\n",
        "                        print(f\"{result['group1']:25} vs {result['group2']:25}: \"\n",
        "                              f\"p = {result['p_value']:.4e}, effect size = {result['effect_size']:.4f}\")\n",
        "        else:\n",
        "            print(f\"\\n警告: {layer_name}の解析に十分なデータがありません。\")\n",
        "    else:\n",
        "        print(f\"\\n警告: {layer_name}に有効なデータがありません。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NShTeYYE3VEr",
        "outputId": "e35d081f-480c-4c59-d3a1-b0a860b0d097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer17\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 17.9227\n",
            "p-value: 2.1813e-02\n",
            "\n",
            "有意差のあるペア (p < 0.05):\n",
            "Infectious keratitis      vs Scar                     : p = 2.2595e-03, effect size = -0.4349\n",
            "Infectious keratitis      vs Tumor                    : p = 4.3770e-04, effect size = -0.5172\n",
            "Infectious keratitis      vs Deposit                  : p = 3.4077e-02, effect size = -0.3068\n",
            "Infectious keratitis      vs APAC                     : p = 3.9997e-02, effect size = -0.4545\n",
            "Infectious keratitis      vs Lens opacity             : p = 3.8009e-04, effect size = -0.5981\n",
            "Infectious keratitis      vs Bullous keratopathy      : p = 7.7172e-03, effect size = -0.4079\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer20\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 14.8646\n",
            "p-value: 6.1833e-02\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer23\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 98.2819\n",
            "p-value: 9.5787e-18\n",
            "\n",
            "有意差のあるペア (p < 0.05):\n",
            "Normal                    vs Infectious keratitis     : p = 2.0957e-03, effect size = -0.4487\n",
            "Normal                    vs Non-infection keratitis  : p = 4.6782e-04, effect size = 0.6774\n",
            "Normal                    vs Scar                     : p = 6.4358e-06, effect size = 0.6528\n",
            "Normal                    vs Tumor                    : p = 9.5263e-05, effect size = 0.5828\n",
            "Normal                    vs APAC                     : p = 4.3688e-03, effect size = -0.6344\n",
            "Normal                    vs Lens opacity             : p = 1.1138e-02, effect size = 0.4329\n",
            "Infectious keratitis      vs Non-infection keratitis  : p = 4.0088e-06, effect size = 0.8834\n",
            "Infectious keratitis      vs Scar                     : p = 1.1665e-07, effect size = 0.7540\n",
            "Infectious keratitis      vs Tumor                    : p = 1.0299e-05, effect size = 0.6485\n",
            "Infectious keratitis      vs Deposit                  : p = 1.9169e-02, effect size = 0.3390\n",
            "Infectious keratitis      vs Lens opacity             : p = 5.5807e-06, effect size = 0.7640\n",
            "Infectious keratitis      vs Bullous keratopathy      : p = 4.4250e-05, effect size = 0.6247\n",
            "Non-infection keratitis   vs Deposit                  : p = 1.2420e-05, effect size = -0.8413\n",
            "Non-infection keratitis   vs APAC                     : p = 1.8356e-04, effect size = -0.9658\n",
            "Non-infection keratitis   vs Lens opacity             : p = 5.7299e-03, effect size = -0.5870\n",
            "Non-infection keratitis   vs Bullous keratopathy      : p = 6.4636e-04, effect size = -0.6805\n",
            "Scar                      vs Deposit                  : p = 1.2680e-08, effect size = -0.8162\n",
            "Scar                      vs APAC                     : p = 3.5528e-05, effect size = -0.9085\n",
            "Scar                      vs Lens opacity             : p = 6.2173e-04, effect size = -0.5728\n",
            "Scar                      vs Bullous keratopathy      : p = 1.7963e-05, effect size = -0.6516\n",
            "Tumor                     vs Deposit                  : p = 1.1342e-06, effect size = -0.7208\n",
            "Tumor                     vs APAC                     : p = 3.8497e-04, effect size = -0.7926\n",
            "Tumor                     vs Lens opacity             : p = 9.8957e-04, effect size = -0.5649\n",
            "Tumor                     vs Bullous keratopathy      : p = 7.7762e-05, effect size = -0.6179\n",
            "Deposit                   vs APAC                     : p = 2.2393e-02, effect size = -0.5069\n",
            "Deposit                   vs Lens opacity             : p = 7.9724e-05, effect size = 0.6678\n",
            "Deposit                   vs Bullous keratopathy      : p = 2.3572e-03, effect size = 0.4688\n",
            "APAC                      vs Lens opacity             : p = 8.3075e-05, effect size = 0.9415\n",
            "APAC                      vs Bullous keratopathy      : p = 2.3333e-04, effect size = 0.8376\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_0\n",
            "================================================================================\n",
            "\n",
            "全てのクラスで同じ値: 0.0000\n",
            "統計的検定は不要です。\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_1\n",
            "================================================================================\n",
            "\n",
            "全てのクラスで同じ値: 0.0000\n",
            "統計的検定は不要です。\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analysis for AOI_0.5_layer24_m_2\n",
            "================================================================================\n",
            "\n",
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 84.7932\n",
            "p-value: 5.2761e-15\n",
            "\n",
            "有意差のあるペア (p < 0.05):\n",
            "Normal                    vs Infectious keratitis     : p = 2.3201e-05, effect size = -0.6168\n",
            "Normal                    vs Non-infection keratitis  : p = 3.6507e-03, effect size = 0.5633\n",
            "Normal                    vs Scar                     : p = 2.2951e-04, effect size = 0.5332\n",
            "Normal                    vs Tumor                    : p = 2.4453e-05, effect size = 0.6301\n",
            "Normal                    vs Deposit                  : p = 4.3993e-02, effect size = -0.2964\n",
            "Normal                    vs Lens opacity             : p = 3.4122e-02, effect size = 0.3616\n",
            "Normal                    vs Bullous keratopathy      : p = 4.4571e-03, effect size = 0.4417\n",
            "Infectious keratitis      vs Non-infection keratitis  : p = 3.7397e-05, effect size = 0.7902\n",
            "Infectious keratitis      vs Scar                     : p = 8.3081e-10, effect size = 0.8734\n",
            "Infectious keratitis      vs Tumor                    : p = 2.7847e-06, effect size = 0.6889\n",
            "Infectious keratitis      vs Deposit                  : p = 2.9880e-02, effect size = 0.3144\n",
            "Infectious keratitis      vs APAC                     : p = 1.0952e-02, effect size = 0.5623\n",
            "Infectious keratitis      vs Lens opacity             : p = 1.0401e-05, effect size = 0.7416\n",
            "Infectious keratitis      vs Bullous keratopathy      : p = 3.6722e-10, effect size = 0.9580\n",
            "Non-infection keratitis   vs Deposit                  : p = 4.3364e-04, effect size = -0.6779\n",
            "Scar                      vs Tumor                    : p = 4.2495e-03, effect size = 0.4176\n",
            "Scar                      vs Deposit                  : p = 1.0491e-06, effect size = -0.7004\n",
            "Tumor                     vs Deposit                  : p = 6.3253e-06, effect size = -0.6687\n",
            "Tumor                     vs APAC                     : p = 4.3709e-02, effect size = -0.4519\n",
            "Tumor                     vs Lens opacity             : p = 2.8844e-02, effect size = -0.3754\n",
            "Tumor                     vs Bullous keratopathy      : p = 1.3958e-03, effect size = -0.5000\n",
            "Deposit                   vs APAC                     : p = 3.6195e-02, effect size = 0.4653\n",
            "Deposit                   vs Lens opacity             : p = 2.9648e-03, effect size = 0.5033\n",
            "Deposit                   vs Bullous keratopathy      : p = 1.6502e-05, effect size = 0.6635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 解析するレイヤーのリスト\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# データを長形式に変換\n",
        "df_melted = pd.melt(df[layers], var_name='Layer', value_name='AOI Value')\n",
        "\n",
        "# レイヤー名を簡略化\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "df_melted['Layer'] = df_melted['Layer'].map(layer_mapping)\n",
        "\n",
        "# 欠損値を除外\n",
        "df_melted = df_melted.dropna()\n",
        "\n",
        "# Kruskal-Wallis H-testを実行\n",
        "groups = [group['AOI Value'].values for name, group in df_melted.groupby('Layer')]\n",
        "h_statistic, p_value = stats.kruskal(*groups)\n",
        "\n",
        "print(\"Kruskal-Wallis H-test結果:\")\n",
        "print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "# レイヤーのペアワイズ比較（Mann-Whitney U test）を実行\n",
        "layer_names = sorted(df_melted['Layer'].unique())\n",
        "significant_pairs = []\n",
        "\n",
        "print(\"\\n有意差のあるペア (p < 0.05):\")\n",
        "for i, layer1 in enumerate(layer_names):\n",
        "    for layer2 in layer_names[i+1:]:\n",
        "        group1 = df_melted[df_melted['Layer'] == layer1]['AOI Value']\n",
        "        group2 = df_melted[df_melted['Layer'] == layer2]['AOI Value']\n",
        "\n",
        "        try:\n",
        "            stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
        "            n1, n2 = len(group1), len(group2)\n",
        "            effect_size = 1 - (2 * stat) / (n1 * n2)  # Common language effect size\n",
        "\n",
        "            if p_value < 0.05:  # Bonferroni補正を適用する場合は 0.05/len(pairs) を使用\n",
        "                print(f\"{layer1:10} vs {layer2:10}: p = {p_value:.4e}, effect size = {effect_size:.4f}\")\n",
        "                significant_pairs.append((layer1, layer2, p_value, effect_size))\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "# 各レイヤーの基本統計量（中央値と四分位数を含む）\n",
        "print(\"\\n各レイヤーの基本統計量:\")\n",
        "stats_summary = df_melted.groupby('Layer')['AOI Value'].agg([\n",
        "    ('count', 'count'),\n",
        "    ('median', 'median'),\n",
        "    ('mean', 'mean'),\n",
        "    ('std', 'std'),\n",
        "    ('Q1', lambda x: x.quantile(0.25)),\n",
        "    ('Q3', lambda x: x.quantile(0.75))\n",
        "]).round(4)\n",
        "\n",
        "print(stats_summary)\n",
        "\n",
        "# マトリックス形式で有意差を表示\n",
        "print(\"\\n有意差マトリックス (★: p < 0.05)\")\n",
        "matrix = pd.DataFrame(index=layer_names, columns=layer_names)\n",
        "np.fill_diagonal(matrix.values, '-')\n",
        "matrix = matrix.fillna('　')\n",
        "\n",
        "for pair in significant_pairs:\n",
        "    matrix.loc[pair[0], pair[1]] = \"★\"\n",
        "    matrix.loc[pair[1], pair[0]] = \"★\"\n",
        "\n",
        "print(matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJkb_wra3VGX",
        "outputId": "462b1142-f699-4bdc-a5b3-2ec012506bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kruskal-Wallis H-test結果:\n",
            "H-statistic: 1075.0618\n",
            "p-value: 3.3610e-230\n",
            "\n",
            "有意差のあるペア (p < 0.05):\n",
            "Layer 17   vs Layer 20  : p = 1.7921e-09, effect size = -0.3264\n",
            "Layer 17   vs Layer 23  : p = 1.3526e-20, effect size = -0.5048\n",
            "Layer 17   vs Layer 24_0: p = 1.5463e-83, effect size = -0.9780\n",
            "Layer 17   vs Layer 24_1: p = 1.5463e-83, effect size = -0.9780\n",
            "Layer 17   vs Layer 24_2: p = 5.2958e-44, effect size = -0.7548\n",
            "Layer 20   vs Layer 23  : p = 3.5406e-09, effect size = -0.3203\n",
            "Layer 20   vs Layer 24_0: p = 1.5463e-83, effect size = -0.9780\n",
            "Layer 20   vs Layer 24_1: p = 1.5463e-83, effect size = -0.9780\n",
            "Layer 20   vs Layer 24_2: p = 1.4529e-46, effect size = -0.7774\n",
            "Layer 23   vs Layer 24_0: p = 1.9684e-86, effect size = -1.0000\n",
            "Layer 23   vs Layer 24_1: p = 1.9684e-86, effect size = -1.0000\n",
            "Layer 23   vs Layer 24_2: p = 7.0350e-40, effect size = -0.7171\n",
            "Layer 24_0 vs Layer 24_2: p = 5.7682e-83, effect size = 0.9736\n",
            "Layer 24_1 vs Layer 24_2: p = 5.7682e-83, effect size = 0.9736\n",
            "\n",
            "各レイヤーの基本統計量:\n",
            "            count  median    mean     std      Q1      Q3\n",
            "Layer                                                    \n",
            "Layer 17      227  0.4042  0.3627  0.2008  0.2019  0.5234\n",
            "Layer 20      227  0.2569  0.2629  0.1501  0.1512  0.3702\n",
            "Layer 23      227  0.1524  0.1851  0.1189  0.1004  0.2474\n",
            "Layer 24_0    227  0.0000  0.0000  0.0000  0.0000  0.0000\n",
            "Layer 24_1    227  0.0000  0.0000  0.0000  0.0000  0.0000\n",
            "Layer 24_2    227  0.0543  0.0643  0.0481  0.0318  0.0828\n",
            "\n",
            "有意差マトリックス (★: p < 0.05)\n",
            "           Layer 17 Layer 20 Layer 23 Layer 24_0 Layer 24_1 Layer 24_2\n",
            "Layer 17          -        ★        ★          ★          ★          ★\n",
            "Layer 20          ★        -        ★          ★          ★          ★\n",
            "Layer 23          ★        ★        -          ★          ★          ★\n",
            "Layer 24_0        ★        ★        ★          -          　          ★\n",
            "Layer 24_1        ★        ★        ★          　          -          ★\n",
            "Layer 24_2        ★        ★        ★          ★          ★          -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59zc1fuTqBK7"
      },
      "source": [
        "###**ANOVA_heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exil8PT2ZzsS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'})\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6M5cbfdFlv3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# カスタムカラーマップを作成（オレンジ、グレー、白）\n",
        "colors = ['#FFA500', '#D3D3D3', 'white']  # オレンジ、グレー、白\n",
        "custom_cmap = ListedColormap(colors)\n",
        "\n",
        "# p値に基づいてデータを作成（NaNは2、p<0.05は0、それ以外は1）\n",
        "color_data = np.where(np.isnan(matrix), 2, np.where(matrix < 0.05, 0, 1))\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(color_data,\n",
        "            annot=matrix,  # 元のp値を表示\n",
        "            fmt=\".3f\",\n",
        "            cmap=custom_cmap,\n",
        "            cbar=False,  # カラーバーを非表示に\n",
        "            xticklabels=class_names,  # x軸のラベルをクラス名に設定\n",
        "            yticklabels=class_names)  # y軸のラベルをクラス名に設定\n",
        "\n",
        "# カスタムカラーバーを追加\n",
        "sm = plt.cm.ScalarMappable(cmap=ListedColormap(colors[:2]), norm=plt.Normalize(vmin=0, vmax=1))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, label='p-value', ticks=[0.25, 0.75])\n",
        "cbar.set_ticklabels(['p < 0.05', 'p ≥ 0.05'])\n",
        "\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.xticks(rotation=45, ha='right')  # x軸のラベルを45度回転\n",
        "plt.yticks(rotation=0)  # y軸のラベルを水平に\n",
        "plt.tight_layout()  # レイアウトを調整\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbxW3P2OqFTb"
      },
      "source": [
        "###**Tukey_sumaho/slit別**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m7Z9FCzbSmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "def perform_anova_and_tukey(df, title):\n",
        "    # 欠損値を削除\n",
        "    df = df.dropna(subset=[layer_name])\n",
        "\n",
        "    # ANOVAを実行\n",
        "    anova_result = stats.f_oneway(\n",
        "        df[df['class_name'] == 'infection'][layer_name],\n",
        "        df[df['class_name'] == 'normal'][layer_name],\n",
        "        df[df['class_name'] == 'non-infection'][layer_name],\n",
        "        df[df['class_name'] == 'scar'][layer_name],\n",
        "        df[df['class_name'] == 'tumor'][layer_name],\n",
        "        df[df['class_name'] == 'deposit'][layer_name],\n",
        "        df[df['class_name'] == 'APAC'][layer_name],\n",
        "        df[df['class_name'] == 'lens opacity'][layer_name],\n",
        "        df[df['class_name'] == 'bullous'][layer_name]\n",
        "    )\n",
        "\n",
        "    print(f\"{title} ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "    # 事後検定（TukeyのHSD検定）を実行\n",
        "    tukey_result = pairwise_tukeyhsd(endog=df[layer_name], groups=df['class_name'])\n",
        "\n",
        "    # 結果をDataFrameに変換\n",
        "    tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "    # マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "    matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "    # マトリックスにp値を入力\n",
        "    for i in range(len(tukey_result_df)):\n",
        "        group1 = tukey_result_df.loc[i, 'group1']\n",
        "        group2 = tukey_result_df.loc[i, 'group2']\n",
        "        p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "        matrix.loc[group1, group2] = p_value\n",
        "        matrix.loc[group2, group1] = p_value\n",
        "\n",
        "    # 同じクラス間のセルをNaNに設定\n",
        "    np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "    # ヒートマップを描画\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'}, mask=matrix.isnull())\n",
        "    plt.title(f'Pairwise Comparison P-Values (Tukey HSD) - {title}')\n",
        "    plt.show()\n",
        "\n",
        "# スリットランプデータの解析\n",
        "perform_anova_and_tukey(df_slit, \"Slit Lamp Data\")\n",
        "\n",
        "# スマートフォンデータの解析\n",
        "perform_anova_and_tukey(df_sumaho, \"Smartphone Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ubPn9Erqnid",
        "outputId": "a4e19c97-a3c2-44cd-9dac-99c4b7acdb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t-statistic: 5.652411183767835, p-value: 2.8095252129652275e-08\n",
            "scar + non-infection mean: 0.11625931718387256, std: 0.08804314854287512\n",
            "others mean: 0.07454914396969002, std: 0.05818428866497734\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 両方のデータを結合\n",
        "df_combined = pd.concat([df_slit, df_sumaho]).reset_index(drop=True)\n",
        "\n",
        "# \"scar\" + \"non-infection\" グループとその他のクラスに分類\n",
        "df_combined['group'] = df_combined['class_name'].apply(lambda x: 'scar + non-infection' if x in ['scar', 'non-infection'] else 'others')\n",
        "\n",
        "# グループごとにデータを抽出\n",
        "scar_non_infection_group = df_combined[df_combined['group'] == 'scar + non-infection'][layer_name].dropna()\n",
        "others_group = df_combined[df_combined['group'] == 'others'][layer_name].dropna()\n",
        "\n",
        "# t検定を実行\n",
        "t_stat, p_value = stats.ttest_ind(scar_non_infection_group, others_group)\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
        "\n",
        "# 統計値を計算\n",
        "scar_non_infection_mean = scar_non_infection_group.mean()\n",
        "scar_non_infection_std = scar_non_infection_group.std()\n",
        "others_mean = others_group.mean()\n",
        "others_std = others_group.std()\n",
        "\n",
        "print(f\"scar + non-infection mean: {scar_non_infection_mean}, std: {scar_non_infection_std}\")\n",
        "print(f\"others mean: {others_mean}, std: {others_std}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcxqbJ_GrbPY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNxI+YB3SiZAZeJttl0YLy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}