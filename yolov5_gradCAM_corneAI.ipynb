{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPHQPOAIr/ZxWVYwsyTRNHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/yolov5_gradCAM_corneAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d42M6k9QpvSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5_GradCAM_CorneAI**\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "id": "2cakhs2BZLRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "ytBOWsuXZQzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "P7oJE1rLrE_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7073c57-bd76-428f-9ad1-8c3b9d4cbb48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p48tU-_wYVHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d21362-8c6f-436e-8224-c528b2dc0d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 35.39 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ],
      "metadata": {
        "id": "wkvc2KijYes5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python main.py --model-path $model_path --img-path $img_path --output-dir out"
      ],
      "metadata": {
        "id": "cjgRkuSuYinZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Top3 analysis**"
      ],
      "metadata": {
        "id": "wvBLe9pbpCwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit\"\n",
        "print(len(os.listdir(img_dir)))"
      ],
      "metadata": {
        "id": "GWizQkPAjczd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ebdef3-95b8-489c-ef56-927f327cf4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "FN98dpHRuyNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c32ecff-f6bc-493e-d360-42793dcbd660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rMIlXO_mIun8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAM**"
      ],
      "metadata": {
        "id": "uoTET4OvDiRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GradCAM通常バージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "# 以下の関数は変更なし\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "# YOLOV5GradCAMクラスを更新\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []  # クラス名を格納するリストを初期化\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        return alpha.view(b, k, 1, 1)\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)\n",
        "        weights = (positive_gradients * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        return eigen_cam.unsqueeze(1)\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[200:234]\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        #cv2.imwrite(output_path, final_image)\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー →Best\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    ##target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    ##target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "    #for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ],
      "metadata": {
        "id": "6hBb7eKP986c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GradCAM\n",
        "広く見るバージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[210:211] ###########画像を指定\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img = apply_heatmap_to_entire_image(masks[i], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "ojsVa-JDefA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def print_model_structure(model):\n",
        "    \"\"\"\n",
        "    モデルの構造を表示する関数\n",
        "    \"\"\"\n",
        "    print(\"Model structure:\")\n",
        "    for i, (name, module) in enumerate(model.model.named_modules()):\n",
        "        if not isinstance(module, torch.nn.Sequential) and not isinstance(module, torch.nn.ModuleList):\n",
        "            print(f\"{i}: {name} - {module.__class__.__name__}\")\n",
        "\n",
        "def find_yolo_layer(model, layer_index):\n",
        "    \"\"\"\n",
        "    YOLOv5モデルの後ろから指定されたインデックスのレイヤーを返す\n",
        "    \"\"\"\n",
        "    module_list = list(model.model.modules())\n",
        "    module_list = [m for m in module_list if not isinstance(m, torch.nn.Sequential) and not isinstance(m, torch.nn.ModuleList)]\n",
        "    target_layer = module_list[-layer_index]\n",
        "    return target_layer, f\"{target_layer.__class__.__name__} (index: -{layer_index})\"\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_index, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer, layer_name = find_yolo_layer(self.model, layer_index)\n",
        "        self.layer_name = layer_name\n",
        "        print(f\"Selected layer: {self.layer_name}\")\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method, saliency_method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    print(f\"Using layer: {saliency_method.layer_name}\")\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[200:234]\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img = apply_heatmap_to_entire_image(masks[i], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    layer_index = 1  # モデルの後ろから1番目のレイヤーを選択\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'\n",
        "\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=(img_size, img_size), names=names)\n",
        "\n",
        "    for method in ['gradcam', 'gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_index=layer_index, img_size=(img_size, img_size), method=method)\n",
        "            folder_main(img_path, method, saliency_method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "uS-LWXlVgFpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3RmQue4SefDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**EigenCAM**"
      ],
      "metadata": {
        "id": "RiAwrM4475X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        # GradCAMの重み計算\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        # GradCAM++の重み計算\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        res_img = result.copy()\n",
        "        for j, mask in enumerate(masks[i]):\n",
        "            bbox = boxes[0][j]\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "            res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    #####ファイル名を数字でソート################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[6:15]\n",
        "    #############################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            for j, mask in enumerate(masks[i]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        #cv2.imwrite(output_path, final_image)\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_0'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit'  # 出力ディレクトリ\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "\n",
        "    # GradCAMとGradCAM++の両方を実行\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['eigencam']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)\n"
      ],
      "metadata": {
        "id": "_KXqjIkf1yg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3G5YQn4dBm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EigenCAM\n",
        "広く可視化するバージョン\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.7, n_heatmap, 0.3, 0)\n",
        "    return res_img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        res_img = apply_heatmap_to_entire_image(mask, res_img)\n",
        "    return res_img, None\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def folder_main(folder_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[6:15]\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method=method)\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            res_img = result.copy()\n",
        "            res_img, _ = get_res_img(None, [masks[i]], res_img)\n",
        "            for j, bbox in enumerate(boxes[0]):\n",
        "                res_img = put_text_box(bbox, cls_names[i], res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "\n",
        "        cv2_imshow(final_image)\n",
        "\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "    target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho'  # 出力ディレクトリ\n",
        "\n",
        "    for method in ['eigencam']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "kRvM0njcdBov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2RWDTh3dBqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o_y_jlLBdBsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsuMkl6BdBus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=(img_size, img_size), names=names)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "7-Ydkg4tdBwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.exit()"
      ],
      "metadata": {
        "id": "SFDwQ_bfIYaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LayerごとTop1の可視化**"
      ],
      "metadata": {
        "id": "MDjAHX8v7tS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                          'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "                          'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
        "                          'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "                          'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "                          'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img): ##マージしているレイヤー名\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, f\"{cls_names[0]} - {method} - {layer_name}\", res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    #cv2_imshow(final_image)\n",
        "    cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path, method, file_list):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv'] ##\n",
        "        target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "        ##target_layers = ['model_17_m_0_cv2_conv', 'model_20_m_0_cv2_conv', 'model_23_m_0_cv2_conv']\n",
        "        #target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            print(f\"boxes: {boxes}\")\n",
        "\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, f\"{cls_names[0]}-{'layer'+layer_name.replace('model_', '')}\", res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image) ##画像を表示\n",
        "        #cv2.imwrite(output_path, final_image) ##画像を保存\n",
        "\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer24'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho_layer24'  # 出力ディレクトリ\n",
        "\n",
        "    # ファイル名を数字でソート\n",
        "    if os.path.isdir(img_path):\n",
        "        file_list = sorted(os.listdir(img_path), key=lambda x: int(x.split('.')[0]))\n",
        "        ##########必要に応じて、処理するファイルの範囲を指定########\n",
        "        file_list = file_list[210:211]\n",
        "        ############################################################\n",
        "\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method, file_list)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "Xwry3nj9B9ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbQBtqIHTaCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yvJ9wVXLUSov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0HgW2xNUSrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "## 画像全体（bbox外）も表示\n",
        "#################################\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                          'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "                          'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
        "                          'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "                          'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "                          'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "def apply_heatmap_to_entire_image(mask, image):\n",
        "    mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    n_heatmap = (heatmap / 255).astype(np.float32)\n",
        "    image = image / 255\n",
        "    res_img = cv2.addWeighted(image, 0.5, n_heatmap, 0.5, 0)  ###ここでヒートマップと元画像のマージの比率を決める\n",
        "    return res_img\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        res_img = apply_heatmap_to_entire_image(mask, res_img)\n",
        "    return res_img, None\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img): ##マージしているレイヤー名\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "\n",
        "    # バウンディングボックスを太く描画\n",
        "    thickness = 3  # 線の太さを増やす（必要に応じて調整してください）\n",
        "    res_img = Box.put_box(res_img, bbox, thickness=thickness)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    text_thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, text_thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, text_thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "\n",
        "def main(img_path, method):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "    #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "    target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "    images = [img]  # 元画像を追加\n",
        "\n",
        "    for layer_name in target_layers:\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"{layer_name} time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "        res_img = result.copy()\n",
        "        if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "            for j, mask in enumerate(masks[0]):\n",
        "                bbox = boxes[0][j]\n",
        "                res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                res_img = put_text_box(bbox, f\"{cls_names[0]} - {method} - {layer_name}\", res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    #cv2_imshow(final_image)\n",
        "    cv2.imwrite(output_path, final_image)\n",
        "\n",
        "\n",
        "def folder_main(folder_path, method, file_list):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    for index, item in enumerate(file_list, start=1):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {index}: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(padded_img[..., ::-1])\n",
        "\n",
        "        #target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv'] ##\n",
        "        target_layers = ['model_17_cv3_act', 'model_20_cv3_act', 'model_23_cv3_act'] ##\n",
        "        ##target_layers = ['model_17_m_0_cv2_conv', 'model_20_m_0_cv2_conv', 'model_23_m_0_cv2_conv']\n",
        "        #target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2'] ##\n",
        "        images = [padded_img]  # 元画像を追加\n",
        "\n",
        "        for layer_name in target_layers:\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=method)\n",
        "            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "            print(f\"boxes: {boxes}\")\n",
        "\n",
        "            result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "            result = result[..., ::-1]  # convert to bgr\n",
        "\n",
        "            res_img = result.copy()\n",
        "            if len(masks) > 0:  # masksが空でない場合のみ処理を行う\n",
        "                for j, mask in enumerate(masks[0]):\n",
        "                    bbox = boxes[0][j]\n",
        "                    res_img, _ = get_res_img(bbox, [mask], res_img)\n",
        "                    res_img = put_text_box(bbox, f\"{cls_names[0]}-{'layer'+layer_name.replace('model_', '')}\", res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix=f'-res-{method}')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image) ##画像を表示\n",
        "        #cv2.imwrite(output_path, final_image) ##画像を保存\n",
        "\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method, masks, logits, boxes, images, final_image\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    device = 'cpu'  # 'cuda' または 'cpu'\n",
        "    img_size = 640  # 入力画像サイズ\n",
        "\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_slit_layer24'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam++_img_sumaho_layer24'  # 出力ディレクトリ\n",
        "\n",
        "    # ファイル名を数字でソート\n",
        "    if os.path.isdir(img_path):\n",
        "        file_list = sorted(os.listdir(img_path), key=lambda x: int(x.split('.')[0]))\n",
        "        ##########必要に応じて、処理するファイルの範囲を指定########\n",
        "        file_list = file_list[210:211]\n",
        "        ############################################################\n",
        "\n",
        "    #for method in ['gradcam', 'gradcampp', 'eigencam']:\n",
        "    #for method in ['gradcam', 'gradcampp']:\n",
        "    for method in ['gradcampp']:\n",
        "        print(f\"{method.upper()}を実行中:\")\n",
        "        if os.path.isdir(img_path):\n",
        "            folder_main(img_path, method, file_list)\n",
        "        else:\n",
        "            main(img_path, method)"
      ],
      "metadata": {
        "id": "0FyVE0SKUStt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaJLDNoaA8D1",
        "outputId": "e533cfd2-1039-4839-9b12-dbe2c4a1739e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPJ1d31SasiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (640,640)\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "HQ0eRLpgoi_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q81i6ptncfDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hauGiazwcfG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXvgJcOytU51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sort GradCam Images**\n",
        "\n",
        "GradCAM画像をクラスごとにフォルダ分け"
      ],
      "metadata": {
        "id": "YHLMn6kZIlSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara234.csv\"\n",
        "\n",
        "# CSVファイルを読み込み\n",
        "df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
        "\n",
        "# データフレームの最初の数行を表示\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZaYBsjL7NTcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dirs = [\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit',\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_sumaho'\n",
        "]\n",
        "destination_base_dirs = [\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_slit_sorted',\n",
        "    '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_sumaho_sorted'\n",
        "]\n",
        "\n",
        "# Iterate over the source and destination directory pairs\n",
        "for source_dir, destination_base_dir in zip(source_dirs, destination_base_dirs):\n",
        "    print(f'Processing files from {source_dir} to {destination_base_dir}')\n",
        "    # Iterate over the dataframe and copy files to the appropriate class folder\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        serial_number = row['serial_number']\n",
        "        class_num = row['class_num']\n",
        "        source_file = os.path.join(source_dir, f'{serial_number}-res.jpg')\n",
        "        destination_dir = os.path.join(destination_base_dir, str(class_num))\n",
        "\n",
        "        # Create the destination directory if it does not exist\n",
        "        os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "        # Copy the file to the destination directory\n",
        "        destination_file = os.path.join(destination_dir, f'{serial_number}-res.jpg')\n",
        "        if os.path.exists(source_file):\n",
        "            shutil.copy(source_file, destination_file)\n",
        "        else:\n",
        "            print(f'File {source_file} does not exist.')\n",
        "\n",
        "print('Files have been copied successfully.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBll2vlDM26D",
        "outputId": "f238c4e9-9e95-44f6-cffc-3b90ec61e326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Processing files from /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit to /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_slit_sorted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 228/228 [00:02<00:00, 88.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing files from /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_sumaho to /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_sumaho_sorted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 228/228 [00:02<00:00, 89.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been copied successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Compare area of interest**\n",
        "\n",
        "注目度n%以上の画像を"
      ],
      "metadata": {
        "id": "dxtwlXulSgO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ニ値化して表示**"
      ],
      "metadata": {
        "id": "2lQctr8O90Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###画像として表示"
      ],
      "metadata": {
        "id": "VllpVTyg9zYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "6iDm_dPKM28s"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "#img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "#output_dir = 'out'  # 出力ディレクトリ\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "img_size = 640  # 入力画像サイズ\n",
        "target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "#target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "method = 'gradcam'  # 'gradcam' または 'gradcampp'\n",
        "device = 'cpu'  # 'cuda' または 'cpu'\n",
        "\n",
        "# クラス名リストを指定\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_res_img(bbox, masks, res_img, threshold):\n",
        "    total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        mask[mask < threshold*255] = 0  # 追加: 128未満の値を0にする\n",
        "        mask[mask >= threshold*255] = 255  # 追加: 128未満の値を0にする\n",
        "        binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "\n",
        "        # bboxの範囲内のマスクの部分を取得\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "        # 閾値を超える共通部分のピクセル数をカウント\n",
        "        intersect_pixels = np.sum(mask_bbox)\n",
        "        total_intersect_pixels += intersect_pixels\n",
        "\n",
        "        # mask_bbox のピクセル数を取得\n",
        "        mask_bbox_area = mask_bbox.size\n",
        "\n",
        "        # AOI を計算\n",
        "        AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "        print(f\"Area of Interest (AOI): {AOI}\")\n",
        "\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # this is a bug in cv2. It does not put box on a converted image from torch unless it's buffered and read again!\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    # Set the font size and other parameters\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)  # Green color in BGR format\n",
        "    thickness = 2\n",
        "\n",
        "    # Calculate the text size and position\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10  # Adjust the horizontal position to place the text near the left edge\n",
        "    text_y = text_size[1] + 10  # Adjust the vertical position to place the text near the top edge\n",
        "\n",
        "    # Put the text on the image\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        model: yolov5 model.\n",
        "        layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer\n",
        "    \"\"\"\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "\n",
        "def main(img_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size,\n",
        "                                      names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "\n",
        "    if len(masks) > 0:\n",
        "        mask = masks[0][0]  # top1のマスクのみ使用\n",
        "        bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "        cls_name = cls_names[0]  # top1のクラス名のみ使用\n",
        "        res_img = result.copy()\n",
        "        res_img, _ = get_res_img(bbox, [mask], res_img, GradCAM_threshold)\n",
        "        res_img = put_text_box(bbox, cls_name, res_img)\n",
        "        images.append(res_img)\n",
        "\n",
        "    final_image = concat_images(images)\n",
        "    img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "    output_path = f'{output_dir}/{img_name}'\n",
        "    #os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f'[INFO] Saving the final image at {output_path}')\n",
        "    cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    ##############################################################################################\n",
        "    ######## ファイル名を数字でソート#############################################################\n",
        "    file_list = sorted(os.listdir(folder_path), key=lambda x: int(x.split('.')[0]))[0:5]\n",
        "    ##############################################################################################\n",
        "    ##############################################################################################\n",
        "\n",
        "    for item in file_list:\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img_basename = os.path.basename(img_path)\n",
        "        print(f\"Processing image: {img_basename}\")\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # 元の画像の縦横比を維持しながらリサイズ\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = img_size, int(img_size * w / h)\n",
        "        else:\n",
        "            new_h, new_w = int(img_size * h / w), img_size\n",
        "        img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "        # パディングを追加して640×640pxにする\n",
        "        delta_w = img_size - new_w\n",
        "        delta_h = img_size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "        # if method == 'gradcam':\n",
        "        #     saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size, method= \"gradcampp\")\n",
        "\n",
        "        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "        print(f\"boxes: {boxes}\")\n",
        "\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "\n",
        "        if len(masks) > 0:\n",
        "            mask = masks[0][0]  # top1のマスクのみ使用\n",
        "            bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "            cls_name = cls_names[0]  # top1のクラス名のみ使用\n",
        "            res_img = result.copy()\n",
        "            res_img, _ = get_res_img(bbox, [mask], res_img, GradCAM_threshold)\n",
        "            res_img = put_text_box(bbox, cls_name, res_img)\n",
        "            images.append(res_img)\n",
        "\n",
        "        final_image = concat_images(images)\n",
        "        img_name = split_extension(os.path.split(img_path)[-1], suffix='-res')\n",
        "        output_path = f'{output_dir}/{img_name}'\n",
        "        #os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f'[INFO] Saving the final image at {output_path}')\n",
        "        cv2_imshow(final_image)  # cv2.imwriteの代わりにcv2_imshowを使用\n",
        "\n",
        "        # オブジェクトを明示的に削除\n",
        "        del saliency_method\n",
        "        del masks\n",
        "        del logits\n",
        "        del boxes\n",
        "        del images\n",
        "        del final_image\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #target_layer = 'model_17_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_20_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_cv3_act'  # GradCAM を適用するレイヤー  →デフォルト\n",
        "    #target_layer = 'model_23_m_0_cv2_conv'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_23_m_0_cv2_act'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_0'  # GradCAM を適用するレイヤー\n",
        "    #target_layer = 'model_24_m_1'  # GradCAM を適用するレイヤー\n",
        "    target_layer = 'model_24_m_2'  # GradCAM を適用するレイヤー\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "    #img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "    #output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'  # 出力ディレクトリ\n",
        "\n",
        "    GradCAM_threshold = 0.5 #GradCAMの閾値設定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)"
      ],
      "metadata": {
        "id": "HV90u_Z9Urn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Area of interestの計算**\n",
        "\n",
        "結果をcsvに保存する"
      ],
      "metadata": {
        "id": "iEUdHWgh79uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# ###これを押すとcsvが更新されてしまうので注意！！！\n",
        "\n",
        "# # CSVファイルのパス\n",
        "# input_file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# # CSVファイルをDataFrameとして読み込む\n",
        "# df = pd.read_csv(input_file_path)\n",
        "\n",
        "# # 'Unnamed'が含まれる列を削除する\n",
        "# df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# # 例としてthresholdの値を設定\n",
        "# threshold = 0.5\n",
        "\n",
        "# # 指定されたレイヤー\n",
        "# layers = [\"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "# # 各レイヤーに対して新しい列を作成\n",
        "# for layer in layers:\n",
        "#     df[f'AOI_{threshold}_layer{layer}'] = None  # 初期値を設定\n",
        "\n",
        "# # 修正後のDataFrameを表示する\n",
        "# print(df.head())\n",
        "\n",
        "# # DataFrameをCSVファイルとして保存\n",
        "# df.to_csv(file_path, index=False)\n",
        "\n",
        "# print(f\"Updated DataFrame has been saved to: {file_path}\")\n"
      ],
      "metadata": {
        "id": "Up7ykhgI5l3n",
        "outputId": "c0dfe489-0247-4a42-f4a8-48c5f857fd38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   serial_number    basename disease_English  class_num  top1  ...  \\\n",
            "0              1   9TTR_R_01       infection          0     0  ...   \n",
            "1              2  12TTR_R_01       infection          0     0  ...   \n",
            "2              3  13TTR_R_01       infection          0     2  ...   \n",
            "3              4  21TTR_L_01       infection          0     0  ...   \n",
            "4              5  25TTR_R_01            APAC          6     6  ...   \n",
            "\n",
            "   AOI_0.5_layer24_m_1  AOI_0.5_layer24_m_2  AOI_0.5_layer0  AOI_0.5_layer1  \\\n",
            "0                 None                 None        0.241153        0.051920   \n",
            "1                 None                 None        0.000000        0.045741   \n",
            "2                 None                 None        0.000000        0.024565   \n",
            "3                 None                 None        0.219612        0.066713   \n",
            "4                 None                 None        0.664061        0.021391   \n",
            "\n",
            "   AOI_0.5_layer2  \n",
            "0        0.220417  \n",
            "1        0.141625  \n",
            "2        0.071921  \n",
            "3        0.000000  \n",
            "4        0.110155  \n",
            "\n",
            "[5 rows x 20 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'file_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e62639aef048>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# DataFrameをCSVファイルとして保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Updated DataFrame has been saved to: {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#################\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(self.img_size[0], xyxy[2])\n",
        "                    xyxy[3] = min(self.img_size[1], xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                      in_source=Box.BoxSource.Torch,\n",
        "                                      to_source=Box.BoxSource.Numpy,\n",
        "                                      return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "wj6fl4uVCCji"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS3iA_8QKWZC",
        "outputId": "62cb563e-f0b4-4fa1-fd18-cccd60deda76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxS9jH2fO59C",
        "outputId": "865799b6-b98e-42b0-ad24-259268719ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "##################\n",
        "## 3-layerで解析##\n",
        "##################\n",
        "##################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "   for mask in masks:\n",
        "       # マスクの前処理\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "       # bboxの範囲内のマスク部分を取得\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # 閾値を超える共通部分のピクセル数をカウント\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox のピクセル数を取得\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI を計算\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   # if len(bbox) == 0 or len(masks) == 0:\n",
        "   #     return 0.0\n",
        "\n",
        "   # for mask in masks:\n",
        "   #     mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "   #     mask[mask < threshold*255] = 0\n",
        "   #     mask[mask >= threshold*255] = 255\n",
        "   #     heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "   #     n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "   #     aoi = np.sum(n_heatmat > 0) / (n_heatmat.shape[0] * n_heatmat.shape[1])\n",
        "   #     print(f\"Area of Interest (original_AOI): {aoi}\")\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "   df = pd.read_csv(csv_path)\n",
        "   df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "  #  for layer_num in [17, 20, 23]:\n",
        "  #      if f'AOI_{threshold}_layer{layer_num}' not in df.columns:\n",
        "  #          df[f'AOI_{threshold}_layer{layer_num}'] = None\n",
        "   for layer_num in [0, 1, 2]:\n",
        "       if f'AOI_{threshold}_layer24_m_{layer_num}' not in df.columns:\n",
        "           df[f'AOI_{threshold}_layer24_m_{layer_num}'] = None\n",
        "\n",
        "   if end_index is None:\n",
        "       end_index = len(df)\n",
        "\n",
        "   for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "       if index < start_index or index >= end_index:\n",
        "           continue\n",
        "\n",
        "       serial_number = row['serial_number']\n",
        "\n",
        "       if pd.isna(serial_number):\n",
        "           print(f\"Skipping row {index} due to NaN serial_number\")\n",
        "           continue\n",
        "\n",
        "       img_name = f\"{int(serial_number)}.jpg\"\n",
        "       img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "       # Rest of the code remains the same\n",
        "       if os.path.exists(img_path):\n",
        "           img = cv2.imread(img_path)\n",
        "           torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "           for layer_num, saliency_method in zip([0, 1, 2], saliency_methods):\n",
        "               try:\n",
        "                   masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "               except ValueError as e:\n",
        "                   print(f\"Error processing image {img_name}: {str(e)}\")\n",
        "                   continue\n",
        "\n",
        "               if len(masks) > 0 and len(boxes) > 0:\n",
        "                   mask = masks[0][0]  # top1のマスクのみ使用\n",
        "                   bbox = boxes[0][0]  # top1のバウンディングボックスのみ使用\n",
        "\n",
        "                   aoi = get_aoi(bbox, [mask], threshold)\n",
        "                   #df.at[index, f'AOI_{threshold}_layer{layer_num}'] = aoi\n",
        "                   df.at[index, f'AOI_{threshold}_layer24_m_{layer_num}'] = aoi\n",
        "\n",
        "                   print(f\"Image: {img_name}, Layer: {layer_num}, AOI: {aoi}\")\n",
        "       else:\n",
        "           print(f\"Image not found: {img_name}\")\n",
        "\n",
        "   df.to_csv(csv_path, index=False)\n",
        "\n",
        "# 使用例\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv\"\n",
        "#folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial\"\n",
        "#csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "threshold = 0.5\n",
        "\n",
        "##############################################\n",
        "start_index = 0 # 開始するインデックスを指定\n",
        "end_index = 2 # 終了するインデックスを指定\n",
        "##############################################\n",
        "\n",
        "# モデルとsaliency_methodの定義\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cpu'\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "#target_layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv']\n",
        "target_layers = ['model_24_m_0', 'model_24_m_1', 'model_24_m_2']\n",
        "\n",
        "saliency_methods = [YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\") for layer in target_layers]\n",
        "\n",
        "calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)"
      ],
      "metadata": {
        "id": "OrjCYpUtJlyR",
        "outputId": "4e1691e9-5a56-411d-d9f8-de9465a3e333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov5-gradcam/models/experimental.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(attempt_download(w), map_location=device)\n",
            "INFO:models.yolo:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:utils.torch_utils:Model Summary: 213 layers, 7034398 parameters, 0 gradients\n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Model is loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 0/228 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "<ipython-input-19-898a5b3ba414>:151: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: 0, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "<ipython-input-19-898a5b3ba414>:151: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: 1, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Processing images:   0%|          | 1/228 [00:03<13:33,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 1.jpg, Layer: 2, AOI: 0.12720177383592018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "<ipython-input-19-898a5b3ba414>:151: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: 0, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-898a5b3ba414>:151: RuntimeWarning: invalid value encountered in cast\n",
            "  mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: 1, AOI: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 228/228 [00:07<00:00, 32.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 2.jpg, Layer: 2, AOI: 0.09516491022638564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LE7ZYlphCpBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show_result\n",
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# DataFrameとしてCSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# DataFrameの最初の数行を表示する\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "gVNlV1lh_rRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**注目点色塗り**"
      ],
      "metadata": {
        "id": "fmGNyvTdM3c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#注目点に色を塗る（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            color = np.array([0,0,0], dtype=np.uint8) #マスクの色：白は[0,0,0]\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + (1 - binary_mask[..., np.newaxis]) * color\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + (1 - black_mask[..., np.newaxis]) * color\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/8.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.1  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ],
      "metadata": {
        "id": "sx9KdyNZNzz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 注目点をblurする（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            # ぼかし効果を適用\n",
        "            blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + blurred_result * (1 - binary_mask[..., np.newaxis])\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + black_blurred_result * (1 - black_mask[..., np.newaxis])\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/41.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.3  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ],
      "metadata": {
        "id": "VEn_KHC-T8C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Analyze results**"
      ],
      "metadata": {
        "id": "69WoakWi-ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the CSV files\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# Create confusion matrices for Slit and Sumaho datasets\n",
        "conf_matrix_slit = pd.crosstab(df_slit['class_num'], df_slit['top1'])\n",
        "conf_matrix_sumaho = pd.crosstab(df_sumaho['class_num'], df_sumaho['top1'])\n",
        "\n",
        "# Rename the index and columns for better readability\n",
        "conf_matrix_slit.index = class_names\n",
        "conf_matrix_slit.columns = class_names\n",
        "conf_matrix_sumaho.index = class_names\n",
        "conf_matrix_sumaho.columns = class_names\n",
        "\n",
        "# Plotting confusion matrices using seaborn\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Slit Lamp Data Confusion Matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_slit, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Slit Lamp Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "\n",
        "# Smartphone Data Confusion Matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_sumaho, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Smartphone Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h03rgBUoWi_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "eb537fd2-a041-41bd-9d85-7dbd1213af03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for Slit and Sumaho datasets\n",
        "accuracy_slit = (conf_matrix_slit.values.diagonal().sum() / conf_matrix_slit.values.sum()) * 100\n",
        "accuracy_sumaho = (conf_matrix_sumaho.values.diagonal().sum() / conf_matrix_sumaho.values.sum()) * 100\n",
        "\n",
        "accuracy_slit, accuracy_sumaho\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTt2gTWAAD3",
        "outputId": "f1b38ec9-f064-4e4c-afd0-b750544e153f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.08771929824562, 77.63157894736842)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# AOI_0.5の平均と標準偏差を計算\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')['AOI_0.5'].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')['AOI_0.5'].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuQdE9zzPm68",
        "outputId": "566f1a40-1af2-46c7-add8-b6d6decf3f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ± SD AOI for Slit Lamp Data\n",
            "                   mean       std\n",
            "infection      0.086643  0.037705\n",
            "normal         0.050446  0.016585\n",
            "non-infection  0.271836  0.145000\n",
            "scar           0.205724  0.104553\n",
            "tumor          0.195092  0.097643\n",
            "deposit        0.072846  0.041480\n",
            "APAC           0.068622  0.036099\n",
            "lens opacity   0.137005  0.033883\n",
            "bullous        0.126359  0.041344\n",
            "\n",
            "Mean ± SD AOI for Smartphone Data\n",
            "               mean  std\n",
            "infection       0.0  0.0\n",
            "normal          0.0  0.0\n",
            "non-infection   0.0  0.0\n",
            "scar            0.0  0.0\n",
            "tumor           0.0  0.0\n",
            "deposit         0.0  0.0\n",
            "APAC            0.0  0.0\n",
            "lens opacity    0.0  0.0\n",
            "bullous         0.0  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIの平均と標準偏差を計算\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v9DUXYHkJ5x",
        "outputId": "22f77242-45d5-4c74-a9aa-ba5ef8278df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ± SD AOI for Slit Lamp Data\n",
            "              AOI_0.5_layer24_m_0      AOI_0.5_layer24_m_1       \\\n",
            "                             mean  std                mean  std   \n",
            "infection                     0.0  0.0                 0.0  0.0   \n",
            "normal                        0.0  0.0                 0.0  0.0   \n",
            "non-infection                 0.0  0.0                 0.0  0.0   \n",
            "scar                          0.0  0.0                 0.0  0.0   \n",
            "tumor                         0.0  0.0                 0.0  0.0   \n",
            "deposit                       0.0  0.0                 0.0  0.0   \n",
            "APAC                          0.0  0.0                 0.0  0.0   \n",
            "lens opacity                  0.0  0.0                 0.0  0.0   \n",
            "bullous                       0.0  0.0                 0.0  0.0   \n",
            "\n",
            "              AOI_0.5_layer24_m_2            \n",
            "                             mean       std  \n",
            "infection                0.047342  0.023897  \n",
            "normal                   0.026101  0.010555  \n",
            "non-infection            0.116942  0.075453  \n",
            "scar                     0.073900  0.030724  \n",
            "tumor                    0.112100  0.069113  \n",
            "deposit                  0.036668  0.025242  \n",
            "APAC                     0.063290  0.037150  \n",
            "lens opacity             0.072591  0.040474  \n",
            "bullous                  0.067484  0.022869  \n",
            "\n",
            "Mean ± SD AOI for Smartphone Data\n",
            "              AOI_0.5_layer24_m_0      AOI_0.5_layer24_m_1            \\\n",
            "                             mean  std                mean       std   \n",
            "infection                     0.0  0.0            0.000000  0.000000   \n",
            "normal                        0.0  0.0            0.001019  0.004992   \n",
            "non-infection                 0.0  0.0            0.000000  0.000000   \n",
            "scar                          0.0  0.0            0.000000  0.000000   \n",
            "tumor                         0.0  0.0            0.000000  0.000000   \n",
            "deposit                       0.0  0.0            0.005983  0.034369   \n",
            "APAC                          0.0  0.0            0.000000  0.000000   \n",
            "lens opacity                  0.0  0.0            0.000000  0.000000   \n",
            "bullous                       0.0  0.0            0.000000  0.000000   \n",
            "\n",
            "              AOI_0.5_layer24_m_2            \n",
            "                             mean       std  \n",
            "infection                0.063889  0.037739  \n",
            "normal                   0.037293  0.024043  \n",
            "non-infection            0.178614  0.139032  \n",
            "scar                     0.125316  0.081693  \n",
            "tumor                    0.165419  0.065204  \n",
            "deposit                  0.083059  0.058240  \n",
            "APAC                     0.079910  0.030570  \n",
            "lens opacity             0.111812  0.069523  \n",
            "bullous                  0.082440  0.040750  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Layerごとに解析**"
      ],
      "metadata": {
        "id": "Z2rBLnp6lbjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"24_m_2\"\n",
        "#\"24_m_0\", \"24_m_1', \"24_m_2'\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形してSeabornでプロットできるようにする\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 合算データを作成\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# グラフ1: スリットランプとスマートフォンのデータ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "#plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# グラフ2: 合算データ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='steelblue')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "#plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jkZ4ZbaoUEMX",
        "outputId": "962ac142-3e77-495f-88b6-2b83bcd79c1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKAAAAMQCAYAAAAQNB1HAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACgL0lEQVR4nOzde1zUVeL/8ffMIBc1EEsmDRw1LGyS7IJsU2tt7ZrrqmWKdlVbSu1uGbm4uGVLsaW2ttvFLpNpl29BW23ZZbuZXXCTtsyaqCwTQQ3SRTBF1OH8/ujHrAQqtw/jMK/n48HDOJ/bm+GSvD2f87EZY4wAAAAAAAAAi9iDHQAAAAAAAACdGwUUAAAAAAAALEUBBQAAAAAAAEtRQAEAAAAAAMBSFFAAAAAAAACwFAUUAAAAAAAALEUBBQAAAAAAAEtRQAEAAAAAAMBSFFAAAAAAAACwFAUUAKBD9evXTzabrdFb9+7ddcIJJyg7O1tbt24Ndsz9euyxxxplt9vtiouL09ChQ3X77bfrxx9/bHRc/b6h7osvvtB5552nhIQEORwO2Ww23XrrrS0+z3XXXRd4TV566aVmH/faa6/p4osvVv/+/dW1a1fFxsbquOOO0zXXXCOfz7ff49555512/Ry8+eabGjlypI444gjFxMQoJSVFf/zjH5v83B/Mvtn297Zo0aIWn9cYo3nz5un4449XTEzMIfc1uGPHDv3tb3/TiBEj1KdPH0VFRal79+469thjdckll+if//yn6urqgh1zv6ZMmSKbzabHHnusRcfV/wyZMmWKJbnaS2tyrl+/vtHXrsPhUI8ePTRgwACNHj1ad9xxh0pKSqwLDgA4ZEUEOwAAIDyddtppSk5OliTV1dVp06ZNKiws1F/+8hctXbpU7733ngYMGBDklPvXrVs3jR8/XpLk9/u1bt06/fvf/1ZRUZGWLl2qd999V06n05Jrr1+/Xv3795fL5dL69estuUZTduzYod/97ndav369TjnlFJ1zzjlyOBwaMmRIi85TW1urJ598MvD+o48+qtGjRx/wmOrqal100UV6+eWXJUlut1ujRo3Snj179NFHH+m+++7TAw88oD/84Q/Kzc21tGj561//qhtvvFE2m02//OUv5XQ69d577+mOO+7QP/7xD73//vs64ogjWnxep9OpESNGNLnt2GOPbfH5HnjgAd18882Ki4vTb3/7W8XGxrb4HFZ5/fXXdckll+iHH35QRESETj75ZP3yl7/U3r179e233+rJJ5/Uk08+qbS0NK1atSrYcdEK48aNU/fu3SVJ27dv1+bNm/Xmm29q2bJlysnJ0dSpUzV//vzAPm312GOP6bLLLtPkyZNbXAoCADqIAQCgA7lcLiPJLF68uNG2zZs3m2OOOcZIMuPGjev4cM2wePFiI8m4XK5G2z788EPTvXt3I8lceumlDbZJMu31v93vvvtuvxmstHz5ciPJeDyeNp3n//7v/4wk06dPH2Oz2UxERIT5/vvv97t/bW2tSU9PN5JM//79zfvvv99ge11dnVm6dKnp2rWrkWRuuOGG/WZv6+fg448/NjabzTgcDvPKK68Exnfs2GHOPvvsVn3t1mc744wz2pTt58444wwjybz++uvtet62WrZsmXE4HEaS+f3vf2/Ky8sb7VNSUmKmTZtm4uPjg5CweSZPnrzfn2UHsm3bNlNcXGw2bdpkTbB2Uv+zbvLkyc0+pv5nkyTz3XffNdq+c+dOc99995nDDjvMSDK//OUvza5du4KWFwDQsbgFDwBwyDjyyCOVlZUlSXrrrbeCnKblhg4dqpkzZ0qSnnvuOe3duzfIidrXhg0bJEkDBw5s03m8Xq8k6frrr9cZZ5yhvXv3aunSpfvdf+7cufrwww/Vo0cPLV++XKeddlqD7TabTZdeeqmeeeYZST/NUHrzzTfblHF/8vLyZIzRZZddpt/+9reB8a5du8rr9cput+sf//iHvvzyS0uu3xLt9flqT1u3btUll1wiv9+v6667Tl6vVwkJCY3269u3rxYtWqQXXnih40NaLC4uTikpKerdu3ewo3S4mJgYXXXVVXrnnXcUHR2t9957T3fddVewYwEAOggFFADgkHLkkUdK0n7Lm507d+ovf/mLTjrpJB122GHq2rWr3G63cnJyVFlZ2WDfZ599VjabTb169VJZWVmjc/3rX/+Sw+FQXFyc1q5d2y75Tz75ZEk/3a62ZcuWZh3z3//+V7Nnz5bb7VbXrl112GGH6eSTT9Zdd92lmpqaBvtOmTJF/fv3lySVlJQ0Wm+lJf71r39p1KhRSkhIUGRkpPr06aOJEyfqo48+arBf/RpFkydPliQtWbKk1ddcv3693nrrLUVERGjSpEnKzMyU9NNteE3Zvn277r33XknSnDlz5HK59nvuUaNGacyYMZKk22+/vUW5mmP37t2BWwAvuuiiRttdLlegHHv++efb/frNdeaZZ8pms+m7776TJPXv3z/wufr5el3N/Rr4+bnfeecdvffeexo9erR69eolu93erNue7r33Xm3btk0JCQnNKh6GDRvWaKwl3y/S/75+zzzzTNXW1mru3Lk65phjFB0drb59+2rWrFnatWuXJKmqqko33XSTBgwYoOjoaPXr10+33nrrQcvkTz/9VOeff7569eqlmJgYpaam6p577pHf72+07/7WVto35549e3TnnXfK7XYrJiZGhx9+uM4//3wVFxfvN0NlZaVuueUWDRkyJPCzcfDgwcrNzdXOnTubPGbv3r1auHChBg8erOjoaPXq1Uvjxo3TZ599dsCPt61OOukkXXvttZJ+Kox//vq++eabuvbaazVkyBAdccQRioqKUmJioiZOnKiioqJG5+vXr58uu+wySY1/Pp155pmB/UpKSnTnnXfqrLPOUt++fRUVFaUePXro9NNP14MPPnhIrzkGAJ1CsKdgAQDCy4FuwTPGmDlz5hhJJj09vdG2rVu3miFDhhhJJjY21owZM8aMGzfOHHHEEYHbs35+28e1115rJJnTTz/d7NmzJzBeVlZmevXqZSSZZ555ptn5D3QLnjHGPPHEE4FbUP773/8GxrWf27++/fbbwGvSq1cvM27cODNmzJjALSonnXRSg/M8/PDDZty4cUaS6datm5k8eXKDt+bKyckxkozNZjOnnXaaufDCCwOvrcPhMF6vN7BvcXGxmTx5sjnttNOMJHP00Ue36prG/O/zO2bMGGPMT7fkxMXFGUnmgw8+aLT/Cy+8EHjtKioqDnr+Z5991kgydrvdbNu2LTDeHrfgffbZZ4FzVFdXN7nPDTfcYCSZjIyMZp+3PltKSoqZO3eumTp1qrnuuuvM/fffb0pKSlqcMy8vz0yePNl069YtcEtg/efq+eefD+zXkq+BevW39V111VXGbreb4447zlxwwQVm+PDh5qmnnjpothNPPNFIMtdee22LPy5jWv79Ysz/Xt9TTz3VnHHGGYGfHaNGjQp87Y0aNcps3brVHHvssYHzDh8+3ERHRxtJZvr06Y2y1N+Cd+WVV5ro6GjTr18/M3HiRDN8+HATGRlpJJnx48eburq6Bsft71axfW9x/fWvf226du1qRowYYcaNG2eSkpKMJNOjR48mb23z+XyBfXr37m1GjBhhRo8ebZxOp5FkhgwZ0uD7wRhj/H6/Oe+884wkExkZaYYPH24mTpxo+vXrZ6Kjo81VV13V7rfg7evTTz8N7Lty5coG244++mgTGRlpTjzxRDNmzBhz/vnnm+OOO85IMhEREebZZ59tsP/MmTP3+/MpLy8vsN+f//znwP8rzj77bHPBBReYM844I/D5Ov/88xt9vgAA7YcCCgDQoZoqoPx+vykrKzN///vfTVRUlHE4HOall15qdOzEiRMD5dSWLVsC49u3bze//e1vm1yfqLa21gwdOtRIMrNmzTLGGLNnzx5z+umnG0nm6quvblH+gxVQ48ePN5JM3759G4zvr/yoX9tozJgx5scffwyMV1RUmJNOOslIMhdddFGDY9q6BtSrr75qJJno6OhG6wM98sgjRpLp0qWL+fzzzxtsa+saK36/P/BL8gsvvBAYnzZtWmA9oJ+rL6z69+/frGuUlJQEXuu33347MN4eBdSLL74YKAH25+677zaSzCmnnNLs8+6b7edvERER5oYbbmhQnjZX/fdaU0VAa78G6gsoSea+++5rUZ49e/YYu91uJJmlS5e2+OMxpnXfL/u+vkOHDm3ws2P9+vUmPj7eSDKDBw82o0ePNjt27AhsLyoqMhEREcZutzcqA+sLqPpCbt/P0eeffx4ouBctWtTguIMVUJLMiSeeaDZv3hzYVlNTY8455xwjyUydOrXBcTt37jRHH320kWRycnJMbW1tYNuOHTvMhRdeaCSZyy67rMFx9957r5FknE6n+eKLLwLje/bsMVdeeWUgi1UFlN/vDxQ/jzzySINtzz//fKMisX48IiLCHH744Wbnzp0NtjXn59OqVavMZ5991mh848aN5oQTTjCSTH5+/gFzAwBajwIKANCh6n8p3t9bWlpao0WmjfmpWLDb7cZms5lPP/200faysrLAbIWfz6T57rvvTHx8vLHZbObll182N998s5FkTj755BYvgNtUAbV3716zdu1ac/311wc+jrvvvrvBcU2VH++9956RZLp27drkItwfffRRYDZPaWlpg4+nLQVU/WLZN954Y5PbR40aZSSZK664osF4Wwuo+tLD6XQ2+GV91apVRpLp3r272b59e4Njpk+fbiSZX/ziF826xq5duwKv9b4z29qjgHryySeNJHPUUUftd5+HHnrISDLHHHNMs8/78ccfmxkzZpgVK1aYzZs3mx07dpg1a9aYG264wXTp0qXJz0VzHKiAau3XQH0BddZZZ7U4z/fffx/4HLz22mstPr613y/1n3ubzdZk+XDdddcFvv6aWhB99OjRRpJZsmRJg/H6Aqp3796mpqam0XF///vfjSQzcODABuMHK6BsNptZvXp1o/P9+9//NpLMgAEDGow/8MADgVlcTdm+fbtJSEgwERERDUqd5ORkI8k88MADjY6pqakxRx55pKUFlDEmcI0777yz2deoL9RefvnlBuNt/fn0r3/9y0gtm70IAGiZCAEAEASnnXaakpOTA+9v2bJFa9asUVFRkW644QY9+eSTDRZPfvfdd1VXV6eTTjpJqampjc531FFH6ZxzztE///lPLV++XB6PJ7CtX79+euyxx3Teeefpwgsv1Pbt2xUXF6f8/HxFRUW1Kn/9+ks/Z7fbNWPGDM2YMeOg53jnnXckSSNGjJDT6Wy0/eSTT9YJJ5ygTz/9VCtWrNDFF1/cqqz72rt3rz744ANJarQGTb3MzEwtW7ZMy5cvb/P19vXII49IkiZNmqSIiP/9FSQtLU3HH3+8Pv/8cz3zzDOBdaFawxjT5pwd7cQTT9SJJ57YYGzw4MG6++67dfrpp2vcuHF6+OGHddVVV2nIkCFtvl57fA2MHz++zTlaqq3fL3379tXxxx/f6Lj6nzMnn3xykwui12/ftGlTk7kmTJig6OjoRuOTJ0/Wtddeq7Vr12rTpk3q06fPgT/AfXKecMIJjcYHDRokSdq4cWOD8fp1ySZOnNjk+bp3765TTjlFr7zyioqKijR8+HBt3LhR33zzjSTpkksuaXRMdHS0JkyYoL/97W/Nytxa9WsuNfWzdNOmTXr55Zf15ZdfqqqqKrBOlM/nkyR99dVXGjlyZIuvWVtbq9dff11FRUWqqKhQbW2tjDHavn174LwAAGtQQAEAguLyyy9v9Mvv3r179ac//Ul5eXk644wz9NVXX+mwww6T9L9fuuoX4G7K0Ucf3WDffY0ZM0aXX365Hn74YUnSQw89pAEDBrQ6f7du3QK/hNtsNnXv3l3HHHOMRo0adcCM+2rux/Tpp582+TG1xtatWwMLLu/vugd6HVvrhx9+0IsvvihJ+v3vf99o++9//3vdeOONevTRRxsUUEcccYQkqby8vFnXqaioCPx3r1692hK5kfqvxR07dux3nx9//FGSFBsb2y7XPP/88zVkyBCtXr1aL730UrsUUO3xNdCvX78WX/fwww+X3W5XXV1dg89Tc7X1+6Vv375NHtO9e/cDbq//vNe/Zj+3vzyHHXaYDj/8cG3dulVlZWUtKqCaUv81VVtb22B83bp1kqRLL71Ul1566QHP/cMPP0hS4KEMRxxxRODj/7nm/hxrLb/fr23btkmSevbs2WDb3Llzdfvtt2vPnj37Pb66urrF1/z3v/+tiRMnBp4Q2V7nBQA0DwUUAOCQERERodzcXD388MPavHmzli5dqquvvrpdzr1161a9+uqrgff//e9/a8KECa0+3xFHHNGsp37hJ48//rj27NmjiIgIXX755Y221xc3hYWF+vLLL5WSkiLpf08V/O677/TDDz8ctFRatWqVpJ9mov18VlFb1Zcu27Zt0/bt2wPFxL5KS0sb7NseBg0apNWrVzf5JMdgiYmJafExERERSk1N1erVq1VUVHTQsqS92e0Hfvjzwba3RUtm5rU0R/0sov3NDNvXgZ4i2dE+//xz7d69W9JPM/7qPffcc7r11lvVvXt33XvvvTrrrLPUp08fxcTEyGazafbs2crLy2vxbMedO3fqvPPOU3l5uS677DJdeeWVSk5OVmxsrBwOh77++msde+yxITmLEgBCBQUUAOCQYrfb1a9fP23ZsqXBI8ePOuooSf/71/6m1G+r37eeMUaXXnqpysrKdN555+ndd9/VX//6V5155pkaM2aMBR9F87TlY2qtww8/XFFRUaqtrdW6deuavJ2xva8pSV6vV1LD278OtO+8efMkSWeddZYOO+wwbd++XUuXLtXMmTMPeOzSpUslSb/85S/Vo0ePtgffx7HHHquuXbtq586d+uijj/SrX/2q0T4fffSRpJ8eM99etm7dKklNFl6tEayvAUk699xztXr1aj3zzDOaN29ei26BDcb3S3N89913TY5v37498LlLTEy07PpJSUn68ssvlZmZ2exbI+tfny1btujHH39schbU+vXr2zNmI0888YSkn74e64tmScrPz5ck3X777Zo6dWqj49auXduq67377rsqLy/XSSedpEcffbTdzgsAaD7r/qkHAIBWqKurC/zis+8vRcOGDZPdbtfq1av16aefNjpu8+bNeu211ySpUTHwl7/8Ra+++qoGDRqkJ554QkuWLJHNZtOUKVNUUlJi3QdzEGeeeaYk6bXXXmvyFrNPPvlEq1evlt1u17BhwwLjkZGRkhRYE6UlIiIidPrpp0vSfmdw1f9y1lTB0horV67UF198oaioKFVWVsr89BCURm+vvPKKpJ9mS9V/bLGxsYFZcLm5uQf8fC1btkwvvfSSJGn27Nntkn1fkZGR+t3vfidJeuqppxptLykpUWFhoSRp7Nix7XLNjRs36r333pMkDR06tF3OGYyvgXrXXnut4uLiVFFRoVmzZh10//qPXWr994vVCgoKGt0WJ/30dSxJycnJlhZiv/3tbyX9r7hpjsTExMAtyE19LdfW1qqgoKB9Ajbh448/1r333itJuvHGG+VwOALb/vvf/0pqerZWRUWF3njjjSbPebCfi/Xn3d8tjvWFGADAOhRQAIBDxt69e5WTk6MtW7ZIUoPZSX379lVGRoaMMZo2bVpgZoH005o8U6dO1a5du+TxeBosQP7uu+9qzpw56tq1qwoKCtStWzeNGjVKM2fOVGVlpSZMmHDAdUasdPrppys9PV01NTWaNm2adu7cGdi2ZcsWTZs2TZJ0wQUXKCkpKbCtV69eioyM1Pfffx/4paol6mcRPfDAA3rrrbcabHvsscf04osvqkuXLrr++utb82E1Uj/76dxzzz3grKThw4fryCOPVHl5uZYtWxYYv/XWW3XKKado27Zt+tWvfhUoeeoZY/TEE08EFmG+9tprNXz48HbJ/nN/+MMfZLPZtHjx4kDhKf10e09mZqb8fr/GjRsXuIWw3qpVq5SSktJoXJLuueeewNf8vtasWaPRo0erpqZGRx99tM4999x2+zg6+mug3uGHH66lS5fKbrfrnnvu0eWXX97kelAbN27UNddco/POOy8w1trvF6tt2rRJN910k/x+f2CsuLhYt912myTphhtusPT6U6dOlcvlUkFBgWbNmhVYTHtf33//fWD9u3r1D0q49dZb9eWXXwbG/X6/brrppv0uut4WNTU1euCBB3TmmWdq165dOvPMM3XTTTc12Kd+sfWHHnoocIueJFVVVWny5Mmqqqpq8tz1s8y++OKLJrfXn/ett95qtM9DDz2kZ555pnUfFACg+YLx6D0AQPiqfzT8aaedZiZPnhx4GzVqlElKSgo8wvuPf/xjo2O3bNliTjjhBCPJxMXFmfPOO8+MHz/e9OrVy0gy/fv3b/Do74qKCtOnTx8jySxevLjBuXbv3m1+8YtfGElmxowZzc5f/6hvl8vVoo+7/uP6uW+//TbwmiQkJJjx48ebc88918TGxhpJ5qSTTmrw6PR648ePN5JMUlKSufDCC01mZqbJzMxsdp6cnJzAI99PP/10c9FFF5mTTjrJSDIOh8N4vd79fuwtecz59u3bTffu3Zt8bHpTbrzxxiYfKb9t2zYzYsSIwOs4ePBgM2HCBDN27FiTmJhoJBm73W5uvvlmU1dX1+i89Y+4b4+/+tx9992B1+7MM880EyZMML179zaSzLHHHmt++OGHFl0/Li7OOBwOc/LJJ5vx48ebCRMmmJNPPtnY7XYjyfTt29d88cUXLc5Z/3W17/fEvlrzNXDGGWcYSWb58uUtzrOvV155xRxxxBFGkomIiDC/+MUvzMSJE824cePMkCFDjM1mM5LML37xiwbHteb7pf61P+OMM5rMcrCv61tuucVIMrfcckuD8cmTJxtJZvr06SY6Otr079/fXHDBBeacc84xkZGRRpIZO3Zso6/H/V3vYDmN2f/Pkc8//9z069fPSDI9evQww4YNMxdddJE577zzzHHHHWdsNptxOp0NjvH7/Wb06NFGkomMjDTnnHOOueCCC0z//v1NdHS0ufLKK1v8/f7dd98FMo4bNy7w8338+PHG4/GY6OjowPfq9OnTzY8//tjoHOvWrTM9evQwksxRRx1lxo0bZ8aMGWPi4uJM7969ze9///smPx+1tbWBn/cnnniimTRpksnMzDR33XVXYJ9zzz038PEOHz7cXHDBBSYlJcXYbDbzxz/+sVU/2wEAzUcBBQDoUPW/PP78LTIy0rhcLjNx4sQD/nK7Y8cOk5eXZ4YMGWK6du1qoqOjzaBBg8zs2bMb/OLp9/vN8OHDD/gLVElJienZs6eRZJ5//vlm5W/vAsoYY7Zu3Wqys7PNoEGDTHR0tOnatas58cQTzV/+8hezc+fO/R4zbdo007dvX9OlS5dWlSuvvvqqGTlypDn88MNNRESEOfLII01GRob58MMPm9y/NQWU1+s1ksyRRx5p9u7de9D9V69eHShANm7c2Gj7yy+/bC644ALTt29fEx0dbbp3726OPfZYc+WVV5o1a9bs97ztWUAZY8wbb7xhRowYYXr27GmioqLMwIEDTXZ2tqmurm7x9e+66y5z7rnnmuTkZBMXF2ciIiJMz549zemnn27mzZu333MezMEKKGNa/jXQXgWUMT+Vk3/961/Nb37zG3PkkUeayMhI07VrV3PMMceYSy65xCxbtqzJMrGl3y9WF1CLFy82H3/8sRk9erQ5/PDDTVRUlHG73ebuu+82e/bsafb12lJAGWNMdXW1ueuuu8ypp55qevToYbp06WJ69+5t0tLSTFZWliksLGx0zJ49e8yCBQvMcccdZ6Kioszhhx9uzj33XLN69epWfb/vW0DVv9ntdhMbG2v69etnRo0aZW6//XZTUlJy0PNcfPHFpm/fviYqKsq4XC4zffp08/333+/382GMMZ999pkZM2aM6dWrV6DA3ff13L17t5k3b54ZPHiw6dq1q+nZs6cZPny4ef311wPZKaAAwDo2Y3jUAwAAAAAAAKzDGlAAAAAAAACwFAUUAAAAAAAALBUR7AAAAAAdbcuWLY2evnUgl19+uU4//XQLEwEAAHRuFFAAACDs/Pjjj1qyZEmz9z/zzDMpoAAAANqARcgBAAAAAABgKdaAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUigh0glNXV1WnTpk067LDDZLPZgh0HAAAAAACgwxhjtH37dvXp00d2+4HnOFFAtcGmTZuUlJQU7BgAAAAAAABBU1paqsTExAPuQwHVBocddpikn17o2NjYIKcBAAAAAADoONXV1UpKSgr0IwdCAdUG9bfdxcbGUkABAAAAAICw1JxliViEHAAAAAAAAJaigAIAAAAAAIClKKAAAAAAAABgKQooAAAAAAAAWIoCCgAAAAAAAJaigAIAAAAAAIClQraAqq2t1axZs9SnTx/FxMQoPT1db7zxxkGPu/XWW2Wz2Rq9RUdHd0BqAAAAAACA8BMR7ACtNWXKFD377LOaMWOGBg4cqMcee0wjR47U8uXLdfrppx/0+AceeEDdu3cPvO9wOKyMCwAAAAAAELZCsoBatWqVnn76ac2bN0833XSTJGnSpEk6/vjjdfPNN6uwsPCg5xg/fryOOOIIq6MCAAAAAACEvZC8Be/ZZ5+Vw+HQ1KlTA2PR0dHKzMzUypUrVVpaetBzGGNUXV0tY4yVUQEAAAAAAMJeSBZQn3zyiY455hjFxsY2GB86dKgkafXq1Qc9x4ABAxQXF6fDDjtMl1xyicrLy62ICgAAAAAAEPZC8ha8zZs3q3fv3o3G68c2bdq032Pj4+N1zTXX6NRTT1VUVJTee+893XfffVq1apU++uijRqXWvmpra1VbWxt4v7q6ug0fBQAAAAAAQHgIyQKqpqZGUVFRjcbrn2RXU1Oz32Ovv/76Bu+PGzdOQ4cO1cUXX6z7779ff/jDH/Z7bF5enubOndvK1AAAAAAAAOEpJG/Bi4mJaTATqd6uXbsC21vioosu0pFHHqk333zzgPtlZ2erqqoq8NactaYAAAAAAADCXUjOgOrdu7c2btzYaHzz5s2SpD59+rT4nElJSfrvf/97wH2ioqKanHkFAAAAAACA/QvJGVBDhgzR119/3WgNpg8//DCwvSWMMVq/fr169erVXhEBAAAAAADw/4VkATV+/Hj5/X499NBDgbHa2lotXrxY6enpSkpKkiRt2LBBX375ZYNjf/jhh0bne+CBB/TDDz9oxIgR1gYHAAAAAAAIQyF5C156eroyMjKUnZ2tiooKJScna8mSJVq/fr28Xm9gv0mTJmnFihUyxgTGXC6XJk6cqMGDBys6Olrvv/++nn76aQ0ZMkTTpk0LxocDAAAAAADQqYVkASVJS5cu1Zw5c/T444+rsrJSqampWrZsmYYNG3bA4y6++GIVFhbqH//4h3bt2iWXy6Wbb75Zf/zjH9W1a9cOSg8AAAAAABA+bGbf6UFokerqasXFxamqqkqxsbHBjgMAAAAAANBhWtKLhOQaUAAAAAAAAAgdFFAAAAAAAACwVMiuAQUAaB2/3y+fz6fKykrFx8fL7XbL4XAEOxYAAACATowCCgDCSGFhobxer8rLywNjTqdTmZmZ8ng8QUwGAAAAoDPjFjwACBOFhYXKy8uTy+XS/PnzVVBQoPnz58vlcikvL0+FhYXBjggAAACgk+IpeG3AU/AAhAq/36+pU6fK5XIpJydHdvv//v2hrq5Oubm5Kikp0UMPPcTteAAAAACahafgAQAa8Pl8Ki8v14QJExqUT5Jkt9uVkZGh8vJy+Xy+ICUEAAAA0JlRQAFAGKisrJQkuVyuJrfXj9fvBwAAAADtiQIKAMJAfHy8JKmkpKTJ7fXj9fsBAAAAQHuigAKAMOB2u+V0OpWfn6+6uroG2+rq6lRQUCCn0ym32x2khAAAAAA6MwooAAgDDodDmZmZKioqUm5uroqLi7Vz504VFxcrNzdXRUVFyszMZAFyAAAAAJbgKXhtwFPwAISawsJCeb1elZeXB8acTqcyMzPl8XiCmAwAAABAqGlJLxLRQZkAAIcAj8ej9PR0+Xw+VVZWKj4+Xm63m5lPAAAAACxFAQUAYcbhcCg1NTXYMQAAAACEEdaAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYKmQLqNraWs2aNUt9+vRRTEyM0tPT9cYbb7T4PL/5zW9ks9l0zTXXWJASAAAAAAAAIVtATZkyRXfffbcuvvhi3XPPPXI4HBo5cqTef//9Zp/jueee08qVKy1MCQAAAAAAgJAsoFatWqWnn35aeXl5mjdvnqZOnaq3335bLpdLN998c7POsWvXLs2cOVOzZs2yOC0AAAAAAEB4C8kC6tlnn5XD4dDUqVMDY9HR0crMzNTKlStVWlp60HPcddddqqur00033WRlVAAAAAAAgLAXkgXUJ598omOOOUaxsbENxocOHSpJWr169QGP37Bhg/7yl7/ozjvvVExMjFUxAQAAAAAAICki2AFaY/Pmzerdu3ej8fqxTZs2HfD4mTNn6sQTT9QFF1zQouvW1taqtrY28H51dXWLjgcAAAAAAAhHIVlA1dTUKCoqqtF4dHR0YPv+LF++XP/4xz/04Ycftvi6eXl5mjt3bouPAwAAAAAACGcheQteTExMg5lI9Xbt2hXY3pS9e/fquuuu06WXXqq0tLQWXzc7O1tVVVWBt+asNQUAAAAAABDuQnIGVO/evbVx48ZG45s3b5Yk9enTp8njli5dqq+++koPPvig1q9f32Db9u3btX79eiUkJKhr165NHh8VFdXkzCsAAAAAAADsX0jOgBoyZIi+/vrrRmsw1d9WN2TIkCaP27Bhg/bs2aPTTjtN/fv3D7xJP5VT/fv31+uvv25pdgAAAAAAgHBjM8aYYIdoqQ8//FC/+MUvNG/ePN10002Sflog/Pjjj9fhhx+uf//735J+Kpx27typlJQUSdKXX36pL7/8stH5xo4dq5EjR+qKK65Qenp6kwucN6W6ulpxcXGqqqpq9EQ+AAAAAACAzqwlvUhI3oKXnp6ujIwMZWdnq6KiQsnJyVqyZInWr18vr9cb2G/SpElasWKF6ju2lJSUQBn1c/3799d5553XEfEBAAAAAADCSkgWUNJPt8zNmTNHjz/+uCorK5Wamqply5Zp2LBhwY4GAAAAAACAfYTkLXiHCm7BAwAAAAAA4aolvUhILkIOAAAAAACA0EEBBQAAAAAAAEtRQAEAAAAAAMBSFFAAAAAAAACwFAUUAAAAAAAALEUBBQAAAAAAAEtRQAEAAAAAAMBSFFAAAAAAAACwFAUUAAAAAAAALEUBBQAAAAAAAEtRQAEAAAAAAMBSFFAAAAAAAACwFAUUAAAAAAAALEUBBQAAAAAAAEtRQAEAAAAAAMBSFFAAAAAAAACwVESwAwAAgPbl9/vl8/lUWVmp+Ph4ud1uORyOYMcCAABAGKOAAgCgEyksLJTX61V5eXlgzOl0KjMzUx6PJ4jJAAAAEM64BQ8AgE6isLBQeXl5crlcmj9/vgoKCjR//ny5XC7l5eWpsLAw2BEBAAAQpmzGGBPsEKGqurpacXFxqqqqUmxsbLDjAADCmN/v19SpU+VyuZSTkyO7/X//xlRXV6fc3FyVlJTooYce4nY8AAAAtIuW9CLMgAIAoBPw+XwqLy/XhAkTGpRPkmS325WRkaHy8nL5fL4gJQQAAEA4o4ACAKATqKyslCS5XK4mt9eP1+8HAAAAdCQKKAAAOoH4+HhJUklJSZPb68fr9wMAAAA6EgUUAACdgNvtltPpVH5+vurq6hpsq6urU0FBgZxOp9xud5ASAgAAIJxFBDsAAABoO4fDoczMTOXl5enPf/6zTj75ZEVGRmr37t36z3/+o48++kjZ2dksQA4AAICgoIACAKCT8Hg8Gjt2rF544QUVFRUFxu12u8aOHSuPxxPEdAAAAAhnFFAAcAjatWuXysrKgh2jzRITExUdHR3sGGGjsLBQzz//vE455RSdfPLJioqKUm1trf7zn//o+eefV0pKCiUUAAAAgoICCgAOQWVlZZoxY0awY7TZwoULlZycHOwYYcHv98vr9SotLU05OTmy2/+3zOPIkSOVm5srr9er9PR0bsMDAABAh6OAAoBDUGJiohYuXGjZ+UtLS7VgwQLNnDlTSUlJll0nMTHRsnOjIZ/Pp/LycmVlZTUon6SfbsHLyMhQVlaWfD6fUlNTg5QSAAAA4YoCCgAOQdHR0R0ycygpKYkZSp1EZWWlJMnlcjW5vX68fj8AAACgI9kPvgsAADjUxcfHS5JKSkqa3F4/Xr8fAAAA0JEooAAA6ATcbrecTqfy8/NVV1fXYFtdXZ0KCgrkdDrldruDlBAAAADhjAIKAIBOwOFwKDMzU0VFRcrNzVVxcbF27typ4uJi5ebmqqioSJmZmSxADgAAgKBgDSgAADoJj8ej7Oxseb1eZWVlBcadTqeys7Pl8XiCmA4AAADhjAIKAIBOxOPxKD09XT6fT5WVlYqPj5fb7WbmEwAAAIKKAgoAgE7G4XAoNTU12DEAAACAANaAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGCpFhdQ33//vZ577jm9+OKLqqys3O9+K1as0G233damcAAAAAAAAAh9LSqgFixYoH79+ikjI0Njx45VYmKi7rjjDhljGu37zjvvaO7cue0WFAAAAAAAAKGp2QXU66+/rqysLMXExOiKK67QVVddpdjYWM2ZM0ejR49WbW2tlTkBAAAAAAAQoppdQC1cuFBdu3bVqlWrtGjRIv3973/X119/rYkTJ+qVV17R6NGjtWvXLiuzAgAAAAAAIAQ1u4AqKirSuHHjNHDgwMDYYYcdpqeeekqzZs3Sm2++qVGjRlFCAQAAAAAAoIFmF1Dbt29XYmJik9vy8vL0xz/+UW+//bZ+97vfqaampt0CAgAAAAAAILRFNHfHo446Shs3btzv9j//+c+SpNtvv10jR47UKaec0vZ0AAAAAAAACHnNLqCOP/54vfXWWwfcZ98SqrCwsG3JAAAAAAAA0Ck0+xa83/3ud9q4caNefvnlA+735z//WTk5OdqzZ0+bwwEAAAAAACD0NXsG1Pjx42WMUbdu3Q6672233aYBAwaopKSkTeEAAAAAAAAQ+ppdQPXs2VPTpk1r9omnTJnSaGzDhg1av369hg0b1uzzAAAAAAAAILQ1+xa89rB48WL96le/6shLAgAAAAAQVH6/X2vWrNGKFSu0Zs0a+f3+YEcCOlyzZ0ABAAAAAICWKSwslNfrVXl5eWDM6XQqMzNTHo8niMmAjtWhM6AAAAAAAAgXhYWFysvLk8vl0vz581VQUKD58+fL5XIpLy+Pp8cjrFBAAQAAAADQzvx+v7xer9LS0pSTk6OUlBTFxMQoJSVFOTk5SktLk9fr5XY8hA0KKAAAAAAA2pnP51N5ebkmTJggu73hr952u10ZGRkqLy+Xz+cLUkKgY1FAAQAAAADQziorKyVJLperye314/X7AZ0dBRQAAAAAAO0sPj5eklRSUtLk9vrx+v2Azo4CCgAAAACAduZ2u+V0OpWfn6+6uroG2+rq6lRQUCCn0ym32x2khEDHooACAAAAAKCdORwOZWZmqqioSLm5uSouLtbOnTtVXFys3NxcFRUVKTMzUw6HI9hRgQ4R0ZEXi4uLU9++fTvykgAAAAAABIXH41F2dra8Xq+ysrIC406nU9nZ2fJ4PEFMB3SsDi2gZsyYoRkzZnTkJQEAAAAACBqPx6P09HT5fD5VVlYqPj5ebrebmU8IO60uoIwx+uc//6lPP/1UmzZt0p49exrtY7PZ5PV62xQQAAAAAIBQ5nA4lJqaGuwYQFC1qoD65ptvNGrUKK1du1bGmP3uRwEFAAAAAACAVhVQV199tb7++mtdeeWVuvDCC9W7d29FRHTo3XwAAAAAAAAIEa1qjd577z2NGTNG9913X3vnAQAAAAAAQCdjb81Bhx12mJKTk9s7CwAAAAAAADqhVhVQv/nNb1RYWNjeWVqktrZWs2bNUp8+fRQTE6P09HS98cYbBz3u+eef1znnnKM+ffooKipKiYmJGj9+vD7//PMOSA0AAAAAABB+WlVAzZs3T5s2bVJWVpZ27drV3pmaZcqUKbr77rt18cUX65577pHD4dDIkSP1/vvvH/C4zz77TPHx8br++ut1//3368orr9Qnn3yioUOH6tNPP+2g9AAAAAAAAOHDZg70GLsD+Oqrr3TqqafK7/dr4MCBio2NbXxym01vvfVWm0P+3KpVq5Senq558+bppptukiTt2rVLxx9/vBISElo8O6u8vFyJiYnKzMzUokWLmn1cdXW14uLiVFVV1eTHDwCHqm+++UYzZszQwoULuaUaAAAAQKu0pBdp1SLkn3zyiX7zm99o27ZtkqSPP/64yf1sNltrTn9Qzz77rBwOh6ZOnRoYi46OVmZmpmbPnq3S0lIlJSU1+3wJCQnq2rVr4OMBAAAAAABA+2nVLXgzZszQtm3bdOedd2rDhg3as2eP6urqGr35/f72zivppwLsmGOOadSuDR06VJK0evXqg55j27Zt+uGHH/TZZ5/p8ssvV3V1tc4++2wr4gIAAAAAAIS1Vs2A+s9//qOJEycqKyurvfM0y+bNm9W7d+9G4/VjmzZtOug5fvGLX+irr76SJHXv3l05OTnKzMw84DG1tbWqra0NvF9dXd2S2AAAAAAAAGGpVQVUbGysnE5ne2dptpqaGkVFRTUaj46ODmw/mMWLF6u6ulrr1q3T4sWLVVNTI7/fL7t9/5PC8vLyNHfu3NYHBwAAAAAACEOtKqDOPfdcvf3226qrqztgYWOVmJiYBjOR6tU/kS8mJuag5zj11FMD/33BBRdo0KBBkqT58+fv95js7GzdeOONgferq6tbtNYUAAAAAABAOGpVe3TnnXcqKipKF198sTZu3NjemQ6qd+/e2rx5c6Px+rE+ffq06Hzx8fE666yz9OSTTx5wv6ioKMXGxjZ4AwAAAAAAwIG1agbUkCFDtHv3bn300UfKz89XfHx8k2WMzWbTt99+2+aQTV1/+fLlqq6ubnDdDz/8MLC9pWpqalRVVdVeEQEAAAAAAPD/tWoGVF1dnbp06aK+ffuqb9++Ouyww2SMafRWV1fX3nklSePHj5ff79dDDz0UGKutrdXixYuVnp4euC1uw4YN+vLLLxscW1FR0eh869ev11tvvaVTTjnFkrwAAAAAAADhrFUzoNavX9/OMVomPT1dGRkZys7OVkVFhZKTk7VkyRKtX79eXq83sN+kSZO0YsUKGWMCY4MHD9bZZ5+tIUOGKD4+XmvXrpXX69WePXv0l7/8JRgfDgAAAAAAQKfWqgKqtVasWKEVK1boT3/6U5vPtXTpUs2ZM0ePP/64KisrlZqaqmXLlmnYsGEHPO7KK6/Uyy+/rNdee03bt29XQkKChg8frtmzZ2vw4MFtzgUAAAAAAICGbGbf6UEWmzt3rm677Tb5/f6OuqSlqqurFRcXp6qqKhYkBxBSvvnmG82YMUMLFy5UcnJysOMAAAAACEEt6UU6dAYUAABAONm1a5fKysqCHaPNEhMTFR0dHewYAAAghFFAAQAAWKSsrEwzZswIdow2Y7YkAABoKwooAAAAiyQmJmrhwoWWnb+0tFQLFizQzJkzA08BtkJiYqJl5wYAAOGBAgoAAMAi0dHRHTJzKCkpiRlKAADgkGYPdgAAAAAAAAB0bhRQAAAAAAAAsBQFFAAAAAAAACxFAQUAAAAAAABLdWgBNWTIEE2aNKkjLwkAAAAAAIAga9cC6m9/+5vefvvt/W4/99xztXjx4va8JAAAAAAAAA5x7VpAzZgxQ08//XR7nhIAAAAAAAAhLqK5O+bn5zdrv3Xr1jXYd8KECS1PBQAAAAAAgE6j2QXUBRdcIJvNdsB9bDabli9fruXLl8sYI5vNRgEFAAAAAAAQ5ppdQElSt27dNH36dHXr1q3RNmOMbrvtNp100kkaPXp0uwUEAAAAAABAaGt2AfV///d/uuaaa/SPf/xDXq9XZ555ZqN96guoW265pT0zAgAAAAAAIIQ1exHyiRMn6vPPP5fb7dbZZ5+tq666Sjt27LAyGwAAAAAAADqBFj0Fz+l06sUXX5TX69X//d//ye12680337QqGwAAAAAAADqBFhVQ9aZMmaI1a9bomGOO0TnnnKOpU6dq+/bt7Z0NAAAAAAAAnUCrCihJSkpK0uuvv6577703MBvqYE/JAwAAAAAAQPhpdQFV78orr9Snn36qo48+WrGxseratWt75AIAAAAAAEAn0eyn4B3IgAEDtHz58vY4FQAAAAAAADqZNs+AAgAAAAAAAA7EsgKqurpaGzZssOr0AAAAAAAACBEtKqC+/vprjR49WrGxserZs6cuvPBCrV27tsl9//rXv6p///7tEhIAADSf3+/XmjVrtGLFCq1Zs0Z+vz/YkQAAABDmmr0G1KZNm/TLX/5SP/zwg2JiYmS32/XMM8/opZde0sMPP6wLL7zQypwAAKAZCgsL5fV6VV5eHhhzOp3KzMyUx+MJYjIAAACEs2bPgLr99tv1ww8/6M4779T27dtVWVmpp59+Wt27d9ell16qRx55xMqcAADgIAoLC5WXlyeXy6X58+eroKBA8+fPl8vlUl5engoLC4MdEQAAAGGq2QXUa6+9pmHDhikrK0t2u102m00TJkxQUVGRBg0apOnTp+vBBx+0MisAANgPv98vr9ertLQ05eTkKCUlRTExMUpJSVFOTo7S0tLk9Xq5HQ8AAABB0ewCauPGjUpPT280npSUpBUrVuj444/XVVddpQceeKBdAwIAgIPz+XwqLy/XhAkTZLc3/N+73W5XRkaGysvL5fP5gpQQAAAA4azZa0DFxcWptra2yW09e/bU22+/rbPOOkvXXHON6urq2i0gAAA4uMrKSkmSy+Vqcnv9eP1+AAAAQEdq9gyoAQMG6MMPP9zv9voSavDgwbruuuuUn5/fLgEBAMDBxcfHS5JKSkqa3F4/Xr8fAAAA0JGaXUD9+te/1qpVq7Ru3br97lNfQqWmpqq4uLhdAgIAgINzu91yOp3Kz89vNBO5rq5OBQUFcjqdcrvdQUoIAACAcNbsAmr8+PEaOnSoXn311QPuV19CDRs2TH379m1zQAAAcHAOh0OZmZkqKipSbm6uiouLtXPnThUXFys3N1dFRUXKzMyUw+EIdlQAAACEoWavAXXCCSdo5cqVzdo3Pj5e77zzTmszAQCAVvB4PMrOzpbX61VWVlZg3Ol0Kjs7Wx6PJ4jpAAAAEM6aXUDt6/e//70GDx6sG264ob3zAACANvB4PEpPT5fP51NlZaXi4+PldruZ+QQAAICgalUB9dRTT1E+AQBwiHI4HEpNTQ12DAAAACCg2WtA7evoo4/W5s2b2zsLAAAAAAAAOqFWFVC///3v9fLLL2vjxo3tnQcAAAAAAACdTKtuwRs3bpyWL18uj8ejm2++WWlpaXI6nbLZbI325Ul4AAAAAAAA4a1VBdSAAQNks9lkjNF111233/1sNpv27t3b6nAAAAAAAAAIfa0qoCZNmtTkbCcAAAAAAADg51pVQD322GPtHAMAAAAAAACdVasWIQcAAAAAAACaq1UzoOp9//33eu655/Tll19qx44d8nq9kqQffvhB3333nQYPHqyYmJh2CQoAAAAAAIDQ1OoC6v7779fMmTNVW1sr6acFx+sLqIqKCp166qlatGiRrrjiivZJCgAAAAAAgJDUqlvwXnrpJV1zzTUaPHiwXnzxRV155ZUNtrvdbqWmpuqFF15oj4wAAAAAAAAIYa2aATVv3jz17dtXy5cvV7du3fSf//yn0T6DBw/We++91+aAAAAAAAAACG2tmgG1evVq/e53v1O3bt32u89RRx2l8vLyVgcDAAAAAABA59CqAqqurk5dunQ54D4VFRWKiopqVSgAAAAAAAB0Hq0qoI499tgD3l63d+9evfvuuxo8eHCrgwEAAAAAAKBzaFUBdfHFF+uTTz7R3LlzG23z+/266aabtG7dOk2aNKnNAQEAAAAAABDaWrUI+bXXXquXXnpJt912m5588klFR0dLkiZMmKCPPvpI69ev1/Dhw5WZmdmuYQEAAAAAABB6WjUDqkuXLvrXv/6lP/zhD9q6das+//xzGWP07LPP6r///a9mzZqlF198UTabrb3zAgAAAAAAIMS0agaUJEVGRur2229Xbm6uvvrqK/33v/9VbGysBg0aJIfD0Z4ZAQAAAAAAEMJaXUDVs9lsSklJaY8sAAAAAAAA6IRadQvecccdp7/+9a/aunVre+cBAABt5Pf7tWbNGq1YsUJr1qyR3+8PdiQAAACEuVbNgNqwYYNuuukmzZ49W+edd56uuOIKnXXWWe2dDQAAtFBhYaG8Xq/Ky8sDY06nU5mZmfJ4PEFMBqA1/H6/fD6fKisrFR8fL7fbzXIXAICQ1KoC6vvvv9eTTz6pRx55RM8884zy8/PVv39/XX755ZoyZYqOPPLI9s4JAAAOorCwUHl5eUpLS1NWVpZcLpdKSkqUn5+vvLw8ZWdnU0IBIYRCGQDQmbTqFrzu3btr2rRpKioq0qeffqqrrrpKlZWVmj17tvr27avzzz9fr776qowx7Z0XAAA0we/3y+v1Ki0tTTk5OUpJSVFMTIxSUlKUk5OjtLQ0eb1ebscDQkR9oexyuTR//nwVFBRo/vz5crlcysvLU2FhYbAjAgDQIq0qoPY1ePBg/f3vf9emTZv0+OOP6/TTT9c///lPjRo1Si6XS3PnztXGjRvbIysAANgPn8+n8vJyTZgwQXZ7w/+92+12ZWRkqLy8XD6fL0gJATQXhTIAoDNqcwFVLyoqSuecc45GjhypI488UsYYlZWVae7cuRowYICuvvpq7dy5s70uBwAA9lFZWSlJcrlcTW6vH6/fD8Chi0IZANAZtUsB9frrr2vChAlKTEzUrFmzZLPZNGfOHH3zzTfKz8/XSSedpEWLFunqq69uj8sBAICfiY+PlySVlJQ0ub1+vH4/AIcuCmUAQGfUqkXIJWnjxo169NFHtXjx4sBfaocPH65p06Zp9OjRgadzDBgwQOPHj9fo0aP1z3/+s31SAwCABtxut5xOp/Lz85WTk9Ng1kRdXZ0KCgrkdDrldruDmBJAc+xbKKekpDTaTqEMAAhFrZoBNWrUKPXr10+33HKLampqNGvWLH377bd69dVXdd555zX5aFiPx6Oqqqo2BwYAAI05HA5lZmaqqKhIubm5Ki4u1s6dO1VcXKzc3FwVFRUpMzOTx7cDIWDfQrmurq7BNgplAECoatUMqFdffVW/+tWvNG3aNI0dO1YREQc/zejRo9WnT5/WXA4AADSDx+NRdna2vF6vsrKyAuNOp1PZ2dk8th0IEfWFcl5ennJzc5WRkSGXy6WSkhIVFBSoqKhI2dnZFMoAgJDSqgLqq6++UnJycouOOf7443X88ce35nIAAKCZPB6P0tPT5fP5VFlZqfj4eLndbn5RBUIMhTIAoLNpVQHV0vIJAAB0HIfDodTU1GDHANBGFMoAgM6k1YuQS9KuXbtUVFSkTZs2qba2tsl9Jk2a1JZLAAAAAGGLQhkA0Fm0uoC67777NGfOnP0uLG6Mkc1mo4ACAAAAAAAIc616Ct5zzz2na6+9VklJSZo/f76MMTr33HN1xx13aMSIETLGaNy4cXr00UfbOy8AAAAAAABCTKsKqIULFyohIUErV67UDTfcIEkaMmSIZs2apZdffllPPPGEXnjhBblcrnYNCwAAAAAAgNDTqgJqzZo1GjNmjLp27RoY8/v9gf++6KKLdNZZZ+m2225re0IAAAAAAACEtFYVUHv27FGvXr0C78fExGjbtm0N9jnhhBP08ccftykcAAAAAAAAQl+rCqg+ffpo8+bNgfddLpc++eSTBvuUlJQoIqJND9kDAAAAAABAJ9CqAiotLa3B7KYRI0bogw8+UF5ennw+nx588EE999xzSktLa7egAAAAAAAACE2tKqAyMjJUW1ur9evXS5Kys7OVmJionJwcpaam6sorr1T37t111113tWdWAAAAAAAAhKBW3SM3duxYjR07NvB+r169tHr1aj3yyCNat26dXC6XLr30Uh111FHtFhQAAAAAAAChqd0WaYqPj1dWVlZ7nQ4AAAAAAACdRKtuwQMAAAAAAACaq1kzoJYuXdrqC0yaNKnVxwIAAAAAACD0NauAmjJlimw2W4tObIyRzWajgAIAoIP5/X75fD5VVlYqPj5ebrdbDocj2LEAAAAQxppVQC1evNjqHAAAoB0UFhbqkUceUUVFRWAsISFBl19+uTweTxCTAQAAIJw1q4CaPHmy1TkAAEAbFRYW6o477lBkZGSD8W3btumOO+7Q7NmzKaEAAAAQFB26CPk999yjAQMGdOQlAQAIC36/X/fdd58k6YQTTtD8+fNVUFCg+fPn64QTTpAk3X///fL7/cGMCQAAgDDVoQXUtm3bVFJS0pGXBAAgLHz22WeqqqrScccdpzlz5iglJUUxMTFKSUnRnDlzNGjQIG3btk2fffZZsKMCAAAgDHVoAQUAAKxRXyxdfPHFstsb/u/dbrfroosuarAfAAAA0JEooAAACAMtfZotAAAA0J4ooAAA6AQGDx4sSXryySdVV1fXYFtdXZ2eeuqpBvsBAAAAHYkCCgCATmDw4MGKi4vTF198odzcXBUXF2vnzp0qLi5Wbm6uvvjiC8XFxVFAAQAAICgigh0AAAC0ncPh0NVXX6077rhDq1ev1qpVqwLboqKiJElXX321HA5HsCICAAAgjDEDCgCATsLj8Wj27Nnq0aNHg/G4uDjNnj1bHo8nOMEAAAAQ9pgBBQBAJ+LxeJSeni6fz6fKykrFx8fL7XYz8wkAAABB1aEFlDFGxpiOvCQAAGHH4XAoNTU12DEAAACAgA69Be+yyy7T8uXLO/KSAAAAAAAACLJ2mwH1448/aufOnTriiCNktzfda7lcLrlcrva6JAAAAAAAAEJAs2dAbdiwQdXV1Y3Gly1bpiFDhiguLk69e/dWjx49dMUVV6iysrJdgwIAAAAAACA0NbuA6t+/v+65554GY48//rjOO+88ffbZZzr66KOVnp4um80mr9ers846S7W1te0eGAAAAAAAAKGl2QXUzxcQ37Fjh66//nr16NFDb7zxhr7++msVFhZq8+bNuuCCC7RmzRrdd999loQGAAAAAABA6Gj1IuRvvvmmtm3bpttuu01nnXVWYLxr16569NFHddRRR6mgoKBdQgIAAAAAACB0tbqAWrt2rWw2m0aNGtVoW3R0tH7961+ruLi4TeEAAAAAAAAQ+lpdQNXV1UmSjjzyyCa3O51O1dTUtPb0AAAAAAAA6CQiWrLz+vXr9e6770pSYIHxzZs3y+VyNdr3+++/V3x8fDtEBAAAAAAAQChrUQG1ZMkSLVmyRNJPi5LbbDa98847mjx5cqN9i4uL1a9fv3YJCQAAAABAqPL7/fL5fKqsrFR8fLzcbrccDkewYwEdqtkF1C233NLkeI8ePRqNrV27VkVFRbr66qtbHQwAAAAAgFBXWFgor9er8vLywJjT6VRmZqY8Hk8QkwEdq80FVFN69+6tdevWqWfPnq0KBQAAAABAqCssLFReXp7S0tKUlZUll8ulkpIS5efnKy8vT9nZ2ZRQCBstugWvubp3767u3btbcWoAOCRUVFSouro62DFarbS0tMGfoSo2NlYJCQnBjgEAANCI3++X1+tVWlqacnJyZLf/9AywlJQU5eTkKDc3V16vV+np6dyOh7BgSQEFAJ1ZRUWFpk2frj27dwc7SpstWLAg2BHapEtkpB5ctIgSCgAAHHJ8Pp/Ky8uVlZUVKJ/q2e12ZWRkKCsrSz6fT6mpqUFKCXQcSwqompoaFRUVSZKGDRtmxSUAIGiqq6u1Z/dudRngkT0mLthxwlZdTZX2rCtUdXU1BRQAADjkVFZWSlKTT43fd7x+P6Czs6SA2rBhg84880zZ7Xbt3bvXiksAQNDZY+Jk78ZadwAAAGgsPj5eklRSUqKBAwc2egpeSUlJg/2Azs6SAqpr164aNmyYbDabFacHAAAAAOCQ5na75XQ69eCDD6qqqkoVFRWBbQkJCYqLi5PT6ZTb7Q5iSqDj2A++S8slJSXpnXfe0fLly604vSSptrZWs2bNUp8+fRQTE6P09HS98cYbBz3uueee08SJEzVgwAB17dpVxx57rGbOnKlt27ZZlhUAAAAAEF4cDodOO+00rV27Vrt379Y111yjpUuX6pprrtHu3bu1du1anXbaaSxAjrARsouQT5kyRc8++6xmzJihgQMH6rHHHtPIkSO1fPlynX766fs9burUqerTp48uueQS9e3bV5999pnuvfdevfLKK/r4448VExPTgR8FAAAAAKAz8vv9+uCDD5ScnKzq6mrde++9gW1Op1PJycn64IMPNHnyZEoohIWQLKBWrVqlp59+WvPmzdNNN90kSZo0aZKOP/543XzzzSosLNzvsc8++6zOPPPMBmMnn3yyJk+erCeffFKXX365ldEBAAAAAGFg36fgNbUG1Ndff81T8BBWWlxA7dixQ88//7xWrFihtWvXqqqqSpIUFxengQMH6swzz9R5552nbt26tXvYes8++6wcDoemTp0aGIuOjlZmZqZmz56t0tJSJSUlNXnsz8snSRo7dqwmT56s4uJiqyIDAAAAAMLIvk/BczgcjUomnoKHcNOiAuq5557TlVdeqS1btsgY02j7u+++q0cffVS9evXS/fffr/PPP7/dgu7rk08+0THHHKPY2NgG40OHDpUkrV69er8FVFO+//57SdIRRxzRfiEBAAAAAGFr36fgpaSkNNrOU/AQbpq9CPnbb7+tjIwMGWN0yy23aOXKldqyZYt2796t3bt3a8uWLVq5cqX+9Kc/ye/3a8KECZYtQr5582b17t270Xj92KZNm1p0vjvvvFMOh0Pjx48/4H61tbWqrq5u8AYAAAAAwM/VPwUvPz9fdXV1DbbV1dWpoKCAp+AhrDS7gMrNzVWvXr20evVq/elPf1J6erp69uypiIgIRUREqGfPnkpPT9ctt9yiTz75RIcffrhyc3MtCV1TU6OoqKhG49HR0YHtzfXUU0/J6/Vq5syZGjhw4AH3zcvLU1xcXOCtJbOsAAAAAADhw+FwKDMzU0VFRcrNzVVxcbF27typ4uJi5ebmqqioSJmZmSxAjrDR7Fvw/vOf/2jKlCnq06fPQfdNTEzUxIkTtWTJkjaF25+YmBjV1tY2Gt+1a1dge3O89957yszM1DnnnKPbb7/9oPtnZ2frxhtvDLxfXV1NCQUAAAAAaJLH41F2dra8Xq+ysrIC406nU9nZ2fJ4PEFMB3SsZhdQTa35ZMUxzdG7d29t3Lix0fjmzZslqVkl2aeffqoxY8bo+OOP17PPPquIiIO/FFFRUU3OvAIAAAAAoCkej0fp6emNnoLHzCeEm2bfgnfiiSfqmWeeCZQ8B7Jx40Y988wzOumkk9oUbn+GDBmir7/+utEaTB9++GFg+4F8++23GjFihBISEvTKK6+oe/fuluQEAAAAAKD+KXhnnHGGUlNTKZ8Qlpo9A2r27Nn67W9/qyFDhui6667Tb37zGw0cOFBxcXGSpKqqKq1du1avv/66/v73v2vLli2aPXu2JaHHjx+v+fPn66GHHtJNN90k6acFwhcvXqz09PTAbXEbNmzQzp07Gzxx4Pvvv9fw4cNlt9v1r3/9S7169bIkIwAAB7Jr1y6VlZUFO0a7SExMDKzDCAAAADSl2QXUOeeco6VLl+q6667TnDlz9Kc//anJ/YwxiouL09KlSzV8+PB2C7qv9PR0ZWRkKDs7WxUVFUpOTtaSJUu0fv16eb3ewH6TJk3SihUrGtwKOGLECK1bt04333yz3n//fb3//vuBbU6nU7/5zW8syQwAwL7Kyso0Y8aMYMdoFwsXLlRycnKwYwAAAOAQ1uwCSpIuueQSjRo1Svn5+VqxYoXWrl2rqqoqSVJcXJwGDhyoM844QxMmTFCPHj2syBuwdOlSzZkzR48//rgqKyuVmpqqZcuWadiwYQc87tNPP5Uk3XXXXY22nXHGGRRQAIAOkZiYqIULF1p2/tLSUi1YsEAzZ860/IEZiYmJlp4fAAAAoa9FBZQk9ejRQ1OnTtXUqVOtyNNs0dHRmjdvnubNm7fffd55551GY1YtjA4AQEtER0d3yKyhpKQkZicBAAAg6Jq9CHl7mDt3brOeNgcAAAAAAIDOo8PbIGYgAQAAAAAOJTwcBLAe05EAAEDYqqioUHV1dbBjtFppaWmDP0NVbGysEhISgh0DQBjj4SCA9SigAABAWKqoqNC06dO1Z/fuYEdpswULFgQ7Qpt0iYzUg4sWUUIBCBqrHw4iddwDQng4CA5VFFAAACAsVVdXa8/u3eoywCN7TFyw44Stupoq7VlXqOrqagooAEHTUQ8HkXhACMIXBRQAAAhr9pg42bv1DHYMAACATq1Dn4IHAAAAAACA8EMBBQAAAAAAAEtRQAEAAAAAAMBSHVpAGWNkjOnISwIAAAAAACDIOrSAuuGGG/Tdd9915CUBAAAAAAAQZM1+Cl5FRUWrLrDv43Tj4uIUF8djjgEAAAAAAMJJswuoI488UjabrUUnt9ls2rt3b4tDAQAAAAAAoPNodgE1bNiwFhdQAAAAAAAAQLMLqHfeecfCGAAAAAAAAOisOnQRcgAAAAAAAISfZs+AasrGjRu1efNmSVLv3r111FFHtUsoAAAAAAAAdB4tLqB+/PFHzZ8/X48++qg2btzYYNtRRx2lzMxMzZw5U927d2+3kAAAAEA48vv98vl8qqysVHx8vNxutxwOR7BjAQDQYi0qoL799lv99re/1bfffitjjPr06aOkpCRJUmlpqcrKynTbbbfpqaee0muvvab+/ftbEhoAAADo7AoLC+X1elVeXh4YczqdyszMlMfjCWIyAABartlrQNXW1up3v/udvvnmG1144YUqLi5WWVmZVq5cqZUrV6qsrEzFxcW66KKLtHbtWo0cOVK1tbVWZgcAAAA6pcLCQuXl5cnlcmn+/PkqKCjQ/Pnz5XK5lJeXp8LCwmBHBACgRZpdQD3wwAP6+uuvdcstt+iJJ57Qscce22ifY489Vo8//rjmzp2rr776SosWLWrXsAAAAEBn5/f75fV6lZaWpuzsbO3evVurVq3S7t27lZ2drbS0NHm9Xvn9/mBHBQCg2Zp9C94//vEPJScn609/+tNB983JydETTzyhgoICXX/99W0KCAAAAIQTn8+n8vJyjRgxQtOnT290C94555yjVatWyefzKTU1NYhJAQBovmbPgPriiy80fPhw2Wy2g+5rs9k0fPhwFRcXtykcAAAAEG4qKyslSUuWLGnyFrylS5c22A8AgFDQ7AJqx44diouLa/aJY2NjtWPHjlaFAgAAAMJV/d+5jzvuOOXk5CglJUUxMTFKSUlRTk6OBg0a1GA/AABCQbMLqISEBH3zzTfNPvG3336rXr16tSoUAAAAgKY1544EAAAONc0uoE499VS9+uqr+v777w+67/fff6+XX35Zp512WpvCAQAAAOGmqqpKklRcXKzc3FwVFxdr586dDd7fdz8AAEJBswuo6dOn68cff9TYsWO1ZcuW/e63detWjR07Vjt37tTUqVPbJSQAAAAQLuLj4yVJkyZNUklJibKysjRhwgRlZWWppKREl156aYP9AAAIBc1+Ct6vfvUrXXHFFXr44Yc1aNAgTZs2TWeddZaSkpIkSaWlpXrrrbf08MMPa8uWLcrMzNRZZ51lWXAAAACgM3K73XI6nSouLtaiRYtUXFysyspKxcfHa9CgQcrLy5PT6ZTb7Q52VAAAmq3ZBZQk3X///YqNjdVf//pX5eXlKS8vr8F2Y4zsdrtuuOEG3XXXXe0aFAAAAAgHDodDmZmZgb9vZ2RkKC0tTSUlJcrLy1NRUZGys7PlcDiCHRUAgGZrUQHlcDg0b948TZ06VY899phWrlwZWBPqyCOPlMfj0aRJk3TMMcdIkmpraxUVFdX+qQEAAIBOzOPxKDs7W16vV1lZWYFxp9Op7OxseTyeIKYDAKDlWlRA1Rs4cKBuv/32/W7/+OOP5fV69fTTT2vr1q2tDgcAAACEK4/Ho/T0dPl8vsAteG63m5lPAICQ1KoCqinbtm3TE088Ia/XqzVr1sgYo5iYmPY6PQAAABB2HA6HUlNTgx0DAIA2a3MB9eabb8rr9eqf//ynamtrZYzRqaeeqssuu0wTJ05sj4wAAABAWPL7/cyAAgB0Cq0qoEpLS7V48WItXrxYGzZskDFGRx11lDZu3KgpU6bo0Ucfbe+cAAAAQFgpLCyU1+tVeXl5YMzpdCozM5M1oAAAIcfe3B337NmjgoICjRgxQgMGDNCtt96qLVu26OKLL9brr7+ukpISSVJERLvd1QcAAACEpcLCQuXl5cnlcmn+/PkqKCjQ/Pnz5XK5lJeXp8LCwmBHBACgRZrdFvXp00f//e9/ZbPZ9Ktf/UqTJk3S+eefr27dulmZDwAAAAgrfr9fXq9XaWlpysnJkd3+078Zp6SkKCcnR7m5ufJ6vUpPT+d2PABAyGj2DKitW7fKZrPphhtu0FNPPaVLL72U8gkAAABoZz6fT+Xl5ZowYUKgfKpnt9uVkZGh8vJy+Xy+ICUEAKDlml1ATZkyRTExMbr77ruVmJioMWPGqKCgQLt377YyHwAAABBWKisrJUkul6vJ7fXj9fsBABAKml1APfroo9q8ebMefPBBnXTSSVq2bJkuuOACOZ1OTZs2Te+//76VOQEAAICwEB8fL0mBNVZ/rn68fj8AAEJBswsoSerevbsuv/xyrVy5Uj6fTzNmzFBkZKQefvhhnXHGGbLZbPrqq6/2+z9LAAAAAAfmdrvldDqVn5+vurq6Btvq6upUUFAgp9Mpt9sdpIQAALRciwqofQ0aNEgLFizQxo0blZ+fr+HDh8tms+m9997T0UcfrbPPPluPP/54e2YFAAAAOj2Hw6HMzEwVFRXpz3/+s5YtW6Y33nhDy5Yt05///GcVFRUpMzOTBcgBACGl2U/B2+8JIiI0fvx4jR8/XmVlZVq8eLEee+wxLV++XO+8844uvfTS9sgJAAAAhA2Px6OxY8fqhRdeUFFRUWDc4XBo7Nix8ng8QUwHAEDLtbmA2ldiYqLmzJmjOXPm6K233tKjjz7anqcHAAAAwkJhYaGef/55nXLKKTr55JMVFRWl2tpa/ec//9Hzzz+vlJQUSigAQEhp1wJqX2effbbOPvtsq04PAAAAdEp+v19er1dpaWnKycmR3f6/VTNGjhyp3Nxceb1epaencxseACBktHoNKAAAAADtz+fzqby8XBMmTGhQPkmS3W5XRkaGysvL5fP5gpQQAICWo4ACAAAADiGVlZWSJJfL1eT2+vH6/QAACAWW3YIHAAAAoOXi4+MlSSUlJRo4cKB8Pp8qKysVHx8vt9utkpKSBvsBABAKKKAAAACAQ4jb7ZbT6dSDDz6oqqoqVVRUBLYlJCQoLi5OTqdTbrc7iCkBAGgZbsEDAAAADiEOh0OnnXaa1q5dq927d+uaa67R0qVLdc0112j37t1au3atTjvtNBYgBwCEFGZAAQAAAIcQv9+vDz74QMnJyaqurta9994b2OZ0OpWcnKwPPvhAkydPpoQCgBbYtWuXysrKgh2jXSQmJio6OjrYMVqEAgoAAAA4hNQ/BS8rK6vJNaC+/vprZWVlyefzKTU1NdhxASBklJWVacaMGcGO0S4WLlyo5OTkYMdoEQooAAAA4BCy71PwHA5Ho5KJp+ABQOskJiZq4cKFlp2/tLRUCxYs0MyZM5WUlGTZdaSfPpZQQwEFAAAAHEL2fQpeSkpKo+08BQ8AWic6OrpDZg0lJSWF3OykjsAi5AAAAMAhpP4pePn5+aqrq2uwra6uTgUFBTwFDwAQciigAAAAgEOIw+FQZmamioqKlJubq+LiYu3cuVPFxcXKzc1VUVGRMjMzWYAcABBSuAUPAAAAOMR4PB5lZ2fL6/UqKysrMO50OpWdnS2PxxPEdAAAtBwFFAAAAHAI8ng8Sk9Pb/QUPGY+AQBCEQUUAAAAcIhq6il4AACEItaAAgAAAAAAgKUooAAAAAAAAGApCigAAAAAAABYigIKAAAAAAAAlqKAAgAAAAAAgKUooAAAAAAAAGCpiGAHQOjw+/3y+XyqrKxUfHy83G63HA5HsGMBANAmdTVVwY4Q1nj9AQAIDxRQaJbCwkJ5vV6Vl5cHxpxOpzIzM+XxeIKYDACAttmzrjDYEQAAADo9CigcVGFhofLy8pSWlqasrCy5XC6VlJQoPz9feXl5ys7OpoQCAISsLgM8ssfEBTtG2KqrqaIEBAAgDFBA4YD8fr+8Xq/S0tKUk5Mju/2nZcNSUlKUk5Oj3Nxceb1epaenczseACAk2WPiZO/WM9gxAAAAOjUWIccB+Xw+lZeXa8KECYHyqZ7dbldGRobKy8vl8/mClBAAAAAAABzqKKBwQJWVlZIkl8vV5Pb68fr9AAAAAAAAfo4CCgcUHx8vSSopKWlye/14/X4AAAAAAAA/RwGFA3K73XI6ncrPz1ddXV2DbXV1dSooKJDT6ZTb7Q5SQgAAAAAAcKijgMIBORwOZWZmqqioSLm5uSouLtbOnTtVXFys3NxcFRUVKTMzkwXIAQAAAADAfvEUPByUx+NRdna2vF6vsrKyAuNOp1PZ2dnyeDxBTAcAAAAAAA51FFBoFo/Ho/T0dPl8PlVWVio+Pl5ut5uZTwAAIKzt2rVLZWVlwY7RLhITExUdHR3sGACATooCCs3mcDiUmpoa7BgAAACHjLKyMs2YMSPYMdrFwoULlZycHOwYAIBOigIKzeb3+5kBBQAAsI/ExEQtXLjQ0muUlpZqwYIFmjlzppKSkiy7TmJiomXnDnWdZaYbs9wABBMFFJqlsLBQXq9X5eXlgTGn06nMzEzWgAIAAGErOjq6w2YNJSUlMUMpSDrLTDdmuQEIJgooHFRhYaHy8vKUlpamrKwsuVwulZSUKD8/X3l5eSxEDgAAgE7N6pluzHIDEA4ooHBAfr9fXq9XaWlpysnJkd1ulySlpKQoJydHubm58nq9Sk9P53Y8AAAAdEodNdONWW4AOjN7sAPg0Obz+VReXq4JEyYEyqd6drtdGRkZKi8vl8/nC1JCAAAAAABwqGMGFA6osrJSkuRyuZpchNzlcjXYDwAAAAAA4OcooHBA8fHxkqRly5bptddea7QI+TnnnNNgPwAAAAAAgJ+jgMIBud1uxcXFacmSJRo6dGijRciXLl2quLg4ud3uYEcFAAAAAACHKNaAQrMZYxq9SZLNZgtyMgAAAAAAcChjBhQOyOfzqaqqSpMnT9Zrr72mrKyswDan06lJkyZp6dKl8vl8Sk1NDWJSAAAAAABwqKKAwgHVLy4+atQonX/++Y0WIa+trdXSpUtZhBwAAAAAAOwXBRQOqH5x8ZKSEqWkpDSa5VRSUtJgPwAAAAAAgJ9jDSgckNvtltPpVH5+vurq6hpsq6urU0FBgZxOJ4uQAwAAAACA/WIGFA7I4XAoMzNTeXl5ys3NVUZGRuApeAUFBSoqKlJ2drYcDkewowIdrq6mKtgRwhqvPwAAABA6KKBwUB6PR9nZ2fJ6vY0WIc/OzpbH4wliOiB49qwrDHYEAAAAAAgJFFBoFo/Ho/T09EaLkDPzCeGsywCP7DFxwY4RtupqqiwvASsqKlRdXW3pNaxSWlra4M9QFRsbq4SEhGDHAAAAQBtRQKHZHA5Ho0XIgXBmj4mTvVvPYMeARSoqKjRt+nTt2b072FHaZMGCBcGO0CZdIiP14KJFlFAAAAAhjgIKAIAmVFdXa8/u3cx0C6L6WW7V1dUUUAAAACGOAqoT2bVrl8rKyoIdo10kJiYqOjo62DEAgJluAAAAQDuggOpEysrKNGPGjGDHaBcLFy5UcnJysGMAAAAAAIB2QAHViSQmJmrhwoWWXqO0tFQLFizQzJkzlZSUZNl1EhMTLTs3AAAAgNATyg8HkTrHA0J4OAjaggKqE4mOju6wWUNJSUnMUAIAAADQITrLw0Gk0H5ACA8HQVtQQAEAAAAADmk8HCT4eDgI2ooCCgAAAAAQEng4CBC67MEOAAAAAAAAgM6NAgoAAAAAAACWooACAAAAAACApSigAAAAAAAAYCkKKAAAAAAAAFiKAgoAAAAAAACWooACAAAAAACApSigAAAAAAAAYKmQLaBqa2s1a9Ys9enTRzExMUpPT9cbb7xx0OO++uor3XDDDfJ4PIqOjpbNZtP69eutDwwAAAAAABCmQraAmjJliu6++25dfPHFuueee+RwODRy5Ei9//77Bzxu5cqV+tvf/qbt27dr0KBBHZQWAAAAAAAgfIVkAbVq1So9/fTTysvL07x58zR16lS9/fbbcrlcuvnmmw947JgxY7Rt2zZ99tlnuvjiizsoMQAAAAAAQPgKyQLq2WeflcPh0NSpUwNj0dHRyszM1MqVK1VaWrrfY3v27KnDDjusI2ICAAAAAABAIVpAffLJJzrmmGMUGxvbYHzo0KGSpNWrVwchFQAAAAAAAJoSEewArbF582b17t270Xj92KZNmyy5bm1trWprawPvV1dXW3IdAAAAAACAziQkC6iamhpFRUU1Go+Ojg5st0JeXp7mzp1rybkBAAAAAAh3FRUVITvZo345oAMtCxQKYmNjlZCQ0O7nDckCKiYmpsFMpHq7du0KbLdCdna2brzxxsD71dXVSkpKsuRaAAAAAACEk4qKCk2bPl17du8OdpQ2WbBgQbAjtEmXyEg9uGhRu5dQIVlA9e7dWxs3bmw0vnnzZklSnz59LLluVFRUkzOvAAAAAABA21RXV2vP7t3qMsAje0xcsOOEpbqaKu1ZV6jq6moKKEkaMmSIli9frurq6gYLkX/44YeB7QAAAAAAIPTYY+Jk79Yz2DHQzkLyKXjjx4+X3+/XQw89FBirra3V4sWLlZ6eHrgtbsOGDfryyy+DFRMAAAAAAAAK0RlQ6enpysjIUHZ2tioqKpScnKwlS5Zo/fr18nq9gf0mTZqkFStWyBgTGKuqqtLf//53SdIHH3wgSbr33nvVo0cP9ejRQ9dcc03HfjAAAAAAAACdXEgWUJK0dOlSzZkzR48//rgqKyuVmpqqZcuWadiwYQc8rrKyUnPmzGkwVr9AmMvlooACAAAAAABoZyFbQEVHR2vevHmaN2/efvd55513Go3169evwYwoAAAAAAAAWCsk14ACAAAAAABA6AjZGVAAAADtoa6mKtgRwhqvPwAA4YECCkCA3++Xz+dTZWWl4uPj5Xa75XA4gh0LACwRGxurLpGR2rOuMNhRwl6XyEjFxsYGOwYAALAQBRQASVJhYaG8Xq/Ky8sDY06nU5mZmfJ4PEFMBgDWSEhI0IOLFqm6ujrYUVqttLRUCxYs0MyZM5WUlBTsOK0WGxurhISEYMcAAAAWooACoMLCQuXl5SktLU1ZWVlyuVwqKSlRfn6+8vLylJ2dTQkFoFNKSEjoFMVHUlKSkpOTgx0DAABgv1iEHAhzfr9fXq9XaWlpysnJUUpKimJiYpSSkqKcnBylpaXJ6/XK7/cHOyoAAAAAIERRQAFhzufzqby8XBMmTJDd3vBHgt1uV0ZGhsrLy+Xz+YKUEAAAAAAQ6iiggDBXWVkpSXK5XE1urx+v3w8AAAAAgJaigALCXHx8vCSppKSkye314/X7AQAAAADQUixCDoQ5t9stp9Op/Px85eTkNLgNr66uTgUFBXI6nXK73UFMCQRPXU1VsCOELV57AM1VUVER8k+03PfPUMUTLQEcCAUUEOYcDocyMzOVl5en3NxcZWRkBJ6CV1BQoKKiImVnZ8vhcAQ7KhAUe9YVBjsCAOAAKioqNG36dO3ZvTvYUdpswYIFwY7QJl0iI/XgokWUUACaRAEFQB6PR9nZ2fJ6vcrKygqMO51OZWdny+PxBDEdEFxdBnhkj4kLdoywVFdTRQEI4KCqq6u1Z/dufl4HWf3P7OrqagooAE2igAIg6acSKj09XT6fT5WVlYqPj5fb7WbmE8KePSZO9m49gx0DAHAQ/LwGgEMbBRSAAIfDodTU1GDHAAAAAAB0MhRQAAAAAICQwAMqgofXHm1FAQUAAAAACAmsDQiELgooAAAAAEBIYLH54OHhIGgrCigAAAAAQEhgsXkgdNmDHQAAAAAAAACdGzOgAAT4/X75fD5VVlYqPj5ebrdbDocj2LEAAAAAACGOAgqAJKmwsFBer1fl5eWBMafTqczMTHk8niAmAwAAAACEOgooACosLFReXp7S0tKUlZUll8ulkpIS5efnKy8vT9nZ2ZRQAAAAADpEXU1VsCOELStfewooIMz5/X55vV6lpaUpJydHdvtPS8OlpKQoJydHubm58nq9Sk9P53Y8AAAAAJbjaXudEwUUEOZ8Pp/Ky8uVlZUVKJ/q2e12ZWRkKCsrSz6fT6mpqUFKCQAAACBcdBngkT0mLtgxwlJdTZVlBSAFFBDmKisrJUkul6vJ7fXj9fsBAAAAgJXsMXGyd+sZ7BhoZ/aD7wKgM4uPj5cklZSUNLm9frx+PwAAAAAAWooZUB2soqJC1dXVwY7RaqWlpQ3+DEWxsbFKSEgIdoxDhtvtltPpVH5+foM1oCSprq5OBQUFcjqdcrvdQUwJAEDr8fev4OuIv3+xaHFw8foDOBgKqA5UUVGhadOna8/u3cGO0mYLFiwIdoRW6xIZqQcXLaKE+v8cDocyMzOVl5en3NxcZWRkBJ6CV1BQoKKiImVnZ7MAOQAgJPH3r0NDR/z9i0WLAeDQRgHVgaqrq7Vn924WVAui+gXVqqurKaD24fF4lJ2dLa/Xq6ysrMC40+lUdna2PB5PENMduviXvuDi9QfQHPz9K/g66u9ffI6Dy8qFiwF0DhRQQcCCajgUeTwepaeny+fzqbKyUvHx8XK73cx8akJsbKy6REbyl6xDQJfISMXGxgY7BoAQwN+/Oj8+xwBwaKOAAhDgcDiUmpoa7BiHvISEBD24aFHIryeyYMECzZw5U0lJScGO02qs6QYAAACEBgooIMTs2rVLZWVlwY7RLhITExUdHR3sGK2SkJDQKYqPpKQkJScnBzsGAAAAgE6OAgoIMWVlZZoxY0awY7SLhQsXUn4AAAAAQBiggAJCTGJiohYuXGjZ+Tvy1qzExERLzw8AAAAAODRQQAEhJjo6ukNmDXFrFgAAAACgvdiDHQAAAAAAAACdGzOgAAA4gLqaqmBHCFu89gAAAJ0HBRQAAE2IjY1Vl8hI7VlXGOwoYa1LZKRiY2ODHQMAAABtRAEFAEATEhIS9OCiRaqurg52lFbpyAcKWCk2NlYJCQnBjgEAAIA2ooACAGA/EhISQr784IECAAAAOBSwCDkAAAAAAAAsRQEFAAAAAAAAS1FAAQAAAAAAwFIUUAAAAAAAALAUBRQAAAAAAAAsxVPwAAAAAAAhoa6mKtgRwhavPdqKAgoAAAAAcEiLjY1Vl8hI7VlXGOwoYa1LZKRiY2ODHQMhigIKAAAAAHBIS0hI0IOLFqm6ujrYUVqttLRUCxYs0MyZM5WUlBTsOK0SGxurhISEYMdAiKKACgKmLgYPrz0AAAAQmhISEjpF+ZGUlKTk5ORgxwA6HAVUEDBtFAAAAAAAhBMKqCDoMsAje0xcsGOEpbqaKgpAAAAAAAA6GAVUENhj4mTv1jPYMQAAAAAAADqEPdgBAAAAAAAA0LlRQAEAAAAAAMBSFFAAAAAAAACwFGtAAQAAoNOrq6kKdoSwxWsPAJAooAAAABAGeAouAADBRQEFAACATq/LAI/sMXHBjhGW6mqqKAABtAgzJ4PHyteeAgoAAACdnj0mTvZuPYMdAxbiF9bg4vVHe4iNjVWXyEhK6yDrEhmp2NjYdj8vBRQAAACAkMUvrIcOq35pRfhISEjQg4sWqbq6OthRWqW0tFQLFizQzJkzlZSUFOw4rRYbG6uEhIR2Py8FFAAAAICQFeq/sEr80grsKyEhIeS/jpKSkpScnBzsGIccCigAAAAAIa0z/MIq8UsrgM7NHuwAAAAAAAAA6NwooAAAAAAAAGApbsEDLFBRURGy6xCUlpY2+DNUsQYBAAAAABw6KKCAdlZRUaFp06drz+7dwY7SJgsWLAh2hDbpEhmpBxctooQCAAAAgEMABRTQzqqrq7Vn9251GeCRPSYu2HHCUl1NlfasK1R1dTUFFAAAAAAcAiiggqCupirYEcJWR7729pg42bv17LDrAQAAAABwqKKA6kCxsbHqEhmpPesKgx0lrHWJjFRsbGywYwAAAAAAEDYooDpQQkKCHly0KGQXp5Z+Wph6wYIFmjlzppKSkoIdp1VYnBoAAAAAgI5FAdXBEhISOkX5kZSUpOTk5GDHAAAAAAAAIcAe7AAAAAAAAADo3CigAAAAAAAAYCkKKAAAAAAAAFiKAgoAAAAAAACWooACAAAAAACApSigAAAAAAAAYCkKKAAAAAAAAFgqItgBgM6qrqYq2BHCFq89AAAAABxaKKAAi+xZVxjsCAAA4P/jHyeCh9ceACBRQAGW6TLAI3tMXLBjhKW6mioKQACAJCk2NlZdIiP5/0KQdYmMVGxsbLBjAACCiAIKsIg9Jk72bj2DHQMAgLCWkJCgBxctUnV1dbCjtFppaakWLFigmTNnKikpKdhxWiU2NlYJCQnBjgEACCIKKAAAAHRqCQkJnaL8SEpKUnJycrBjAADQKjwFDwAAAAAAAJaigAIAAAAAAIClKKAAAAAAAABgKQooAAAAAAAAWIoCCgAAAAAAAJbiKXgAAAAAcAC7du1SWVmZZecvLS1t8KdVEhMTFR0dbek1AGB/KKAAAAAA4ADKyso0Y8YMy6+zYMECS8+/cOFCJScnW3oNANgfCijAInU1VcGOELZ47QEAQHtKTEzUwoULgx2jzRITE4MdAUAYo4AC2llsbKy6REZqz7rCYEcJa10iIxUbGxvsGAAAoBOIjo5m5hAAtBEFFNDOEhIS9OCiRaqurg52lFYpLS3VggULNHPmTCUlJQU7TqvFxsYqISEh2DEAAAAAAKKAAiyRkJAQ8uVHUlIS/9IHAAAAAGgX9mAHAAAAAAAAQOdGAQUAAAAAAABLUUABAAAAAADAUhRQAAAAAAAAsBQFFAAAAAAAACxFAQUAAAAAAABLUUABAAAAAADAUhRQAAAAAAAAsBQFFAAAAAAAACxFAQUAAAAAAABLRQQ7AACgsV27dqmsrMyy85eWljb40yqJiYmKjo629BoAAAAADn0UUABwCCorK9OMGTMsv86CBQssPf/C/9fevcfVlP3/A3+dUp2mKynVZCq5jVsR4jOowUhIKE0MigymXHJp5juMUZ8xgzHRfMxX9P2M3JphBhkzLgnhQUauY8ggYty7K1SjWr8/PM7+OZ1TKh3dXs/Hw+PhrL323mvvddbau/dZe+3ISLRu3Vqj+yAiIiIiorqPASgiojrIxsYGkZGRtV2MV2ZjY1PbRSAiIiIiojqAASgiojpILpdz5BARERERETUYnISciIiIiIiIiIg0qt4GoIqKivDJJ5/A2toa+vr6cHFxQUJCQqXWvXv3Lnx9fWFqagpjY2N4eXnhxo0bGi4xEREREREREVHjVG8DUAEBAVixYgU++OADfPvtt9DW1saQIUNw7NixCtd7/Pgx3n33XRw5cgTz589HeHg4zp07B1dXV2RlZb2m0hMRERERERERNR71cg6o5ORkbNmyBcuXL8e8efMAABMmTECnTp3w8ccfIykpqdx1V69ejWvXriE5ORk9evQAAHh4eKBTp06IiIjAV1999VqOgYiIiIiIiIiosaiXI6C2bdsGbW1tTJkyRUqTy+UIDAzEiRMncPv27QrX7dGjhxR8AoD27dtjwIAB+OmnnzRabiIiIiIiIiKixqheBqDOnTuHtm3bwtjYWCm9Z8+eAIDz58+rXa+0tBQXLlxA9+7dVZb17NkT169fR35+fo2Xl4iIiIiIiIioMauXj+Ddv38fVlZWKumKtHv37qldLzs7G0VFRS9dt127dmrXLyoqQlFRkfQ5Ly+vymXXpMLCQty5c0ej+1CMLqtolFlNsLGxgVwu1+g+6itN1/PrqmOA9UyNG9ty49BQ6pl1XD7efxE1DGzLjUNDuS4D9bOe62UAqqCgAHp6eirpipNfUFBQ7noAqrUuACxZsgTh4eFVLu/rcufOHYSEhLyWfUVERGh0+5GRkWjdurVG91Ffva561nQdA6xnatzYlhuHhlLPrOPy8f6LqGFgW24cGsp1Gaif9VwvA1D6+vpKI5EUCgsLpeXlrQegWusCwKeffoo5c+ZIn/Py8tCyZcvKF1zDbGxsEBkZWdvFqBE2Nja1XYQ6i/VM1DCwLTcODaWeWcflayh1DLCeqXFjW24cWM+1q14GoKysrHD37l2V9Pv37wMArK2t1a7XrFkz6OnpSfmqsi7wfOSUutFTdYVcLq93EVCqOtYzUcPAttw4sJ4bPtYxUcPAttw4sJ5rV72chNzJyQlXr15VmYPp5MmT0nJ1tLS00LlzZ5w+fVpl2cmTJ9GqVSsYGRnVeHmJiIiIiIiIiBqzehmA8vHxQUlJCaKjo6W0oqIixMTEwMXFRXos7u+//8Zff/2lsu6pU6eUglBXrlzBoUOHMHr06NdzAEREREREREREjYhMCCFquxDV4evri7i4OMyePRutW7fGhg0bkJycjIMHD6Jfv34AADc3Nxw5cgQvHmJ+fj66du2K/Px8zJs3Dzo6OlixYgVKSkpw/vx5mJubV7oMeXl5MDExwaNHj2BsbFzjx0hEREREREREVFdVJS5SL+eAAoCNGzdi4cKF2LRpE3JyctClSxf89ttvUvCpPEZGRjh8+DBmz56NxYsXo7S0FG5ubli5cmWVgk9ERERERERERFQ59XYEVF3AEVBERERERERE1FhVJS5SL+eAIiIiIiIiIiKi+oMBKCIiIiIiIiIi0igGoIiIiIiIiIiISKMYgCIiIiIiIiIiIo1iAIqIiIiIiIiIiDSKASgiIiIiIiIiItIoBqCIiIiIiIiIiEijGIAiIiIiIiIiIiKNYgCKiIiIiIiIiIg0igEoIiIiIiIiIiLSKAagiIiIiIiIiIhIoxiAIiIiIiIiIiIijWIAioiIiIiIiIiINIoBKCIiIiIiIiIi0igGoIiIiIiIiIiISKMYgCIiIiIiIiIiIo1iAIqIiIiIiIiIiDSKASgiIiIiIiIiItIoBqCIiIiIiIiIiEijGIAiIiIiIiIiIiKNYgCKiIiIiIiIiIg0qkltF6A+E0IAAPLy8mq5JEREREREREREr5ciHqKIj1SEAahXkJ+fDwBo2bJlLZeEiIiIiIiIiKh25Ofnw8TEpMI8MlGZMBWpVVpainv37sHIyAgymay2i/Na5OXloWXLlrh9+zaMjY1ruzikAazjxoH13PCxjhsH1nPjwHpu+FjHjQPrueFrjHUshEB+fj6sra2hpVXxLE8cAfUKtLS0YGNjU9vFqBXGxsaNpkE1VqzjxoH13PCxjhsH1nPjwHpu+FjHjQPrueFrbHX8spFPCpyEnIiIiIiIiIiINIoBKCIiIiIiIiIi0igGoKhK9PT0sGjRIujp6dV2UUhDWMeNA+u54WMdNw6s58aB9dzwsY4bB9Zzw8c6rhgnISciIiIiIiIiIo3iCCgiIiIiIiIiItIoBqCIiIiIiIiIiEijGIAiIiIiIiIiIiKNYgCqnli/fj1kMhlu3rxZ5XWvXbuGQYMGwcTEBDKZDDt37qzx8pUnLCwMMpnste2P6hc3Nze4ubnVdjHqpVfpEwD2C0REmlCf+rfDhw9DJpPh8OHDtV0Uokp71fufxuzmzZuQyWRYv359bRfltVP0zZmZmTWyPXXfQ/5dUzkMQDUC/v7++PPPP/Hll19i06ZN6N69e41u/+nTpwgLC+MNDFE9wn6B6PVJSkpCWFgYcnNza7soRBX64YcfEBkZWdvFqJdWr14NmUwGFxcXtctlMpn0T0tLC9bW1hg0aJDa62RJSQmsra0hk8mwd+/eCvcbFxcHDw8PNG/eHLq6urC2toavry8OHTpUE4dFDdyePXsQFhZW28WgRoQBqHpi/PjxKCgogK2tbZXWKygowIkTJxAYGIjp06dj3LhxsLGxqdGyPX36FOHh4WovoJ999hkKCgpqdH9EVP0+AWC/QPS6JSUlITw8nAEoqlP69euHgoIC9OvXT0pjAKr6YmNjYWdnh+TkZKSmpqrN895772HTpk3YsGEDpk2bhgsXLqB///4qQaZDhw7h/v37sLOzQ2xsrNptCSEwceJEjBo1Cg8fPsScOXOwZs0aBAcH48aNGxgwYACSkpJq/Dip/rK1tUVBQQHGjx8vpe3Zswfh4eG1WCpqbJrUdgGocrS1taGtrV3l9TIyMgAApqamNVyiymnSpAmaNOHXrD4QQqCwsBD6+vq1XRSqhOr2CQD7BaqaJ0+ewMDAoLaLQbWkuLgYpaWl0NXVre2iUA3T0tKCXC6v7WI0CGlpaUhKSsKOHTswdepUxMbGYtGiRSr52rZti3HjxkmfR44ciS5duiAyMhIeHh5S+ubNm9GtWzf4+/tj/vz5avvhiIgIrF+/HiEhIVixYoXSo58LFizApk2beK0lJTKZjG2eah1HQNUTZZ8ztbOzw7Bhw3Ds2DH07NkTcrkcrVq1wsaNG6V1wsLCpNERoaGhkMlksLOzk5bfvXsXkyZNQosWLaCnp4eOHTti3bp1KvsuLCxEWFgY2rZtC7lcDisrK4waNQrXr1/HzZs3YW5uDgAIDw+XhhYrhnKqmwuhuLgYX3zxBRwcHKCnpwc7OzvMnz8fRUVFSvkqc4yNieJcpqamIiAgAKampjAxMcHEiRPx9OlTKV9Vz298fDy6d+8OfX19rF27VpoT4qeffkJ4eDjefPNNGBkZwcfHB48ePUJRURFCQkJgYWEBQ0NDTJw4UWXbMTEx6N+/PywsLKCnp4cOHTogKirqtZwnTdHU+a/u91vds+fsFxqm/Px8hISEwM7ODnp6erCwsMB7772Hs2fPSnlOnjyJIUOGoGnTpjAwMECXLl3w7bffSssvXLiAgIAAtGrVCnK5HJaWlpg0aRKysrKU9qWom5SUFIwdOxZNmzZFnz59XtuxNkRhYWEIDQ0FANjb20vtoaK5OF5sL4ptyGQyXL16FePGjYOJiQnMzc2xcOFCCCFw+/ZteHl5wdjYGJaWloiIiFDZZnp6OgIDA9GiRQvI5XI4Ojpiw4YNSnkUZfrmm28QGRkptceUlJQaPScNxbFjx9CjRw/I5XI4ODhg7dq1avNt3rwZzs7O0NfXR7NmzeDn54fbt28r5XFzc0OnTp1w5swZ/Otf/4K+vj7s7e2xZs0ale1Vpi4BYMuWLXB2doaRkRGMjY3RuXNnpX6h7BxQbm5u2L17N27duiV9T1+8PlD5YmNj0bRpUwwdOhQ+Pj7ljloqq3PnzmjevDnS0tKktIKCAsTFxcHPzw++vr4oKCjAL7/8orReQUEBlixZgvbt2+Obb75RO+/Y+PHj0bNnz1c7sHpk79696Nu3LwwMDGBkZIShQ4fi0qVLSnkCAgJgaGiIu3fvYsSIETA0NIS5uTnmzZuHkpISpbwvaz/lefLkCebOnYuWLVtCT08P7dq1wzfffAMhhFI+mUyG6dOnIzY2Fu3atYNcLoezszOOHj2qlO/WrVsICgpCu3btoK+vDzMzM4wePVrtHFi5ubmYPXu2dL9gY2ODCRMmSPMelb3uBAQE4H//93+l8ij+CSFgZ2cHLy8vlX0UFhbCxMQEU6dOfem5qIsyMzPh6+sLY2NjmJmZYdasWSgsLARQ8RxZZa/LlVWZ/rq8+fjUlefBgweYOHEibGxsoKenBysrK3h5edWrOdEYFq/HUlNT4ePjg8DAQPj7+2PdunUICAiAs7MzOnbsiFGjRsHU1BSzZ8/GmDFjMGTIEBgaGgIAHj58iF69ekmdn7m5Ofbu3YvAwEDk5eUhJCQEwPNn0IcNG4aDBw/Cz88Ps2bNQn5+PhISEnDx4kUMHDgQUVFR+OijjzBy5EiMGjUKANClS5dyyz158mRs2LABPj4+mDt3Lk6ePIklS5bg8uXLiIuLq9IxNka+vr6wt7fHkiVLcPbsWfz3v/+FhYUFli1bBqBq5/fKlSsYM2YMpk6dig8//BDt2rWTli1ZsgT6+vr4n//5H6SmpmLVqlXQ0dGBlpYWcnJyEBYWht9//x3r16+Hvb09Pv/8c2ndqKgodOzYEcOHD0eTJk3w66+/IigoCKWlpQgODn49J0pDavL8a+L7zX6h4fUL06ZNw7Zt2zB9+nR06NABWVlZOHbsGC5fvoxu3bohISEBw4YNg5WVFWbNmgVLS0tcvnwZv/32G2bNmgUASEhIwI0bNzBx4kRYWlri0qVLiI6OxqVLl/D777+r/PEyevRotGnTBl999ZXKTTNVzahRo3D16lX8+OOPWLlyJZo3bw7g+R8pVfX+++/j7bffxtKlS7F7924sXrwYzZo1w9q1a9G/f38sW7YMsbGxmDdvHnr06CE9WlVQUAA3NzekpqZi+vTpsLe3x88//4yAgADk5uZK3xOFmJgYFBYWYsqUKdDT00OzZs1e/UQ0MH/++ScGDRoEc3NzhIWFobi4GIsWLUKLFi2U8n355ZdYuHAhfH19MXnyZGRkZGDVqlXo168fzp07pzQSNScnB0OGDIGvry/GjBmDn376CR999BF0dXUxadIkAJWvy4SEBIwZMwYDBgyQrk+XL1/G8ePHVepbYcGCBXj06BHu3LmDlStXAoB0faCKxcbGYtSoUdDV1cWYMWMQFRWFU6dOoUePHhWul5OTg5ycHLRu3VpK27VrFx4/fgw/Pz9YWlrCzc0NsbGxGDt2rJTn2LFjyM7ORkhISLVHQzckmzZtgr+/P9zd3bFs2TI8ffoUUVFR6NOnD86dO6cUSC0pKYG7uztcXFzwzTff4MCBA4iIiICDgwM++ugjANVrP8DzpwmGDx+OxMREBAYGwsnJCfHx8QgNDcXdu3eldqVw5MgRbN26FTNnzoSenh5Wr16NwYMHIzk5GZ06dQIAnDp1CklJSfDz84ONjQ1u3ryJqKgouLm5ISUlBW+88QYA4PHjx+jbty8uX76MSZMmoVu3bsjMzMSuXbtw584d6drzoqlTp+LevXtISEjApk2bpHSZTIZx48bh66+/RnZ2ttI14Ndff0VeXp7SSL76xNfXF3Z2dliyZAl+//13/Oc//0FOTo5Gfsis6rW3Mry9vXHp0iXMmDEDdnZ2SE9PR0JCAv7+++/684OBoHohJiZGABBpaWlCCCFsbW0FAHH06FEpT3p6utDT0xNz586V0tLS0gQAsXz5cqXtBQYGCisrK5GZmamU7ufnJ0xMTMTTp0+FEEKsW7dOABArVqxQKVNpaakQQoiMjAwBQCxatEglz6JFi8SLX7Pz588LAGLy5MlK+ebNmycAiEOHDklplT3GxkJxLidNmqSUPnLkSGFmZiaEqN753bdvn1LexMREAUB06tRJ/PPPP1L6mDFjhEwmEx4eHkr5e/fuLWxtbZXSFN+fF7m7u4tWrVoppbm6ugpXV9eKD7yO0NT5r+73u2yfUJVtsl+oX0xMTERwcLDaZcXFxcLe3l7Y2tqKnJwcpWWKuhBCfZv88ccfVc6lom7GjBlTM4UnIYQQy5cvV2mvinYYExOjkr9s21HUy5QpU6S04uJiYWNjI2QymVi6dKmUnpOTI/T19YW/v7+UFhkZKQCIzZs3S2n//POP6N27tzA0NBR5eXlKZTI2Nhbp6emvfuAN2IgRI4RcLhe3bt2S0lJSUoS2trbUv928eVNoa2uLL7/8UmndP//8UzRp0kQp3dXVVQAQERERUlpRUZFwcnISFhYW0vW4snU5a9YsYWxsLIqLi8s9BsX1PjExUUobOnSoyjWdKnb69GkBQCQkJAghnve9NjY2YtasWUr5AIjAwECRkZEh0tPTxcmTJ8WAAQNU6n3YsGHinXfekT5HR0eLJk2aKLXJb7/9VgAQcXFxGj22uqjs/U9+fr4wNTUVH374oVK+Bw8eCBMTE6V0f39/AUD8+9//VsrbtWtX4ezsLH2uTPtRZ+fOnQKAWLx4sVK6j4+PkMlkIjU1VUoDIACI06dPS2m3bt0ScrlcjBw5UkpTd/0+ceKEACA2btwopX3++ecCgNixY4dKfsX9gLrrTnBwsNI9mcKVK1cEABEVFaWUPnz4cGFnZ6d0j1EfKK6jw4cPV0oPCgoSAMQff/xRpeuyuvvwsn/XVLa/VtcXC6FaXzk5OWrv3+sbPoJXj3Xo0AF9+/aVPpubm6Ndu3a4ceNGhesJIbB9+3Z4enpCCIHMzEzpn7u7Ox49eiQ92rF9+3Y0b94cM2bMUNlOdV4zvGfPHgDAnDlzlNLnzp0LANi9e3eNHGNDNm3aNKXPffv2RVZWFvLy8qp8fu3t7eHu7q52PxMmTICOjo702cXFBUII6VfYF9Nv376N4uJiKe3FeaQePXqEzMxMuLq64saNG3j06FFlD7VOqsnzr4nvN/uFhsfU1BQnT57EvXv3VJadO3cOaWlpCAkJUZnT68W6eLFNFhYWIjMzE7169QIApUf5FMp+z6lumDx5svR/bW1tdO/eHUIIBAYGSummpqYq7WHPnj2wtLTEmDFjpDQdHR3MnDkTjx8/xpEjR5T24+3tLT1GS6pKSkoQHx+PESNG4K233pLS3377baVr6o4dO1BaWgpfX1+lPtXS0hJt2rRBYmKi0nabNGmi9FiLrq4upk6divT0dJw5cwZA5evS1NQUT548QUJCgkbOAf1/sbGxaNGiBd59910Az/ve999/H1u2bFF5rOv777+Hubk5LCws4OLiguPHj2POnDnSCOOsrCzEx8cr1a+3t7c0NYJCXl4eAMDIyEjDR1f3JSQkIDc3F2PGjFFqZ9ra2nBxcVFpZ4D6e7kX+8zqtp89e/ZAW1sbM2fOVEqfO3cuhBAqk8337t0bzs7O0ue33noLXl5eiI+Pl747L16/nz17hqysLLRu3RqmpqZK1+/t27fD0dERI0eOVClXde7N2rZtCxcXF6XHSbOzs7F371588MEH1dpmXVD2SQzFvaziXrQmVfXa+zL6+vrQ1dXF4cOHkZOTU9PFfW0YgKrHXrzpUWjatOlLv5AZGRnIzc1FdHQ0zM3Nlf5NnDgRwPPnVQHg+vXraNeuXY1NYnjr1i1oaWkpDTUGAEtLS5iamuLWrVtK6dU9xoas7Dlp2rQpgOfDuKt6fu3t7Su9HxMTEwBAy5YtVdJLS0uVAkvHjx/HwIEDYWBgAFNTU5ibm2P+/PkAUO8DUDV5/l/2/S4pKcGDBw+U/v3zzz9VKl/ZbZaH/ULd9fXXX+PixYto2bIlevbsibCwMOlG+fr16wAgDdUvT3Z2NmbNmoUWLVpAX18f5ubmUvtX1yYr6huo9qjrl+VyucqjFSYmJkrt4datW2jTpg20tJRv+95++21p+YtY/xXLyMhAQUEB2rRpo7LsxUfZr127BiEE2rRpo9KvXr58WepTFaytrVUmmm7bti0ASPN7VLYug4KC0LZtW3h4eMDGxgaTJk3Cvn37Xu3ASUVJSQm2bNmCd999F2lpaUhNTUVqaipcXFzw8OFDHDx4UCm/l5cXEhIScODAAZw8eRKZmZmIiIiQ6nPr1q149uwZunbtKm0rOztbJRBgbGwM4PkcgY3dtWvXAAD9+/dXaWf79+9XaWdyuVwlwF72HqK67efWrVuwtrZWCQyW19eq60Patm2Lp0+fSi+MKSgowOeffy7NKdW8eXOYm5sjNzdX6fp9/fr1l94LVNWECRNw/Phxqdw///wznj17pvQWvfqm7Dl3cHCAlpaWRuZQquq192X09PSwbNky7N27Fy1atEC/fv3w9ddf48GDBzVW5teBc0DVY+U98y1eMl9HaWkpAGDcuHHw9/dXm6eiuVpqQmWj5tU9xoasMueksue3ojfelbefl+3/+vXrGDBgANq3b48VK1agZcuW0NXVxZ49e7By5Urp+1df1eT5f9m2bt++rfKHYGJiItzc3F6pfOqwX6i7fH190bdvX8TFxWH//v1Yvnw5li1bhh07dlRpG0lJSQgNDYWTkxMMDQ1RWlqKwYMHq22TfBum5pX3fS87YuJF6r77mmgPrP+aUVpaCplMhr1796qtJ03Or2RhYYHz588jPj4ee/fuxd69exETE4MJEyaonbCcqufQoUO4f/8+tmzZgi1btqgsj42NxaBBg6TPNjY2GDhwYLnbUwSZ3nnnHbXLb9y4gVatWqF9+/YAns9FNmLEiFc4gvpPcQ3btGkTLC0tVZaX/bGsMnNm1aX2M2PGDMTExCAkJAS9e/eGiYkJZDIZ/Pz8NH5P7efnh9mzZyM2Nhbz58/H5s2b0b17d6VAe3334rW4Otflmi7Dy/YbEhICT09P7Ny5E/Hx8Vi4cCGWLFmCQ4cOoWvXrhotZ01hAKoRMjc3h5GREUpKSiq8CALPo8InT57Es2fPlB7HelFVhmDa2tqitLQU165dk6K/wPPJj3Nzc6W3c1H11IXz++uvv6KoqAi7du1S+rVe3RDohqamz7+lpaXK8G9HR8caKWtZ7BfqNisrKwQFBSEoKAjp6eno1q0bvvzyS0RGRgKANPm7Ojk5OTh48CDCw8OVXhag+NWYNE9de1CMnszNzVVKr+ovopVha2uLCxcuoLS0VOmX2L/++ktaTpVnbm4OfX19tW3oypUr0v8dHBwghIC9vb00kqki9+7dw5MnT5RGQV29ehUApMllq1KXurq68PT0hKenJ0pLSxEUFIS1a9di4cKFKiNOFerrYzW1JTY2FhYWFtKbxF60Y8cOxMXFYc2aNZUK6qalpSEpKQnTp0+Hq6ur0rLS0lKMHz8eP/zwAz777DP06dMHTZs2xY8//oj58+c36onIHRwcADwPGr3s/qUqqtN+bG1tceDAAeTn5yuNgiqvr1XXh1y9ehVvvPGGNEpr27Zt8Pf3V3q7aWFhocq1w8HBARcvXqzycVbU5ps1a4ahQ4ciNjYWH3zwAY4fPy7dd9RX165dU/pxNzU1FaWlpbCzs6vx63Jl++uq7tfBwQFz587F3Llzce3aNTg5OSEiIgKbN2+uVjlfNz6C1whpa2vD29sb27dvV9tRKYZ8As+fO8/MzMR3332nkk/x66ri7QtlG406Q4YMAQCVzmvFihUAgKFDh1bqGEi9unB+FTdBL/76/ujRI8TExGh837Wtps+/XC7HwIEDlf4pLlI1jf1C3VRSUqLyiJyFhQWsra1RVFSEbt26wd7eHpGRkSrnWlEX6tokoHq+SXMUAYUX68jY2BjNmzdXeeX26tWra3z/Q4YMwYMHD7B161Yprbi4GKtWrYKhoaHKH7tUMW1tbbi7u2Pnzp34+++/pfTLly8jPj5e+jxq1Choa2sjPDxcpf0JIZCVlaWUVlxcjLVr10qf//nnH6xduxbm5ubSPDGVrcuy29bS0pJGsRYVFZV7bAYGBvX+UfnXpaCgADt27MCwYcPg4+Oj8m/69OnIz8/Hrl27KrU9xeinjz/+WGVbvr6+cHV1lfK88cYb+OSTT3D58mV88sknakc8bt68GcnJyTV3wHWUu7s7jI2N8dVXX+HZs2cqy1+8f6ms6rafIUOGoKSkROX+aOXKlZDJZPDw8FBKP3HihNI8Trdv38Yvv/yCQYMGSddubW1tlfpdtWqVyugYb29v/PHHHypvDgYqHhGr7vr0ovHjxyMlJQWhoaHQ1taGn59fuduqD8oGi1etWgUA8PDwqPHrcmX7a1tbW2hra790v0+fPkVhYaFSmoODA4yMjCr8XtY1HAHVSC1duhSJiYlwcXHBhx9+iA4dOiA7Oxtnz57FgQMHkJ2dDeD5s78bN27EnDlzkJycjL59++LJkyc4cOAAgoKC4OXlBX19fXTo0AFbt25F27Zt0axZM3Tq1Entc8iOjo7w9/dHdHQ0cnNz4erqiuTkZGzYsAEjRoyQJnCk6qkL53fQoEHSr0ZTp07F48eP8X//93+wsLDA/fv3Nb7/2lQXzv+rYL9Q9+Tn58PGxgY+Pj5wdHSEoaEhDhw4gFOnTknzhkRFRcHT0xNOTk6YOHEirKys8Ndff+HSpUuIj4+HsbGxNE/As2fP8Oabb2L//v1IS0ur7cNrNBTBgwULFsDPzw86Ojrw9PTE5MmTsXTpUkyePBndu3fH0aNHpREvNWnKlClYu3YtAgICcObMGdjZ2WHbtm3Sr9mcyLjqwsPDsW/fPvTt2xdBQUHSHxUdO3bEhQsXADz/w2Dx4sX49NNPcfPmTYwYMQJGRkZIS0tDXFwcpkyZgnnz5knbtLa2xrJly3Dz5k20bdsWW7duxfnz5xEdHS2NNq1sXU6ePBnZ2dno378/bGxscOvWLaxatQpOTk5KI03LcnZ2xtatWzFnzhz06NEDhoaG8PT01OCZrL927dqF/Px8DB8+XO3yXr16wdzcHLGxsXj//fdfur3Y2Fg4OTmpzLWpMHz4cMyYMQNnz55Ft27dEBoaikuXLiEiIgKJiYnw8fGBpaUlHjx4gJ07dyI5ORlJSUmvdIz1gbGxMaKiojB+/Hh069YNfn5+MDc3x99//43du3fjnXfeUfuDWUWq2348PT3x7rvvYsGCBbh58yYcHR2xf/9+/PLLLwgJCZFGayl06tQJ7u7umDlzJvT09KSAQ3h4uJRn2LBh2LRpE0xMTNChQwecOHECBw4cgJmZmdK2QkNDsW3bNowePRqTJk2Cs7MzsrOzsWvXLqxZs6bcEfSK69PMmTPh7u6uEmQaOnQozMzM8PPPP8PDwwMWFhZVOpd1TVpaGoYPH47BgwfjxIkT2Lx5M8aOHSudn5q8Lle2vzYxMcHo0aOxatUqyGQyODg44LffflOZv+zq1asYMGAAfH190aFDBzRp0gRxcXF4+PBh/QoMvp6X7dGrKvuqR1tbWzF06FCVfGVf/1je69aFEOLhw4ciODhYtGzZUujo6AhLS0sxYMAAER0drZTv6dOnYsGCBcLe3l7K5+PjI65fvy7lSUpKEs7OzkJXV1fpNZVlX7cuhBDPnj0T4eHh0vZatmwpPv30U1FYWKiUr7LH2FgozmVGRoZSetnvxqueX8WrQH/++We1+zl16tRLy7Vr1y7RpUsXIZfLhZ2dnVi2bJlYt27dS19XWpe9rvNf2XOi7vWv7BdcVdLru6KiIhEaGiocHR2FkZGRMDAwEI6OjmL16tVK+Y4dOybee+89KU+XLl3EqlWrpOV37twRI0eOFKampsLExESMHj1a3Lt3T+W1wuV9z+nVffHFF+LNN98UWlpaUtt9+vSpCAwMFCYmJsLIyEj4+vqK9PT0SteLv7+/MDAwUNmXq6ur6Nixo1Law4cPxcSJE0Xz5s2Frq6u6Ny5s8qrpivqG0jVkSNHpD6uVatWYs2aNWr7t+3bt4s+ffoIAwMDYWBgINq3by+Cg4PFlStXpDyKOjt9+rTo3bu3kMvlwtbWVnz33Xcq+61MXW7btk0MGjRIWFhYCF1dXfHWW2+JqVOnivv370t51L36+/Hjx2Ls2LHC1NRUABC2trY1cq4aIk9PTyGXy8WTJ0/KzRMQECB0dHREZmamACCCg4PV5jtz5owAIBYuXFjutm7evCkAiNmzZyulK+q6WbNmokmTJsLKykq8//774vDhw9U7sDpO3f2PEM+/z+7u7sLExETI5XLh4OAgAgICxOnTp6U85fWZZdttZdpPefLz88Xs2bOFtbW10NHREW3atBHLly8XpaWlSvkU34fNmzeLNm3aCD09PdG1a1el9iiEEDk5OVJ7NzQ0FO7u7uKvv/4Stra2wt/fXylvVlaWmD59unjzzTeFrq6usLGxEf7+/iIzM1MI8f/7+Bf7i+LiYjFjxgxhbm4uZDKZSv8lhBBBQUECgPjhhx9eevx1laKOU1JShI+PjzAyMhJNmzYV06dPFwUFBVK+yl6X1X0P1d2LVqa/FkKIjIwM4e3tLd544w3RtGlTMXXqVHHx4kWl+srMzBTBwcGiffv2wsDAQJiYmAgXFxfx008/1eCZ0jyZEA101lYiIiIionrAzc0NmZmZ1ZrDhYjqH5lMhuDg4CqPzqoNs2fPxvfff48HDx5IUywQVRfngCIiIiIiIiIiJYWFhdi8eTO8vb0ZfKIawTmgiIiIiIiIiAgAkJ6ejgMHDmDbtm3IysrCrFmzartI1EAwAEVEREREREREAICUlBR88MEHsLCwwH/+8x84OTnVdpGogeAcUEREREREREREpFGcA4qIiIiIiIiIiDSKASgiIiIiIiIiItIoBqCIiIiIiIiIiEijGIAiIiIiIiIiIiKNYgCKiIiIiIiIiIg0igEoIiIiIiIiIiLSKAagiIiIiIiIiIhIoxiAIiIiIiIiIiIijWIAioiIiIiIiIiINIoBKCIiIiIiIiIi0igGoIiIiIiIiIiISKMYgCIiIiIiIiIiIo1iAIqIiIiIiIiIiDSKASgiIiKiOiQsLAwymQyHDx+u7aIQERER1RgGoIiIiIhegzNnziAwMBBt2rSBgYEB9PX14eDggPHjxyMhIaG2i0dERESkUQxAEREREWlQaWkp5syZg+7du2Pjxo1o1aoVpk2bhlmzZsHZ2Rm7d+/GoEGD8MUXX9R2UYmIiIg0pkltF4CIiIioIfvss8+wcuVKODk5Ydu2bXBwcFBaXlBQgO+++w5ZWVm1VEIiIiIizeMIKCIiIiINSU1Nxddffw0zMzPs27dPJfgEAPr6+ggNDUV4eHiF21q3bh28vLxgZ2cHuVyOZs2awd3dHYmJiWrzb9++Ha6urrCwsIBcLoe1tTUGDhyI7du3K+VLTEyEh4cHrK2toaenhxYtWqBv376Ijo6u/oETERERlcERUEREREQasn79epSUlGDq1Klo0aJFhXn19PQqXB4cHAxHR0cMHDgQ5ubmuHv3Lnbu3ImBAwdix44d8PLykvJGRUUhKCgIVlZWGDlyJMzMzPDgwQMkJycjLi4O3t7eAIDdu3fD09MTpqam8PLygpWVFTIyMvDHH39g06ZNmDJlyqufBCIiIiIwAEVERESkMcePHwcA9O/f/5W3lZKSAnt7e6W0+/fvo3v37ggNDVUKQP33v/+Frq4uzp8/DwsLC6V1XnzUb926dRBCIDExEY6OjuXmIyIiInpVfASPiIiISEMePHgAALCxsXnlbZUNPgGAlZUVvL29ce3aNdy6dUtpmY6ODnR0dFTWMTMzU0nT19evVD4iIiKi6mIAioiIiKgeuHHjBj788EM4ODhALpdDJpNBJpNh1apVAIB79+5Jef38/PDkyRN06tQJoaGh2LNnD/Ly8lS26efnBwDo1asXpk+fjri4OGRmZr6eAyIiIqJGhQEoIiIiIg2xtLQEANy9e/eVtpOamoru3bsjJiYGrVq1wrRp07Bw4UIsWrQIrq6uAICioiIp/7x58/D999/D2toaERERGDp0KMzMzDBixAikpaVJ+UaPHo2dO3eic+fOWLNmDUaNGgULCwsMGDAA58+ff6UyExEREb2IASgiIiIiDXnnnXcAAAcPHnyl7axcuRI5OTlYv349EhISEBkZiX//+98ICwtD+/btVfLLZDJMmjQJp06dQkZGBuLi4jBq1Cj88ssvGDZsGEpKSqS8Xl5eOHLkCHJycrB3715MnjwZhw8fxuDBg5Gbm/tK5SYiIiJSYACKiIiISEMCAgKgra2N6OhoZGRkVJj3xRFMZV2/fh0AlCYaBwAhhDTReXkUI5+2bt2K/v37IyUlBampqSr5jIyMMHjwYERHRyMgIAAPHz7EyZMnK9w2ERERUWUxAEVERESkIa1bt8bHH3+MzMxMeHh4KD3+plBYWIgVK1YgLCys3O3Y2toCAI4dO6aUvnTpUly8eFEl/+HDhyGEUEp79uwZsrOzAQByuRwAcPToUaXRUArp6elK+YiIiIheVZPaLgARERFRQ7Z48WIUFhZi5cqVaNeuHfr3749OnTpBR0cHaWlpOHDgALKysrB48eJytzFt2jTExMTA29sbvr6+MDMzw++//46zZ89i6NCh2L17t1L+ESNGwNjYGL169YKtrS2ePXuGhIQEpKSkwMfHRwpozZw5E/fu3UOfPn1gZ2cHmUyGY8eOITk5Gb169UKfPn00em6IiIio8WAAioiIiEiDtLS0sGLFCowdOxZRUVE4evQojh49itLSUlhZWcHd3R0TJ07EwIEDy91G165dsX//fnz22WfYsWMHtLW18a9//QvHjx/Hrl27VAJQS5Yswb59+5CcnIxff/0VBgYGcHBwQFRUFAIDA6V8n376KXbs2IEzZ84gPj4eOjo6sLOzw7JlyxAUFARtbW2NnRciIiJqXGSi7PhsIiIiIiIiIiKiGsQ5oIiIiIiIiIiISKMYgCIiIiIiIiIiIo1iAIqIiIiIiIiIiDSKASgiIiIiIiIiItIoBqCIiIiIiIiIiEijGIAiIiIiIiIiIiKNYgCKiIiIiIiIiIg0igEoIiIiIiIiIiLSKAagiIiIiIiIiIhIoxiAIiIiIiIiIiIijWIAioiIiIiIiIiINIoBKCIiIiIiIiIi0igGoIiIiIiIiIiISKP+H4/HDzeu6HqlAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# 各クラスごとに対応のあるt検定を行う\n",
        "results = []\n",
        "for class_name in class_names:\n",
        "    slit_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "    sumaho_data = df_sumaho[df_sumaho['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "    # 対応のあるデータを取るため、最小の長さに合わせる\n",
        "    min_length = min(len(slit_data), len(sumaho_data))\n",
        "    slit_data = slit_data[:min_length]\n",
        "    sumaho_data = sumaho_data[:min_length]\n",
        "\n",
        "    t_stat, p_value = stats.ttest_rel(slit_data, sumaho_data)\n",
        "\n",
        "    # スリットランプデータの統計値\n",
        "    slit_mean = slit_data.mean()\n",
        "    slit_std = slit_data.std()\n",
        "\n",
        "    # スマートフォンデータの統計値\n",
        "    sumaho_mean = sumaho_data.mean()\n",
        "    sumaho_std = sumaho_data.std()\n",
        "\n",
        "    results.append({\n",
        "        'class_name': class_name,\n",
        "        't_stat': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'slit_mean': slit_mean,\n",
        "        'slit_std': slit_std,\n",
        "        'sumaho_mean': sumaho_mean,\n",
        "        'sumaho_std': sumaho_std\n",
        "    })\n",
        "\n",
        "# 全クラスまとめた対応のあるt検定\n",
        "all_slit_data = df_slit[layer_name].dropna().reset_index(drop=True)\n",
        "all_sumaho_data = df_sumaho[layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "# 対応のあるデータを取るため、最小の長さに合わせる\n",
        "min_length_all = min(len(all_slit_data), len(all_sumaho_data))\n",
        "all_slit_data = all_slit_data[:min_length_all]\n",
        "all_sumaho_data = all_sumaho_data[:min_length_all]\n",
        "\n",
        "t_stat_all, p_value_all = stats.ttest_rel(all_slit_data, all_sumaho_data)\n",
        "\n",
        "# 全クラスまとめた統計値\n",
        "all_slit_mean = all_slit_data.mean()\n",
        "all_slit_std = all_slit_data.std()\n",
        "all_sumaho_mean = all_sumaho_data.mean()\n",
        "all_sumaho_std = all_sumaho_data.std()\n",
        "\n",
        "# 結果を追加\n",
        "results.append({\n",
        "    'class_name': 'All Classes',\n",
        "    't_stat': t_stat_all,\n",
        "    'p_value': p_value_all,\n",
        "    'slit_mean': all_slit_mean,\n",
        "    'slit_std': all_slit_std,\n",
        "    'sumaho_mean': all_sumaho_mean,\n",
        "    'sumaho_std': all_sumaho_std\n",
        "})\n",
        "\n",
        "# 結果をデータフレームに変換して表示\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results['p_value'] = df_results['p_value'].map(lambda x: f'{x:.3f}')\n",
        "\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "fgFUt3AFWwvA",
        "outputId": "9bff106f-2bc6-41df-9ec0-8b355c5e7c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      class_name    t_stat p_value  slit_mean  slit_std  sumaho_mean  \\\n",
              "0      infection -2.010170   0.054   0.047514  0.024726     0.063889   \n",
              "1         normal -2.237214   0.035   0.024780  0.011687     0.037293   \n",
              "2  non-infection -0.991773   0.341   0.116942  0.075453     0.165322   \n",
              "3           scar -3.849386   0.001   0.073900  0.030724     0.135584   \n",
              "4          tumor -3.143958   0.004   0.112467  0.070307     0.165419   \n",
              "5        deposit -4.416907   0.000   0.036668  0.025242     0.084672   \n",
              "6           APAC -0.926550   0.397   0.057955  0.041766     0.079910   \n",
              "7   lens opacity -2.979324   0.008   0.072591  0.040474     0.131192   \n",
              "8        bullous -1.443593   0.164   0.066964  0.024595     0.082440   \n",
              "9    All Classes -6.956735   0.000   0.064128  0.048114     0.104198   \n",
              "\n",
              "   sumaho_std  \n",
              "0    0.037739  \n",
              "1    0.024043  \n",
              "2    0.135195  \n",
              "3    0.084415  \n",
              "4    0.065204  \n",
              "5    0.058418  \n",
              "6    0.030570  \n",
              "7    0.067411  \n",
              "8    0.040750  \n",
              "9    0.079396  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d107d54a-6238-41c4-883a-266dde19bdcb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class_name</th>\n",
              "      <th>t_stat</th>\n",
              "      <th>p_value</th>\n",
              "      <th>slit_mean</th>\n",
              "      <th>slit_std</th>\n",
              "      <th>sumaho_mean</th>\n",
              "      <th>sumaho_std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>infection</td>\n",
              "      <td>-2.010170</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.047514</td>\n",
              "      <td>0.024726</td>\n",
              "      <td>0.063889</td>\n",
              "      <td>0.037739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>normal</td>\n",
              "      <td>-2.237214</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.024780</td>\n",
              "      <td>0.011687</td>\n",
              "      <td>0.037293</td>\n",
              "      <td>0.024043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>non-infection</td>\n",
              "      <td>-0.991773</td>\n",
              "      <td>0.341</td>\n",
              "      <td>0.116942</td>\n",
              "      <td>0.075453</td>\n",
              "      <td>0.165322</td>\n",
              "      <td>0.135195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>scar</td>\n",
              "      <td>-3.849386</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.073900</td>\n",
              "      <td>0.030724</td>\n",
              "      <td>0.135584</td>\n",
              "      <td>0.084415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tumor</td>\n",
              "      <td>-3.143958</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.112467</td>\n",
              "      <td>0.070307</td>\n",
              "      <td>0.165419</td>\n",
              "      <td>0.065204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>deposit</td>\n",
              "      <td>-4.416907</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.036668</td>\n",
              "      <td>0.025242</td>\n",
              "      <td>0.084672</td>\n",
              "      <td>0.058418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>APAC</td>\n",
              "      <td>-0.926550</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.057955</td>\n",
              "      <td>0.041766</td>\n",
              "      <td>0.079910</td>\n",
              "      <td>0.030570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>lens opacity</td>\n",
              "      <td>-2.979324</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.072591</td>\n",
              "      <td>0.040474</td>\n",
              "      <td>0.131192</td>\n",
              "      <td>0.067411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bullous</td>\n",
              "      <td>-1.443593</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.066964</td>\n",
              "      <td>0.024595</td>\n",
              "      <td>0.082440</td>\n",
              "      <td>0.040750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>All Classes</td>\n",
              "      <td>-6.956735</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.064128</td>\n",
              "      <td>0.048114</td>\n",
              "      <td>0.104198</td>\n",
              "      <td>0.079396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d107d54a-6238-41c4-883a-266dde19bdcb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d107d54a-6238-41c4-883a-266dde19bdcb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d107d54a-6238-41c4-883a-266dde19bdcb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-69a985ea-33d1-4af4-993f-1a338f043799\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69a985ea-33d1-4af4-993f-1a338f043799')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-69a985ea-33d1-4af4-993f-1a338f043799 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_09498caa-c169-42f6-81e9-b945731909d5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_09498caa-c169-42f6-81e9-b945731909d5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"class_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"bullous\",\n          \"normal\",\n          \"deposit\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"t_stat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8443032979200138,\n        \"min\": -6.956735284813072,\n        \"max\": -0.9265500391021524,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          -1.4435931481564337,\n          -2.2372139729762908,\n          -4.416906938645566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_value\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"0.008\",\n          \"0.035\",\n          \"0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slit_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02946012295598514,\n        \"min\": 0.024780148111130596,\n        \"max\": 0.1169416670533802,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.06696438969431771,\n          0.024780148111130596,\n          0.036667667861288264\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slit_std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.020590952465214165,\n        \"min\": 0.011687291764271645,\n        \"max\": 0.0754529486802198,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.024594968056196008,\n          0.011687291764271645,\n          0.025241872403126413\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sumaho_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.043124223932540375,\n        \"min\": 0.037293498249458594,\n        \"max\": 0.16541944816259196,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.08243997104141557,\n          0.037293498249458594,\n          0.08467166181747428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sumaho_std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03280018295944441,\n        \"min\": 0.024042996876849975,\n        \"max\": 0.1351946759225835,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.040749667728648895,\n          0.024042996876849975,\n          0.05841844058793129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ANOVAを実行\n",
        "anova_result = stats.f_oneway(\n",
        "    df_combined[df_combined['class_name'] == 'infection'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'normal'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'non-infection'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'scar'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'tumor'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'deposit'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'APAC'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'lens opacity'][layer_name],\n",
        "    df_combined[df_combined['class_name'] == 'bullous'][layer_name]\n",
        ")\n",
        "\n",
        "print(f\"ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(df_combined[layer_name], df_combined['class_name'])\n",
        "\n",
        "# 結果を表示\n",
        "print(tukey_result)"
      ],
      "metadata": {
        "id": "LjAiQjnMYHjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ANOVA_heatmap**"
      ],
      "metadata": {
        "id": "59zc1fuTqBK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'})\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "exil8PT2ZzsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# カスタムカラーマップを作成（オレンジ、グレー、白）\n",
        "colors = ['#FFA500', '#D3D3D3', 'white']  # オレンジ、グレー、白\n",
        "custom_cmap = ListedColormap(colors)\n",
        "\n",
        "# p値に基づいてデータを作成（NaNは2、p<0.05は0、それ以外は1）\n",
        "color_data = np.where(np.isnan(matrix), 2, np.where(matrix < 0.05, 0, 1))\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(color_data,\n",
        "            annot=matrix,  # 元のp値を表示\n",
        "            fmt=\".3f\",\n",
        "            cmap=custom_cmap,\n",
        "            cbar=False,  # カラーバーを非表示に\n",
        "            xticklabels=class_names,  # x軸のラベルをクラス名に設定\n",
        "            yticklabels=class_names)  # y軸のラベルをクラス名に設定\n",
        "\n",
        "# カスタムカラーバーを追加\n",
        "sm = plt.cm.ScalarMappable(cmap=ListedColormap(colors[:2]), norm=plt.Normalize(vmin=0, vmax=1))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, label='p-value', ticks=[0.25, 0.75])\n",
        "cbar.set_ticklabels(['p < 0.05', 'p ≥ 0.05'])\n",
        "\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.xticks(rotation=45, ha='right')  # x軸のラベルを45度回転\n",
        "plt.yticks(rotation=0)  # y軸のラベルを水平に\n",
        "plt.tight_layout()  # レイアウトを調整\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W6M5cbfdFlv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tukey_sumaho/slit別**"
      ],
      "metadata": {
        "id": "XbxW3P2OqFTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "def perform_anova_and_tukey(df, title):\n",
        "    # 欠損値を削除\n",
        "    df = df.dropna(subset=[layer_name])\n",
        "\n",
        "    # ANOVAを実行\n",
        "    anova_result = stats.f_oneway(\n",
        "        df[df['class_name'] == 'infection'][layer_name],\n",
        "        df[df['class_name'] == 'normal'][layer_name],\n",
        "        df[df['class_name'] == 'non-infection'][layer_name],\n",
        "        df[df['class_name'] == 'scar'][layer_name],\n",
        "        df[df['class_name'] == 'tumor'][layer_name],\n",
        "        df[df['class_name'] == 'deposit'][layer_name],\n",
        "        df[df['class_name'] == 'APAC'][layer_name],\n",
        "        df[df['class_name'] == 'lens opacity'][layer_name],\n",
        "        df[df['class_name'] == 'bullous'][layer_name]\n",
        "    )\n",
        "\n",
        "    print(f\"{title} ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "    # 事後検定（TukeyのHSD検定）を実行\n",
        "    tukey_result = pairwise_tukeyhsd(endog=df[layer_name], groups=df['class_name'])\n",
        "\n",
        "    # 結果をDataFrameに変換\n",
        "    tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "    # マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "    matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "    # マトリックスにp値を入力\n",
        "    for i in range(len(tukey_result_df)):\n",
        "        group1 = tukey_result_df.loc[i, 'group1']\n",
        "        group2 = tukey_result_df.loc[i, 'group2']\n",
        "        p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "        matrix.loc[group1, group2] = p_value\n",
        "        matrix.loc[group2, group1] = p_value\n",
        "\n",
        "    # 同じクラス間のセルをNaNに設定\n",
        "    np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "    # ヒートマップを描画\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'}, mask=matrix.isnull())\n",
        "    plt.title(f'Pairwise Comparison P-Values (Tukey HSD) - {title}')\n",
        "    plt.show()\n",
        "\n",
        "# スリットランプデータの解析\n",
        "perform_anova_and_tukey(df_slit, \"Slit Lamp Data\")\n",
        "\n",
        "# スマートフォンデータの解析\n",
        "perform_anova_and_tukey(df_sumaho, \"Smartphone Data\")"
      ],
      "metadata": {
        "id": "8m7Z9FCzbSmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 両方のデータを結合\n",
        "df_combined = pd.concat([df_slit, df_sumaho]).reset_index(drop=True)\n",
        "\n",
        "# \"scar\" + \"non-infection\" グループとその他のクラスに分類\n",
        "df_combined['group'] = df_combined['class_name'].apply(lambda x: 'scar + non-infection' if x in ['scar', 'non-infection'] else 'others')\n",
        "\n",
        "# グループごとにデータを抽出\n",
        "scar_non_infection_group = df_combined[df_combined['group'] == 'scar + non-infection'][layer_name].dropna()\n",
        "others_group = df_combined[df_combined['group'] == 'others'][layer_name].dropna()\n",
        "\n",
        "# t検定を実行\n",
        "t_stat, p_value = stats.ttest_ind(scar_non_infection_group, others_group)\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
        "\n",
        "# 統計値を計算\n",
        "scar_non_infection_mean = scar_non_infection_group.mean()\n",
        "scar_non_infection_std = scar_non_infection_group.std()\n",
        "others_mean = others_group.mean()\n",
        "others_std = others_group.std()\n",
        "\n",
        "print(f\"scar + non-infection mean: {scar_non_infection_mean}, std: {scar_non_infection_std}\")\n",
        "print(f\"others mean: {others_mean}, std: {others_std}\")\n"
      ],
      "metadata": {
        "id": "-ubPn9Erqnid",
        "outputId": "a4e19c97-a3c2-44cd-9dac-99c4b7acdb75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t-statistic: 5.652411183767835, p-value: 2.8095252129652275e-08\n",
            "scar + non-infection mean: 0.11625931718387256, std: 0.08804314854287512\n",
            "others mean: 0.07454914396969002, std: 0.05818428866497734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcxqbJ_GrbPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}