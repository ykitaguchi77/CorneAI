{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSnppNXNPwZCDSDvk/5R3h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5-LIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 LIME CorneAI**"
      ],
      "metadata": {
        "id": "mJcOrPki6NXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "Jd_yXvLX6WEA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "decXjLeF5tvO",
        "outputId": "83bb7448-d7da-4b6f-e5e7-7dde74f05d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62qgDIr46oS9",
        "outputId": "4fa5b06f-d4be-4f6d-ddb3-4c4c488ec165"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 17.31 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/3.jpg\""
      ],
      "metadata": {
        "id": "-MjZmDMc6oaL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LIME**"
      ],
      "metadata": {
        "id": "uuseY-WV67TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime --q\n",
        "!pip install scikit-image --q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgNy8U2a72ss",
        "outputId": "40ad5690-d3d1-475d-e4fa-52cfca9558a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import cv2\n",
        "import time\n",
        "import traceback\n",
        "import torchvision\n",
        "from lime import lime_image\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "from utils.metrics import box_iou\n",
        "%matplotlib inline\n",
        "\n",
        "# 角膜AIのクラス定義\n",
        "CORNEA_CLASSES = [\n",
        "    \"infection\",\n",
        "    \"normal\",\n",
        "    \"non-infection\",\n",
        "    \"scar\",\n",
        "    \"tumor\",\n",
        "    \"deposit\",\n",
        "    \"APAC\",\n",
        "    \"lens opacity\",\n",
        "    \"bullous\"\n",
        "]\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"\n",
        "    GPUが利用可能な場合はGPUを、そうでない場合はCPUを設定\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    device : torch.device\n",
        "        使用するデバイス\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"GPU not available, using CPU\")\n",
        "    return device\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size=(640, 640),\n",
        "                 names=CORNEA_CLASSES,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        # モデルのロード\n",
        "        print(\"[INFO] Loading cornea detection model...\")\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model loaded successfully\")\n",
        "\n",
        "        # モデルのクラス数を取得と確認\n",
        "        self.nc = int(self.model.nc)\n",
        "        print(f\"[INFO] Number of classes: {self.nc}\")\n",
        "\n",
        "        # クラス名の設定と検証\n",
        "        self.names = names\n",
        "        if len(self.names) != self.nc:\n",
        "            print(f\"[WARNING] Number of class names ({len(self.names)}) does not match model classes ({self.nc})\")\n",
        "        print(f\"[INFO] Using class names: {self.names}\")\n",
        "\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        # Cold start prevention\n",
        "        print(\"[INFO] Performing cold start prevention...\")\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "        print(\"[INFO] Initialization complete\")\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096\n",
        "        max_nms = 30000\n",
        "        time_limit = 10.0\n",
        "        redundant = True\n",
        "        multi_label &= nc > 1\n",
        "        merge = False\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, nc), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        try:\n",
        "            prediction, logits, _ = self.model(img, augment=False)\n",
        "            prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                          classes=None,\n",
        "                                                          agnostic=self.agnostic)\n",
        "\n",
        "            batch_size = img.shape[0]\n",
        "            self.boxes = [[] for _ in range(batch_size)]\n",
        "            self.class_names = [[] for _ in range(batch_size)]\n",
        "            self.classes = [[] for _ in range(batch_size)]\n",
        "            self.confidences = [[] for _ in range(batch_size)]\n",
        "\n",
        "            for i, det in enumerate(prediction):\n",
        "                if len(det):\n",
        "                    for *xyxy, conf, cls in det:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(self.img_size[1], xyxy[2])\n",
        "                        xyxy[3] = min(self.img_size[0], xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(float(conf.item()))\n",
        "                        cls_idx = int(cls.item())\n",
        "\n",
        "                        if cls_idx >= len(self.names):\n",
        "                            print(f\"[WARNING] Class index {cls_idx} is out of range\")\n",
        "                            cls_idx = 0\n",
        "\n",
        "                        self.classes[i].append(cls_idx)\n",
        "                        self.class_names[i].append(self.names[cls_idx])\n",
        "\n",
        "            return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in forward pass: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return [[[]], [[]], [[]], [[]]], None\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        try:\n",
        "            if len(img.shape) != 4:\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "            im0 = img.astype(np.uint8)\n",
        "            img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "            img = img.transpose((0, 3, 1, 2))\n",
        "            img = np.ascontiguousarray(img)\n",
        "            img = torch.from_numpy(img).to(self.device)\n",
        "            img = img / 255.0\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preprocessing: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "class YOLOLimeExplainer:\n",
        "    def __init__(self, yolo_model, device='cuda', img_size=(640, 640)):\n",
        "        self.model = yolo_model\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    def predict_fn(self, images):\n",
        "        try:\n",
        "            batch_predictions = []\n",
        "            for img in images:\n",
        "                processed_img = self.model.preprocessing(np.expand_dims(img, 0))\n",
        "                if processed_img is None:\n",
        "                    raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    predictions, _ = self.model(processed_img)\n",
        "\n",
        "                class_scores = np.zeros(len(CORNEA_CLASSES))\n",
        "\n",
        "                if predictions[0][0]:\n",
        "                    for cls_idx, conf in zip(predictions[1][0], predictions[3][0]):\n",
        "                        if cls_idx < len(CORNEA_CLASSES):\n",
        "                            class_scores[cls_idx] = max(class_scores[cls_idx], conf)\n",
        "\n",
        "                batch_predictions.append(class_scores)\n",
        "\n",
        "            return np.array(batch_predictions)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction_fn: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return np.zeros((len(images), len(CORNEA_CLASSES)))\n",
        "\n",
        "    def explain_instance(self, image, num_samples=1000, top_labels=5):\n",
        "        try:\n",
        "            if isinstance(image, Image.Image):\n",
        "                image = np.array(image)\n",
        "            if len(image.shape) == 2:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "            explanation = self.explainer.explain_instance(\n",
        "                image,\n",
        "                self.predict_fn,\n",
        "                labels=range(len(CORNEA_CLASSES)),\n",
        "                top_labels=top_labels,\n",
        "                hide_color=0,\n",
        "                num_samples=num_samples\n",
        "            )\n",
        "            return explanation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in explain_instance: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "def visualize_results(explanation, image, class_names=CORNEA_CLASSES, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize LIME explanation results overlaid on the original image\n",
        "\n",
        "    Args:\n",
        "        explanation: LIME explanation object\n",
        "        image: Original image array\n",
        "        class_names: List of class names\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get predictions\n",
        "        prediction = explanation.predict_fn(np.array([image]))[0]\n",
        "\n",
        "        # Sort labels by prediction confidence\n",
        "        sorted_labels = sorted(range(len(prediction)),\n",
        "                             key=lambda x: prediction[x],\n",
        "                             reverse=True)[:2]\n",
        "\n",
        "        # Create a figure with subplots\n",
        "        plt.figure(figsize=(15, 7))\n",
        "\n",
        "        for idx, label in enumerate(sorted_labels):\n",
        "            plt.subplot(1, len(sorted_labels), idx + 1)\n",
        "\n",
        "            # Get the original image and mask from LIME\n",
        "            mask = explanation.local_exp[label]\n",
        "\n",
        "            # Convert the sparse mask to a dense array\n",
        "            dense_mask = np.zeros(explanation.segments.shape, dtype=float)\n",
        "            for i, v in mask:\n",
        "                dense_mask[explanation.segments == i] = v\n",
        "\n",
        "            # Normalize the mask to [0, 1] range\n",
        "            if dense_mask.max() != dense_mask.min():\n",
        "                dense_mask = (dense_mask - dense_mask.min()) / (dense_mask.max() - dense_mask.min())\n",
        "\n",
        "            # Create a colormap (red for positive contributions)\n",
        "            heatmap = np.zeros((dense_mask.shape[0], dense_mask.shape[1], 4))\n",
        "            heatmap[:, :, 2] = dense_mask  # Blue channel\n",
        "            heatmap[:, :, 3] = dense_mask * 1.0  # Alpha channel\n",
        "\n",
        "            # Display original image\n",
        "            plt.imshow(image, alpha=0.8)\n",
        "\n",
        "            # Overlay heatmap\n",
        "            plt.imshow(heatmap, alpha=0.6)\n",
        "\n",
        "            class_name = class_names[label] if label < len(class_names) else f\"Unknown Class {label}\"\n",
        "            plt.title(f'{class_name}\\nConfidence: {prediction[label]:.3f}', fontsize=12)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "            print(f\"Visualization saved to {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nDetailed Confidence Scores:\")\n",
        "        for label in sorted_labels:\n",
        "            print(f\"{class_names[label]}: {prediction[label]:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in visualization: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "class YOLOLimeExplainer:\n",
        "    def __init__(self, yolo_model, device='cuda', img_size=(640, 640)):\n",
        "        self.model = yolo_model\n",
        "        self.device = device\n",
        "        self.img_size = img_size\n",
        "        self.explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "    def predict_fn(self, images):\n",
        "        \"\"\"\n",
        "        Modified prediction function with improved image processing\n",
        "        \"\"\"\n",
        "        try:\n",
        "            batch_predictions = []\n",
        "            for img in images:\n",
        "                # Normalize image if needed\n",
        "                if img.max() > 1.0:\n",
        "                    img = img / 255.0\n",
        "\n",
        "                # Convert image format if needed\n",
        "                if len(img.shape) == 2:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "                elif img.shape[2] == 4:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "                # Ensure image is uint8 for preprocessing\n",
        "                img_uint8 = (img * 255).astype(np.uint8)\n",
        "                processed_img = self.model.preprocessing(np.expand_dims(img_uint8, 0))\n",
        "\n",
        "                if processed_img is None:\n",
        "                    raise ValueError(\"Failed to preprocess image\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    predictions, logits = self.model(processed_img)\n",
        "\n",
        "                class_scores = np.zeros(len(CORNEA_CLASSES))\n",
        "\n",
        "                if predictions[0][0]:  # If there are any detections\n",
        "                    if logits is not None and len(logits[0]) > 0:\n",
        "                        # Use logits if available\n",
        "                        probs = torch.nn.functional.softmax(logits[0], dim=1)\n",
        "                        class_scores = probs.mean(dim=0).cpu().numpy()\n",
        "                    else:\n",
        "                        # Fallback to confidence scores\n",
        "                        for cls_idx, conf in zip(predictions[1][0], predictions[3][0]):\n",
        "                            if cls_idx < len(CORNEA_CLASSES):\n",
        "                                class_scores[cls_idx] = max(class_scores[cls_idx], conf)\n",
        "\n",
        "                batch_predictions.append(class_scores)\n",
        "\n",
        "            return np.array(batch_predictions)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction_fn: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return np.zeros((len(images), len(CORNEA_CLASSES)))\n",
        "\n",
        "    def explain_instance(self, image, num_samples=1000, top_labels=5):\n",
        "        \"\"\"\n",
        "        Generate LIME explanation with improved image handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert PIL Image to numpy array if needed\n",
        "            if isinstance(image, Image.Image):\n",
        "                image = np.array(image)\n",
        "\n",
        "            # Normalize image if needed\n",
        "            if image.max() > 1.0:\n",
        "                image = image.astype(float) / 255.0\n",
        "\n",
        "            # Convert image format if needed\n",
        "            if len(image.shape) == 2:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "            # Generate explanation\n",
        "            explanation = self.explainer.explain_instance(\n",
        "                image,\n",
        "                self.predict_fn,\n",
        "                labels=range(len(CORNEA_CLASSES)),\n",
        "                top_labels=top_labels,\n",
        "                hide_color=0,\n",
        "                num_samples=num_samples\n",
        "            )\n",
        "\n",
        "            # Store predict_fn for later use\n",
        "            explanation.predict_fn = self.predict_fn\n",
        "\n",
        "            return explanation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in explain_instance: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "def run_lime_analysis(model_path, img_path, num_samples=500, save_path=None):\n",
        "    \"\"\"\n",
        "    Run LIME analysis with improved visualization\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the YOLO model weights\n",
        "        img_path: Path to the input image\n",
        "        num_samples: Number of samples for LIME analysis\n",
        "        save_path: Optional path to save the visualization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        print(\"Loading YOLO model...\")\n",
        "        yolo_model = YOLOV5TorchObjectDetector(\n",
        "            model_weight=model_path,\n",
        "            device=device,\n",
        "            img_size=(640, 640),\n",
        "            names=CORNEA_CLASSES\n",
        "        )\n",
        "\n",
        "        print(\"Initializing LIME explainer...\")\n",
        "        lime_explainer = YOLOLimeExplainer(yolo_model)\n",
        "\n",
        "        print(\"Loading and processing image...\")\n",
        "        image = Image.open(img_path)\n",
        "        image_array = np.array(image)\n",
        "\n",
        "        # Normalize image if needed\n",
        "        if image_array.max() > 1.0:\n",
        "            image_array = image_array.astype(float) / 255.0\n",
        "\n",
        "        print(f\"Running LIME analysis with {num_samples} samples...\")\n",
        "        explanation = lime_explainer.explain_instance(\n",
        "            image_array,\n",
        "            num_samples=num_samples,\n",
        "            top_labels=3\n",
        "        )\n",
        "\n",
        "        if explanation is None:\n",
        "            print(\"Failed to generate explanation\")\n",
        "            return None\n",
        "\n",
        "        print(\"Visualizing results...\")\n",
        "        visualize_results(explanation, image_array, save_path=save_path)\n",
        "\n",
        "        return explanation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in run_lime_analysis: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Usage example:\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/1.jpg\"\n",
        "    save_path = \"lime_explanation.png\"  # Optional\n",
        "\n",
        "    explanation = run_lime_analysis(\n",
        "        model_path=model_path,\n",
        "        img_path=img_path,\n",
        "        num_samples=500,\n",
        "        save_path=save_path\n",
        "    )"
      ],
      "metadata": {
        "id": "4ZNSmW3HHhtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fqwpw3L8E4YX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}