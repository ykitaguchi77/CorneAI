{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvkQeOgP4S1IlkJosTjAoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_inference_diverse_iris_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 inference_diverse_iris_model**"
      ],
      "metadata": {
        "id": "Un512TpLoNtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F7fjEQUV-pd",
        "outputId": "58f3d575-b348-473c-9afe-2c1c24434114"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYRY9egwjuIs",
        "outputId": "44fa2a9e-732c-467c-bac5-094209857527"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "opencv-python-headless>=4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "# torch>=1.7.0\n",
        "# torchvision>=0.8.1\n",
        "tqdm>=4.41.0\n",
        "\n",
        "# Logging -------------------------------------\n",
        "##tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "##seaborn>=0.11.0\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUeMX07NqirS",
        "outputId": "69a28fc0-dfe2-4c6e-a343-b2fdc4b9ef28"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Tfb6NYZIBGm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4c92c8-731c-4e50-8053-1b252afc77d7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python-headless>=4.6.0.66 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.9.0.80)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (4.66.1)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop->-r requirements.txt (line 37)) (2.1.0+cu121)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop->-r requirements.txt (line 37)) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop->-r requirements.txt (line 37)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#最新バージョンでも動くので削除\n",
        "# !pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install torchvision==0.11.2+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "ESI_x2upsdf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "weight = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "num = 5\n",
        "img_dir = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data\"\n",
        "img = glob.glob(f\"{img_dir}/*\")[num]\n",
        "img\n",
        "\n",
        "img = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg\""
      ],
      "metadata": {
        "id": "Bnz_lfT_l7NM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py  --weights $weight  --source $img"
      ],
      "metadata": {
        "id": "kPsBV4Itjrn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要な部分だけ抜粋"
      ],
      "metadata": {
        "id": "5BD1oZ3nFO6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference using Cornea journal dataset**"
      ],
      "metadata": {
        "id": "CGUbtkDXypdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/corneaのスマホ判定_滝先生.xlsx\"\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/3.CorneAIを混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\""
      ],
      "metadata": {
        "id": "dGKLXhUeG5ha"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file starting from row 13 (which is indexed as 12 in Python) for headers\n",
        "df = pd.read_excel(excel_path, header=12)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bjsM7cUC3jFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame and it's already loaded\n",
        "# df = pd.read_csv('your_dataframe.csv')  # Replace with your DataFrame loading method\n",
        "\n",
        "# Directory path\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# Define the columns for the new DataFrame\n",
        "columns = [\"image_num\", \"class\"]\n",
        "\n",
        "# Create an empty DataFrame with the specified columns\n",
        "cornea_journal_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Iterating over each file in the directory\n",
        "for filename in os.listdir(images_dir):\n",
        "    if filename.endswith(\".png\"):\n",
        "        # Extracting the numeric part (image number) from the filename\n",
        "        image_num = int(filename.split('.')[0])\n",
        "\n",
        "        # Match with the DataFrame and extract 'クラス' value\n",
        "        matched_row = df[df['Number'] == image_num]\n",
        "        if not matched_row.empty:\n",
        "            class_value = matched_row['クラス'].iloc[0]\n",
        "            # Creating a new row as a DataFrame\n",
        "            row_data = pd.DataFrame({\"image_num\": [image_num], \"class\": [class_value], \"class_name\": [class_names[class_value]]})\n",
        "            # Concatenating the new row DataFrame with the main DataFrame\n",
        "            cornea_journal_df = pd.concat([cornea_journal_df, row_data], ignore_index=True)\n",
        "\n",
        "# Sort the DataFrame by the image_num column\n",
        "cornea_journal_df.sort_values(by='image_num', inplace=True)\n",
        "\n",
        "# Displaying the sorted DataFrame\n",
        "print(cornea_journal_df)\n"
      ],
      "metadata": {
        "id": "_EPP9cvRAMD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像のクラスを確認\n",
        "# Getting the distribution of the 'class' column and sorting by class values\n",
        "class_distribution = cornea_journal_df['class'].value_counts().sort_index()\n",
        "\n",
        "# Displaying the distribution\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOLIBDKDAxPa",
        "outputId": "1ac4960d-8822-41e5-a34a-9396f9bf21aa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    46\n",
            "1     7\n",
            "2     9\n",
            "3    31\n",
            "4    33\n",
            "5    32\n",
            "8     4\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "# def letterbox_image(image, size=(640, 480)):\n",
        "#     ih, iw = image.shape[:2]\n",
        "#     w, h = size\n",
        "\n",
        "#     # Calculate padding to maintain aspect ratio\n",
        "#     scale = min(w / iw, h / ih)\n",
        "#     nw, nh = int(scale * iw), int(scale * ih)\n",
        "#     image = cv2.resize(image, (nw, nh))\n",
        "\n",
        "#     # Calculate padding dimensions\n",
        "#     top = (h - nh) // 2\n",
        "#     bottom = h - nh - top\n",
        "#     left = (w - nw) // 2\n",
        "#     right = w - nw - left\n",
        "\n",
        "#     # Add padding to the image\n",
        "#     return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "BB9E6zqwD0Py"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple inference\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"][0:5]:\n",
        "      img = f\"{images_dir}/{num}.png\"\n",
        "      inference_top3(img, model)"
      ],
      "metadata": {
        "id": "7UmC2m6cE160"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "vJab4D4MH2MF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "Y6SOama5H51r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# New model\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/3.CorneAIを混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\"\n",
        "\"\"\"\n",
        "\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(mixed_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "ZhqjG7deIJkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "C-uVI3bJIOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/baseline_to_mixed.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "PrxCWw7EIQkF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Analyze_results***"
      ],
      "metadata": {
        "id": "CKH8THhJLdyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "#csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/compare_accuracy.csv\"\n",
        "csv_path = dst_path\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "eSFoOT4-LhBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# Create a confusion matrix for top1_baseline\n",
        "cm_baseline = confusion_matrix(df['class_name'], df['top1_baseline'], labels=class_names_order)\n",
        "\n",
        "# Create a confusion matrix for top1_new\n",
        "cm_new = confusion_matrix(df['class_name'], df['top1_new'], labels=class_names_order)\n",
        "\n",
        "\n",
        "# Plot for top1_baseline\n",
        "fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_baseline, annot=True, ax=ax1, fmt='d', cmap='Blues')\n",
        "ax1.set_title('Confusion Matrix for top1_baseline')\n",
        "ax1.set_xlabel('Predicted Labels')\n",
        "ax1.set_ylabel('True Labels')\n",
        "ax1.set_xticklabels(class_names_order, rotation=45)\n",
        "ax1.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot for top1_new\n",
        "fig2, ax2 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_new, annot=True, ax=ax2, fmt='d', cmap='Greens')\n",
        "ax2.set_title('Confusion Matrix for top1_new')\n",
        "ax2.set_xlabel('Predicted Labels')\n",
        "ax2.set_ylabel('True Labels')\n",
        "ax2.set_xticklabels(class_names_order, rotation=45)\n",
        "ax2.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yN7BMwQZLmgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for top1_baseline and top1_new\n",
        "accuracy_baseline = np.trace(cm_baseline) / np.sum(cm_baseline)\n",
        "accuracy_new = np.trace(cm_new) / np.sum(cm_new)\n",
        "\n",
        "accuracy_baseline, accuracy_new\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYZWAdVhM87P",
        "outputId": "1a02d428-e668-4758-f2ef-fc0a0197a673"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MultiModel evaluation flow**"
      ],
      "metadata": {
        "id": "NrWAyrggO9nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation flow #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    # plt.imshow(img_mpl)\n",
        "    # plt.axis('off')  # Turn off axis numbers\n",
        "    # plt.show()\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "        print(\"\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "def evaluation(model_path, title):\n",
        "    model = DetectMultiBackend(model_path, device=\"cpu\", dnn=False)\n",
        "    for num in cornea_journal_df[\"image_num\"]:\n",
        "        img = f\"{images_dir}/{num}.png\"\n",
        "        top3_results = inference_top3(img, model)\n",
        "\n",
        "        # Update the DataFrame with the results\n",
        "        for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "            cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "            cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "evaluation(baseline_weight_path, \"baseline\")\n",
        "evaluation(mixed_weight_path, \"mixed\")\n",
        "evaluation(finetune_100ep_weight_path, \"finetune_100ep\")\n",
        "evaluation(finetune_150ep_weight_path, \"finetune_150ep\")\n",
        "evaluation(finetune_200ep_weight_path, \"finetune_200ep\")\n",
        "evaluation(finetune_250ep_weight_path, \"finetune_250ep\")\n",
        "evaluation(finetune_300ep_weight_path, \"finetune_300ep\")\n",
        "evaluation(finetune_350ep_weight_path, \"finetune_350ep\")\n",
        "evaluation(finetune_400ep_weight_path, \"finetune_400ep\")"
      ],
      "metadata": {
        "id": "tXmOPrcKL526"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/compare_all.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "e2YGhZgdSg1F"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "r8xMxymdUoq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = cornea_journal_df\n",
        "\n",
        "# Create a confusion matrix for top1_baseline\n",
        "cm_baseline = confusion_matrix(df['class_name'], df['top1_baseline'], labels=class_names_order)\n",
        "cm_mixed = confusion_matrix(df['class_name'], df['top1_mixed'], labels=class_names_order)\n",
        "cm_finetune_100ep = confusion_matrix(df['class_name'], df['top1_finetune_100ep'], labels=class_names_order)\n",
        "cm_finetune_150ep = confusion_matrix(df['class_name'], df['top1_finetune_150ep'], labels=class_names_order)\n",
        "cm_finetune_200ep = confusion_matrix(df['class_name'], df['top1_finetune_200ep'], labels=class_names_order)\n",
        "cm_finetune_250ep = confusion_matrix(df['class_name'], df['top1_finetune_250ep'], labels=class_names_order)\n",
        "cm_finetune_300ep = confusion_matrix(df['class_name'], df['top1_finetune_300ep'], labels=class_names_order)\n",
        "cm_finetune_350ep = confusion_matrix(df['class_name'], df['top1_finetune_350ep'], labels=class_names_order)\n",
        "cm_finetune_400ep = confusion_matrix(df['class_name'], df['top1_finetune_400ep'], labels=class_names_order)\n",
        "\n",
        "def draw_confusion_matrix(cm):\n",
        "    # Plot for top1_baseline\n",
        "    fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
        "    sns.heatmap(cm_baseline, annot=True, ax=ax1, fmt='d', cmap='Blues')\n",
        "    ax1.set_title('Confusion Matrix for top1_baseline')\n",
        "    ax1.set_xlabel('Predicted Labels')\n",
        "    ax1.set_ylabel('True Labels')\n",
        "    ax1.set_xticklabels(class_names_order, rotation=45)\n",
        "    ax1.set_yticklabels(class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "draw_confusion_matrix(cm_baseline)\n",
        "draw_confusion_matrix(cm_mixed)\n",
        "draw_confusion_matrix(cm_finetune_100ep)\n",
        "draw_confusion_matrix(cm_finetune_150ep)\n",
        "draw_confusion_matrix(cm_finetune_200ep)\n",
        "draw_confusion_matrix(cm_finetune_250ep)\n",
        "draw_confusion_matrix(cm_finetune_300ep)\n",
        "draw_confusion_matrix(cm_finetune_350ep)\n",
        "draw_confusion_matrix(cm_finetune_400ep)"
      ],
      "metadata": {
        "id": "wh2aLOeHUicu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for top1_baseline and top1_new\n",
        "accuracy_baseline = np.trace(cm_baseline) / np.sum(cm_baseline)\n",
        "accuracy_new = np.trace(cm_new) / np.sum(cm_new)\n",
        "\n",
        "accuracy_baseline, accuracy_new\n",
        "\n"
      ],
      "metadata": {
        "id": "YAACqlJHSVya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference_Maehara_dataset**"
      ],
      "metadata": {
        "id": "bTj36ASWlo3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the zip file and the extraction directory\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "new_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\""
      ],
      "metadata": {
        "id": "aLTTuAJ6lstO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# DataFrame 'df' にExcelファイルを読み込む\n",
        "df = pd.read_excel(excel_path)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4FM6clySpXX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maehara_dfを作成し、1列目にslit_id、2列目にdisease_Englishを含める\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "maehara_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hnDgLBHapiJw",
        "outputId": "88d7a6d5-fad3-489a-fd58-dca2a89ad7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     basename disease_English\n",
              "0    HRS_104L       infection\n",
              "1  21TTR_L_01       infection\n",
              "2     HRS_85R        cataract\n",
              "3     HRS_85L         deposit\n",
              "4    HRS_136L         deposit"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef51e693-faf1-4bf2-9fce-47529813454c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>basename</th>\n",
              "      <th>disease_English</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HRS_104L</td>\n",
              "      <td>infection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21TTR_L_01</td>\n",
              "      <td>infection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HRS_85R</td>\n",
              "      <td>cataract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HRS_85L</td>\n",
              "      <td>deposit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HRS_136L</td>\n",
              "      <td>deposit</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef51e693-faf1-4bf2-9fce-47529813454c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ef51e693-faf1-4bf2-9fce-47529813454c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ef51e693-faf1-4bf2-9fce-47529813454c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0ec573a9-ed8d-471d-9668-fa5c7c6c30f5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ec573a9-ed8d-471d-9668-fa5c7c6c30f5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0ec573a9-ed8d-471d-9668-fa5c7c6c30f5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ファイルの存在確認\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Excelファイルを読み込む\n",
        "excel_data = pd.read_excel(excel_path)\n",
        "\n",
        "# 'basename'列の値を文字列に変換\n",
        "excel_data['basename'] = excel_data['basename'].astype(str)\n",
        "\n",
        "# チェックするディレクトリのパス\n",
        "image_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット'\n",
        "\n",
        "# エラーメッセージのリスト\n",
        "errors = []\n",
        "\n",
        "# 各basenameに対して、.jpgまたは.pngファイルがあるかどうかを確認\n",
        "for basename in excel_data['basename']:\n",
        "    jpg_file = os.path.join(image_dir, basename + '.jpg')\n",
        "    JPG_file = os.path.join(image_dir, basename + '.JPG')\n",
        "    png_file = os.path.join(image_dir, basename + '.png')\n",
        "\n",
        "    if not os.path.exists(jpg_file) and not os.path.exists(png_file) and not os.path.exists(JPG_file):\n",
        "        errors.append(f\"Error: Neither {basename}.jpg nor {basename}.JPG nor {basename}.png exists in the directory.\")\n",
        "\n",
        "# エラーメッセージを表示\n",
        "for error in errors:\n",
        "    print(error)\n",
        "\n",
        "# エラーがあるかどうかを確認\n",
        "if len(errors) > 0:\n",
        "    print(f\"Total missing files: {len(errors)}\")\n",
        "else:\n",
        "    print(\"All files are present.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQmF9ZzyazAe",
        "outputId": "9623009c-0a23-4bce-dbf4-c1e94017b4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Neither nan.jpg nor nan.JPG nor nan.png exists in the directory.\n",
            "Error: Neither nan.jpg nor nan.JPG nor nan.png exists in the directory.\n",
            "Error: Neither nan.jpg nor nan.JPG nor nan.png exists in the directory.\n",
            "Total missing files: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(image, size=(640, 480)):\n",
        "    ih, iw = image.shape[:2]\n",
        "    w, h = size\n",
        "\n",
        "    # Calculate padding to maintain aspect ratio\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw, nh = int(scale * iw), int(scale * ih)\n",
        "    image = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    # Calculate padding dimensions\n",
        "    top = (h - nh) // 2\n",
        "    bottom = h - nh - top\n",
        "    left = (w - nw) // 2\n",
        "    right = w - nw - left\n",
        "\n",
        "    # Add padding to the image\n",
        "    return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2, size=(640, 480))\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "RO0ce7B3rPZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "nyBUtMe3rPbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Assuming baseline_weight_path, images_dir, class_names, and other required variables are defined\n",
        "\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "# Ensure `maehara_df` is a standalone DataFrame\n",
        "maehara_df = maehara_df.copy()\n",
        "\n",
        "# Loop through each row in the DataFrame\n",
        "for index, row in maehara_df.iterrows():\n",
        "    img_id = row['basename']\n",
        "    # Try different file extensions\n",
        "    for ext in ['.jpg', '.png', '.JPG']:\n",
        "        img_path = os.path.join(images_dir, f\"{img_id}{ext}\")\n",
        "        if os.path.exists(img_path):\n",
        "            print(f\"Processing image: {img_path}\")\n",
        "            top3_results = inference_top3(img_path, model)\n",
        "\n",
        "            # Update the DataFrame with the results\n",
        "            for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "                maehara_df.loc[index, f'top{i+1}_baseline'] = class_names[class_num]\n",
        "                maehara_df.loc[index, f'top{i+1}_baseline_likelihood'] = likelihood\n",
        "            break\n",
        "    else:\n",
        "        print(f\"No image found for ID {img_id} with any of the extensions .jpg, .png, .JPG\")\n"
      ],
      "metadata": {
        "id": "YUENkSlvgsag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path to your directory\n",
        "path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(path)\n",
        "\n",
        "# Filter out only PNG and JPG files\n",
        "image_files = [file for file in files if file.endswith('.png') or file.endswith('.jpg')]\n",
        "\n",
        "# Create a DataFrame and save as CSV\n",
        "df = pd.DataFrame(image_files, columns=['File Name'])\n",
        "csv_file = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/image_files_list.csv'\n",
        "df.to_csv(csv_file, index=False)\n",
        "\n",
        "print(f\"CSV file '{csv_file}' created with list of PNG and JPG files.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtMAk6uZrPfP",
        "outputId": "4bdb0ff8-bff6-40c0-cfed-707f35777724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/image_files_list.csv' created with list of PNG and JPG files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqlVIZ1zrPhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5zPrMJLrPjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Augmentation_ideas**"
      ],
      "metadata": {
        "id": "OBSNECHYW6zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "# Define the path to your image\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "image_path = f\"{images_dir}/29.png\"\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "def rgb_to_bgr(img, **kwargs):\n",
        "    \"\"\" Randomly convert image from RGB to BGR with a probability p. \"\"\"\n",
        "    p = kwargs.get('p', 0.5)  # Get the probability value from kwargs, default is 0.5\n",
        "    if np.random.rand() < p:\n",
        "        img = img[..., ::-1]  # Reverse the order of the first and third channel\n",
        "    return img\n",
        "\n",
        "# Define an augmentation pipeline with Albumentations\n",
        "augmentation = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    #A.RandomBrightnessContrast(p=1),\n",
        "    A.ColorJitter(brightness=0.5, hue=0.3, p=0.01),\n",
        "    A.Lambda(image=rgb_to_bgr, p=0.01)  # Custom RGB to BGR conversion with a probability of 0.5\n",
        "])\n",
        "\n",
        "# Apply the augmentation pipeline to the image\n",
        "augmented_img = augmentation(image=img)['image']\n",
        "\n",
        "# Display the augmented image using matplotlib\n",
        "plt.imshow(augmented_img)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GCOz71B2XAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIJREB8qXAXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iq-t00iTXAYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwCaj-QgXAaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}