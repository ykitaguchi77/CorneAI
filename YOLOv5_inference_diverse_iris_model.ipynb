{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN55qePD6wnpNjvxT8v7jpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_inference_diverse_iris_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 inference_diverse_iris_model**"
      ],
      "metadata": {
        "id": "Un512TpLoNtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup"
      ],
      "metadata": {
        "id": "k2ciP17pzhrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F7fjEQUV-pd",
        "outputId": "ebd021f2-845c-46a1-a79b-0d58d2f085b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYRY9egwjuIs",
        "outputId": "4699344a-2a0e-4a36-98a6-f7ba957a2c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "opencv-python-headless>=4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "# torch>=1.7.0\n",
        "# torchvision>=0.8.1\n",
        "tqdm>=4.41.0\n",
        "\n",
        "# Logging -------------------------------------\n",
        "##tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "##seaborn>=0.11.0\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUeMX07NqirS",
        "outputId": "ad4e23c1-92ba-4028-fe4b-f6e8d8a091a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Tfb6NYZIBGm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#最新バージョンでも動くので削除\n",
        "# !pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install torchvision==0.11.2+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "ESI_x2upsdf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "weight = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "num = 5\n",
        "img_dir = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data\"\n",
        "img = glob.glob(f\"{img_dir}/*\")[num]\n",
        "img\n",
        "\n",
        "img = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg\""
      ],
      "metadata": {
        "id": "Bnz_lfT_l7NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python detect.py  --weights $weight  --source $img"
      ],
      "metadata": {
        "id": "kPsBV4Itjrn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要な部分だけ抜粋"
      ],
      "metadata": {
        "id": "5BD1oZ3nFO6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference using Cornea journal dataset**"
      ],
      "metadata": {
        "id": "CGUbtkDXypdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/corneaのスマホ判定_滝先生.xlsx\"\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/3.CorneAIを混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\""
      ],
      "metadata": {
        "id": "dGKLXhUeG5ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file starting from row 13 (which is indexed as 12 in Python) for headers\n",
        "df = pd.read_excel(excel_path, header=12)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bjsM7cUC3jFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame and it's already loaded\n",
        "# df = pd.read_csv('your_dataframe.csv')  # Replace with your DataFrame loading method\n",
        "\n",
        "# Directory path\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# Define the columns for the new DataFrame\n",
        "columns = [\"image_num\", \"class\"]\n",
        "\n",
        "# Create an empty DataFrame with the specified columns\n",
        "cornea_journal_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Iterating over each file in the directory\n",
        "for filename in os.listdir(images_dir):\n",
        "    if filename.endswith(\".png\"):\n",
        "        # Extracting the numeric part (image number) from the filename\n",
        "        image_num = int(filename.split('.')[0])\n",
        "\n",
        "        # Match with the DataFrame and extract 'クラス' value\n",
        "        matched_row = df[df['Number'] == image_num]\n",
        "        if not matched_row.empty:\n",
        "            class_value = matched_row['クラス'].iloc[0]\n",
        "            # Creating a new row as a DataFrame\n",
        "            row_data = pd.DataFrame({\"image_num\": [image_num], \"class\": [class_value], \"class_name\": [class_names[class_value]]})\n",
        "            # Concatenating the new row DataFrame with the main DataFrame\n",
        "            cornea_journal_df = pd.concat([cornea_journal_df, row_data], ignore_index=True)\n",
        "\n",
        "# Sort the DataFrame by the image_num column\n",
        "cornea_journal_df.sort_values(by='image_num', inplace=True)\n",
        "\n",
        "# Displaying the sorted DataFrame\n",
        "print(cornea_journal_df)\n"
      ],
      "metadata": {
        "id": "_EPP9cvRAMD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像のクラスを確認\n",
        "# Getting the distribution of the 'class' column and sorting by class values\n",
        "class_distribution = cornea_journal_df['class'].value_counts().sort_index()\n",
        "\n",
        "# Displaying the distribution\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOLIBDKDAxPa",
        "outputId": "1ac4960d-8822-41e5-a34a-9396f9bf21aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    46\n",
            "1     7\n",
            "2     9\n",
            "3    31\n",
            "4    33\n",
            "5    32\n",
            "8     4\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "# def letterbox_image(image, size=(640, 480)):\n",
        "#     ih, iw = image.shape[:2]\n",
        "#     w, h = size\n",
        "\n",
        "#     # Calculate padding to maintain aspect ratio\n",
        "#     scale = min(w / iw, h / ih)\n",
        "#     nw, nh = int(scale * iw), int(scale * ih)\n",
        "#     image = cv2.resize(image, (nw, nh))\n",
        "\n",
        "#     # Calculate padding dimensions\n",
        "#     top = (h - nh) // 2\n",
        "#     bottom = h - nh - top\n",
        "#     left = (w - nw) // 2\n",
        "#     right = w - nw - left\n",
        "\n",
        "#     # Add padding to the image\n",
        "#     return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "BB9E6zqwD0Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple inference\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"][0:5]:\n",
        "      img = f\"{images_dir}/{num}.png\"\n",
        "      inference_top3(img, model)"
      ],
      "metadata": {
        "id": "7UmC2m6cE160"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "vJab4D4MH2MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "Y6SOama5H51r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# New model\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\"\n",
        "\"\"\"\n",
        "\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(mixed_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "ZhqjG7deIJkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "C-uVI3bJIOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/baseline_to_mixed.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "PrxCWw7EIQkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Analyze_results***"
      ],
      "metadata": {
        "id": "CKH8THhJLdyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "#csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/compare_accuracy.csv\"\n",
        "csv_path = dst_path\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "eSFoOT4-LhBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# Create a confusion matrix for top1_baseline\n",
        "cm_baseline = confusion_matrix(df['class_name'], df['top1_baseline'], labels=class_names_order)\n",
        "\n",
        "# Create a confusion matrix for top1_new\n",
        "cm_new = confusion_matrix(df['class_name'], df['top1_new'], labels=class_names_order)\n",
        "\n",
        "\n",
        "# Plot for top1_baseline\n",
        "fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_baseline, annot=True, ax=ax1, fmt='d', cmap='Blues')\n",
        "ax1.set_title('Confusion Matrix for top1_baseline')\n",
        "ax1.set_xlabel('Predicted Labels')\n",
        "ax1.set_ylabel('True Labels')\n",
        "ax1.set_xticklabels(class_names_order, rotation=45)\n",
        "ax1.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot for top1_new\n",
        "fig2, ax2 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_new, annot=True, ax=ax2, fmt='d', cmap='Greens')\n",
        "ax2.set_title('Confusion Matrix for top1_new')\n",
        "ax2.set_xlabel('Predicted Labels')\n",
        "ax2.set_ylabel('True Labels')\n",
        "ax2.set_xticklabels(class_names_order, rotation=45)\n",
        "ax2.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yN7BMwQZLmgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for top1_baseline and top1_new\n",
        "accuracy_baseline = np.trace(cm_baseline) / np.sum(cm_baseline)\n",
        "accuracy_new = np.trace(cm_new) / np.sum(cm_new)\n",
        "\n",
        "accuracy_baseline, accuracy_new\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYZWAdVhM87P",
        "outputId": "1a02d428-e668-4758-f2ef-fc0a0197a673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MultiModel evaluation pipeline**"
      ],
      "metadata": {
        "id": "NrWAyrggO9nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #######################\n",
        "# # MultiModel evaluation pipeline #\n",
        "# #######################\n",
        "\n",
        "# from models.common import DetectMultiBackend\n",
        "# #from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "# from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "#                            increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "# #from utils.plots import Annotator, colors, save_one_box\n",
        "# #from utils.torch_utils import select_device, time_sync\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# from torchvision import models, transforms\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# device = 'cpu'\n",
        "# #model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# # model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "# class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "#     # Resize and pad image while meeting stride-multiple constraints\n",
        "#     shape = im.shape[:2]  # current shape [height, width]\n",
        "#     if isinstance(new_shape, int):\n",
        "#         new_shape = (new_shape, new_shape)\n",
        "\n",
        "#     # Scale ratio (new / old)\n",
        "#     r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "#     if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "#         r = min(r, 1.0)\n",
        "\n",
        "#     # Compute padding\n",
        "#     # ratio = r, r  # width, height ratios\n",
        "#     new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "#     dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "#     if auto:  # minimum rectangle\n",
        "#         dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "#     elif scaleFill:  # stretch\n",
        "#         dw, dh = 0.0, 0.0\n",
        "#         new_unpad = (new_shape[1], new_shape[0])\n",
        "#         # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "#     dw /= 2  # divide padding into 2 sides\n",
        "#     dh /= 2\n",
        "\n",
        "#     if shape[::-1] != new_unpad:  # resize\n",
        "#         im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "#     top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "#     left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "#     im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "#     return im\n",
        "\n",
        "# def inference_top3(img_path, model):\n",
        "#     img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "#     # Apply letterbox to the image\n",
        "#     img_cv2 = letterbox_image(img_cv2)\n",
        "#     #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "#     # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "#     img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "#     # Get current figure size\n",
        "#     fig_size = plt.gcf().get_size_inches()\n",
        "#     # Set new size (half the current size)\n",
        "#     plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "#     # plt.imshow(img_mpl)\n",
        "#     # plt.axis('off')  # Turn off axis numbers\n",
        "#     # plt.show()\n",
        "\n",
        "#     img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "#     img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "#     img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "#     img_tensor /= 255\n",
        "#     img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "#     pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "#     # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "#     pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "#     # 全てのクラスとlikelihoodのペアを取得\n",
        "#     class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "#     # likelihoodで降順にソート\n",
        "#     class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "#     # 上位3つのクラスとlikelihoodを出力\n",
        "#     print(\"Top 3 Classes and Likelihoods:\")\n",
        "#     for i in range(3):\n",
        "#         likelihood, class_num = class_likelihood_pairs[i]\n",
        "#         print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "#         print(\"\")\n",
        "\n",
        "#     top_classes = []\n",
        "#     for i in range(min(3, len(class_likelihood_pairs))):\n",
        "#         likelihood, class_num = class_likelihood_pairs[i]\n",
        "#         top_classes.append((class_num, likelihood))\n",
        "\n",
        "#     return top_classes\n",
        "\n",
        "# def evaluation(model_path, title):\n",
        "#     model = DetectMultiBackend(model_path, device=\"cpu\", dnn=False)\n",
        "#     for num in cornea_journal_df[\"image_num\"]:\n",
        "#         img = f\"{images_dir}/{num}.png\"\n",
        "#         top3_results = inference_top3(img, model)\n",
        "\n",
        "#         # Update the DataFrame with the results\n",
        "#         for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "#             cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "#             cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "# evaluation(baseline_weight_path, \"baseline\")\n",
        "# evaluation(mixed_weight_path, \"mixed\")\n",
        "# evaluation(finetune_mixed_weight_path, \"finetune_mixed\")\n",
        "# evaluation(finetune_100ep_weight_path, \"finetune_100ep\")\n",
        "# evaluation(finetune_150ep_weight_path, \"finetune_150ep\")\n",
        "# evaluation(finetune_200ep_weight_path, \"finetune_200ep\")\n",
        "# evaluation(finetune_250ep_weight_path, \"finetune_250ep\")\n",
        "# evaluation(finetune_300ep_weight_path, \"finetune_300ep\")\n",
        "# evaluation(finetune_350ep_weight_path, \"finetune_350ep\")\n",
        "# evaluation(finetune_400ep_weight_path, \"finetune_400ep\")"
      ],
      "metadata": {
        "id": "tXmOPrcKL526"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    from utils.general import non_max_suppression\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    top_classes = [(class_num, likelihood) for _, class_num in sorted(class_likelihood_pairs[:3])]\n",
        "    return top_classes\n",
        "\n",
        "# def evaluate_model(model_path, data_df, title, images_dir):\n",
        "#     model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "#     for num in data_df[\"image_num\"]:\n",
        "#         img_path = f\"{images_dir}/{num}.png\"\n",
        "#         img_tensor = preprocess_image(img_path)\n",
        "#         top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "#         for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "#             data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "#             data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "def evaluate_model(model_path, data_df, title, images_dir):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "\n",
        "    # Wrapping the iteration with tqdm for progress tracking\n",
        "    for num in tqdm(data_df[\"image_num\"], desc=f\"Evaluating {title}\"):\n",
        "        img_path = f\"{images_dir}/{num}.png\"\n",
        "        img_tensor = preprocess_image(img_path)\n",
        "        top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "        for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "            data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "            data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "# Example usage\n",
        "weight_paths = [baseline_weight_path, mixed_weight_path, finetune_mixed_weight_path, finetune_100ep_weight_path, finetune_150ep_weight_path, finetune_200ep_weight_path, finetune_250ep_weight_path, finetune_300ep_weight_path, finetune_350ep_weight_path, finetune_400ep_weight_path]\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "for path, title in zip(weight_paths, titles):\n",
        "    evaluate_model(path, cornea_journal_df, title, images_dir)\n"
      ],
      "metadata": {
        "id": "lxMkzMo-jNog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/cornea_journal_all.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "e2YGhZgdSg1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load data\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/cornea_journal_all.csv\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "cornea_journal_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "cornea_journal_df.head()\n"
      ],
      "metadata": {
        "id": "dYatGMIrolDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "r8xMxymdUoq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = cornea_journal_df\n",
        "\n",
        "# Titles for the different model evaluations\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "def draw_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for top1_{title}')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Calculate and display confusion matrices\n",
        "for title in titles:\n",
        "    cm = confusion_matrix(df['class_name'], df[f'top1_{title}'], labels=class_names_order)\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    accuracies[title] = accuracy\n",
        "    draw_confusion_matrix(cm, title)\n",
        "\n",
        "# Print all accuracies at the end\n",
        "for title, accuracy in accuracies.items():\n",
        "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "vdMnTluOeH3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference_Maehara_dataset**"
      ],
      "metadata": {
        "id": "bTj36ASWlo3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the zip file and the extraction directory\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "new_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\""
      ],
      "metadata": {
        "id": "aLTTuAJ6lstO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# DataFrame 'df' にExcelファイルを読み込む\n",
        "df = pd.read_excel(excel_path)\n",
        "df.head()\n",
        "\n",
        "# maehara_dfを作成し、1列目にslit_id、2列目にdisease_Englishを含める\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "maehara_df.head()\n"
      ],
      "metadata": {
        "id": "4FM6clySpXX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ファイルの存在確認\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Excelファイルを読み込む\n",
        "excel_data = pd.read_excel(excel_path)\n",
        "\n",
        "# 'basename'列の値を文字列に変換\n",
        "excel_data['basename'] = excel_data['basename'].astype(str)\n",
        "\n",
        "# チェックするディレクトリのパス\n",
        "image_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット'\n",
        "\n",
        "# エラーメッセージのリスト\n",
        "errors = []\n",
        "\n",
        "# 各basenameに対して、.jpgまたは.pngファイルがあるかどうかを確認\n",
        "for basename in excel_data['basename']:\n",
        "    jpg_file = os.path.join(image_dir, basename + '.jpg')\n",
        "    JPG_file = os.path.join(image_dir, basename + '.JPG')\n",
        "    png_file = os.path.join(image_dir, basename + '.png')\n",
        "\n",
        "    if not os.path.exists(jpg_file) and not os.path.exists(png_file) and not os.path.exists(JPG_file):\n",
        "        errors.append(f\"Error: Neither {basename}.jpg nor {basename}.JPG nor {basename}.png exists in the directory.\")\n",
        "\n",
        "# エラーメッセージを表示\n",
        "for error in errors:\n",
        "    print(error)\n",
        "\n",
        "# エラーがあるかどうかを確認\n",
        "if len(errors) > 0:\n",
        "    print(f\"Total missing files: {len(errors)}\")\n",
        "else:\n",
        "    print(\"All files are present.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uQmF9ZzyazAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(image, size=(640, 480)):\n",
        "    ih, iw = image.shape[:2]\n",
        "    w, h = size\n",
        "\n",
        "    # Calculate padding to maintain aspect ratio\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw, nh = int(scale * iw), int(scale * ih)\n",
        "    image = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    # Calculate padding dimensions\n",
        "    top = (h - nh) // 2\n",
        "    bottom = h - nh - top\n",
        "    left = (w - nw) // 2\n",
        "    right = w - nw - left\n",
        "\n",
        "    # Add padding to the image\n",
        "    return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2, size=(640, 480))\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "RO0ce7B3rPZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "nyBUtMe3rPbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "\n",
        "    # # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    # img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # # Get current figure size\n",
        "    # fig_size = plt.gcf().get_size_inches()\n",
        "    # # Set new size (half the current size)\n",
        "    # plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    # plt.imshow(img_mpl)\n",
        "    # plt.axis('off')  # Turn off axis numbers\n",
        "    # plt.show()\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    # print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        # print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "        # print(\"\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model_path, maehara_df, title, images_dir):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "    results = []\n",
        "\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = maehara_df.copy()\n",
        "\n",
        "    for index, row in tqdm(df_copy.iterrows(), total=df_copy.shape[0], desc=f\"Evaluating {title}\"):\n",
        "        img_id = row['basename']\n",
        "        # Try different file extensions\n",
        "        for ext in ['.jpg', '.png', '.JPG']:\n",
        "            img_path = os.path.join(images_dir, f\"{img_id}{ext}\")\n",
        "            if os.path.exists(img_path):\n",
        "                print(f\"Processing image: {img_path}\")\n",
        "                img_tensor = preprocess_image(img_path)\n",
        "                top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "                # Store results in a list to avoid SettingWithCopyWarning\n",
        "                for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "                    results.append((index, f'top{i+1}_{title}', class_names[class_num]))\n",
        "                    results.append((index, f'top{i+1}_{title}_likelihood', likelihood))\n",
        "                break\n",
        "        else:\n",
        "            print(f\"No image found for ID {img_id} with any of the extensions .jpg, .png, .JPG\")\n",
        "\n",
        "    # Apply the updates outside the loop\n",
        "    for index, column, value in results:\n",
        "        df_copy.at[index, column] = value\n",
        "\n",
        "    # Return the modified DataFrame\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "bA-QeMo7kX10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "weight_paths = [baseline_weight_path, mixed_weight_path, finetune_mixed_weight_path, finetune_100ep_weight_path, finetune_150ep_weight_path, finetune_200ep_weight_path, finetune_250ep_weight_path, finetune_300ep_weight_path, finetune_350ep_weight_path, finetune_400ep_weight_path]\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "# weight_paths = [baseline_weight_path]\n",
        "# titles = [\"baseline\"]\n",
        "\n",
        "for path, title in zip(weight_paths, titles):\n",
        "    maehara_df = evaluate_model(path, maehara_df, title, images_dir)"
      ],
      "metadata": {
        "id": "oWVXdG61kaXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maehara_df"
      ],
      "metadata": {
        "id": "aCY-E6Bqfmkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = maehara_df.dropna() #画像がなく評価が行われていない行を削除\n",
        "\n",
        "# Titles for the different model evaluations\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "def draw_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for top1_{title}')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Calculate and display confusion matrices\n",
        "for title in titles:\n",
        "    cm = confusion_matrix(df['disease_English'], df[f'top1_{title}'], labels=class_names_order)\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    accuracies[title] = accuracy\n",
        "    draw_confusion_matrix(cm, title)\n",
        "\n",
        "# Print all accuracies at the end\n",
        "for title, accuracy in accuracies.items():\n",
        "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "UtMAk6uZrPfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_df.xlsx\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "dqlVIZ1zrPhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load data\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_df.xlsx\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "maehara_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "maehara_df.head()\n"
      ],
      "metadata": {
        "id": "V5zPrMJLrPjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Augmentation_ideas**"
      ],
      "metadata": {
        "id": "OBSNECHYW6zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import glob\n",
        "\n",
        "# Define the path to your image\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = f\"{images_dir}/29.png\"\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[1]\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "def rgb_to_bgr(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to BGR. \"\"\"\n",
        "    # Reverse the order of the first and third channel\n",
        "    return img[..., ::-1]\n",
        "\n",
        "def rgb_to_grb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to GRB. \"\"\"\n",
        "    # Swap the red and green channels\n",
        "    return img[..., [1, 0, 2]]\n",
        "\n",
        "def rgb_to_rbg(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 2, 1]]\n",
        "\n",
        "\n",
        "\n",
        "# Define an augmentation pipeline with Albumentations\n",
        "augmentation = A.Compose([\n",
        "    A.Lambda(image=rgb_to_bgr, p=1.0), # Custom RGB to BGR conversion\n",
        "    #A.Lambda(image=rgb_to_rbg, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.Lambda(image=rgb_to_grb, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.RandomBrightnessContrast(p=1.0, brightness_limit=[0.3,0.3], contrast_limit=[-0.5,-0.5]),\n",
        "])\n",
        "\n",
        "# Apply the augmentation pipeline to the image\n",
        "augmented_img = augmentation(image=img)['image']\n",
        "\n",
        "# Display the augmented image using matplotlib\n",
        "plt.imshow(augmented_img)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GCOz71B2XAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "\n",
        "    # # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    # img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # # Get current figure size\n",
        "    # fig_size = plt.gcf().get_size_inches()\n",
        "    # # Set new size (half the current size)\n",
        "    # plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    # plt.imshow(img_mpl)\n",
        "    # plt.axis('off')  # Turn off axis numbers\n",
        "    # plt.show()\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    # print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        # print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "        # print(\"\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "def setup_model(model_path):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def check_groundtruth(maehara_df, image_dir):\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = maehara_df.copy()\n",
        "\n",
        "    img_id = os.path.basename(image_dir).split(\".\")[0]\n",
        "    groundtruth = df_copy.loc[df_copy['basename'] == img_id, 'disease_English']\n",
        "    return groundtruth.item()\n",
        "\n",
        "def bright_contarst_augment(img, brightness, contrast):\n",
        "    augmentation = A.Compose([\n",
        "        A.RandomBrightnessContrast(p=1.0, brightness_limit=[brightness,brightness], contrast_limit=[contrast,contrast])\n",
        "    ])\n",
        "    # Apply the augmentation\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n",
        "def coarse_dropout(img):\n",
        "    drop_size = round(img.shape[1]*0.02)\n",
        "    augmentation = A.Compose([\n",
        "        #A.CoarseDropout(p=1.0, max_holes=200, max_height=drop_size, max_width=drop_size, min_holes=200, min_height=drop_size, min_width=drop_size, fill_value=(114,114,114))\n",
        "        A.CoarseDropout(p=1.0, max_holes=200, max_height=drop_size, max_width=drop_size, min_holes=200, min_height=drop_size, min_width=drop_size, fill_value=(60,68,124))\n",
        "        ])\n",
        "    # Apply the augmentation\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n",
        "def rgb_to_bgr(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to BGR. \"\"\"\n",
        "    # Reverse the order of the first and third channel\n",
        "    return img[..., ::-1]\n",
        "\n",
        "def rgb_to_grb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to GRB. \"\"\"\n",
        "    # Swap the red and green channels\n",
        "    return img[..., [1, 0, 2]]\n",
        "\n",
        "def rgb_to_rbg(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 2, 1]]\n",
        "\n",
        "def no_augment(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RBG. \"\"\"\n",
        "    # Swap the green and blue channels\n",
        "    return img[..., [0, 1, 2]]\n",
        "\n",
        "def rgb_to_gb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to _GR (Red channel set to 0). \"\"\"\n",
        "    # Set the red channel (first channel) to 0\n",
        "    img[..., 0] = 0\n",
        "    return img\n",
        "\n",
        "def rgb_to_rb(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RB (Green channel set to 0). \"\"\"\n",
        "    # Set the green channel (second channel) to 0\n",
        "    img[..., 1] = 0\n",
        "    return img\n",
        "\n",
        "def rgb_to_sepia(img, **kwargs):\n",
        "    \"\"\" Convert image from RGB to RB (Green channel set to 0). \"\"\"\n",
        "    #全画素についてB(青)の輝度を0.3倍、G(緑)の輝度を0.8倍、R(赤)の輝度はそのまま\n",
        "    img[:, :, 0] =img[:, :, 0] * 0.3\n",
        "    img[:, :, 1] =img[:, :, 1] * 0.8\n",
        "    img[:, :, 2] =img[:, :, 2]\n",
        "    return img\n",
        "\n",
        "\n",
        "def rgb_to_grayscale(img, **kwargs):\n",
        "    \"\"\" Convert a 3-channel RGB image to a 3-channel grayscale image. \"\"\"\n",
        "    # Convert to grayscale using OpenCV\n",
        "    grayscale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    # Stack the grayscale values across the three channels\n",
        "    grayscale_3channel = cv2.cvtColor(grayscale, cv2.COLOR_GRAY2RGB)\n",
        "    return grayscale_3channel\n",
        "\n",
        "def RGB_augment(img, colorpattern):\n",
        "    augmentation = A.Compose([\n",
        "        A.Lambda(image=colorpattern, p=1.0), # rgb_to_bgr, rgb_to_grb, or rgb_to_rbg\n",
        "    ])\n",
        "    # Apply the augmentation pipeline to the image\n",
        "    augmented_img = augmentation(image=img)['image']\n",
        "    return augmented_img\n",
        "\n"
      ],
      "metadata": {
        "id": "eIJREB8qXAXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### RGB to BGR, RBG, & GRB ###############\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "df = pd.read_excel(excel_path)\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "model = setup_model(baseline_weight_path)\n",
        "\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[40]\n",
        "\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = glob.glob(f\"{images_dir}/*.png\")[2]\n",
        "\n",
        "\n",
        "try:\n",
        "    groundtruth = check_groundtruth(maehara_df, image_path)\n",
        "except:\n",
        "    groundtruth = None\n",
        "    pass\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img2 = img.copy()\n",
        "\n",
        "for colorpattern in [no_augment, rgb_to_bgr, rgb_to_grb, rgb_to_rbg, rgb_to_grayscale, rgb_to_gb, rgb_to_sepia]:\n",
        "    print(f\"Applying transformation: {colorpattern.__name__}\")\n",
        "    augmented_img = RGB_augment(img2, colorpattern)\n",
        "    augmented_img = coarse_dropout(augmented_img)\n",
        "    # augmented_img = RGB_augment(img2, colorpattern)\n",
        "    img = letterbox_image(augmented_img)\n",
        "    img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img_tensor = torch.from_numpy(img).float() / 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "    top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "    top1_class = class_names[top3_results[0][0]]\n",
        "    top1_likelihood = top3_results[0][1]\n",
        "    try:\n",
        "        print(f\"Groundtruth: {groundtruth}\")\n",
        "    except:\n",
        "        pass\n",
        "    print(f\"pred: {top1_class}, likelihood: {top1_likelihood}\")\n",
        "\n",
        "    augmented_img = cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB) #cv2 --> PIL\n",
        "    plt.imshow(augmented_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "G616y4PX5Tpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## Brightness & contrast ############\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "df = pd.read_excel(excel_path)\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "model = setup_model(baseline_weight_path)\n",
        "\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "image_path = glob.glob(f\"{images_dir}/*.jpg\")[37]\n",
        "\n",
        "# images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "# image_path = glob.glob(f\"{images_dir}/*.png\")[14]\n",
        "\n",
        "try:\n",
        "    groundtruth = check_groundtruth(maehara_df, image_path)\n",
        "except:\n",
        "    groundtruth = None\n",
        "    pass\n",
        "\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img2 = img.copy()\n",
        "\n",
        "for brightness in [-0.2, 0, 0.2, 0.4]:\n",
        "    for contrast in [-0.8, -0.4, -0.2, 0, 0.2, 0.4]:\n",
        "        print(f\"Brightness: {brightness}, Contrast: {contrast}\")\n",
        "        augmented_img = coarse_dropout(img2)\n",
        "        augmented_img = bright_contarst_augment(augmented_img, brightness, contrast)\n",
        "        img = letterbox_image(augmented_img)\n",
        "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img_tensor = torch.from_numpy(img).float() / 255\n",
        "        img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "        top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "        top1_class = class_names[top3_results[0][0]]\n",
        "        top1_likelihood = top3_results[0][1]\n",
        "        try:\n",
        "            print(f\"Groundtruth: {groundtruth}\")\n",
        "        except:\n",
        "            pass\n",
        "        print(f\"pred: {top1_class}, likelihood: {top1_likelihood}\")\n",
        "\n",
        "        augmented_img = cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB) #cv2 --> PIL\n",
        "        plt.imshow(augmented_img)\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "lhvjVRbhAg6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an augmentation pipeline with Albumentations\n",
        "augmentation = A.Compose([\n",
        "    A.Lambda(image=rgb_to_bgr, p=1.0), # Custom RGB to BGR conversion\n",
        "    #A.Lambda(image=rgb_to_rbg, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.Lambda(image=rgb_to_grb, p=1.0), # Custom RGB to RBG conversion\n",
        "    #A.RandomBrightnessContrast(p=1.0, brightness_limit=[0.3,0.3], contrast_limit=[-0.5,-0.5]),\n",
        "])"
      ],
      "metadata": {
        "id": "LwCaj-QgXAaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}