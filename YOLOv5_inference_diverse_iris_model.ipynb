{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0imEoqeCJTIMLFEG9V6mP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CorneAI/blob/main/YOLOv5_inference_diverse_iris_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 inference_diverse_iris_model**"
      ],
      "metadata": {
        "id": "Un512TpLoNtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F7fjEQUV-pd",
        "outputId": "60645673-9ea0-47c3-8fcd-f4737768719c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYRY9egwjuIs",
        "outputId": "de096760-66b0-4bf0-b371-165df576c5a4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "opencv-python-headless>=4.6.0.66\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "##requests>=2.23.0\n",
        "scipy>=1.4.1\n",
        "# torch>=1.7.0\n",
        "# torchvision>=0.8.1\n",
        "tqdm>=4.41.0\n",
        "\n",
        "# Logging -------------------------------------\n",
        "##tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "##pandas>=1.1.4\n",
        "##seaborn>=0.11.0\n",
        "\n",
        "# Export --------------------------------------\n",
        "# coremltools>=4.1  # CoreML export\n",
        "# onnx>=1.9.0  # ONNX export\n",
        "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
        "# scikit-learn==0.19.2  # CoreML quantization\n",
        "# tensorflow>=2.4.1  # TFLite export\n",
        "# tensorflowjs>=3.9.0  # TF.js export\n",
        "\n",
        "# Extras --------------------------------------\n",
        "# albumentations>=1.0.3\n",
        "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
        "# pycocotools>=2.0  # COCO mAP\n",
        "# roboflow\n",
        "thop  # FLOPs computation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUeMX07NqirS",
        "outputId": "e300a716-a66e-4308-b846-b9a5684687a2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Tfb6NYZIBGm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca41484f-72d3-4545-e58d-5a549e85f1b9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python-headless>=4.6.0.66 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.9.0.80)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (4.66.1)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop->-r requirements.txt (line 37)) (2.1.0+cu121)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop->-r requirements.txt (line 37)) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop->-r requirements.txt (line 37)) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop->-r requirements.txt (line 37)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#最新バージョンでも動くので削除\n",
        "# !pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install torchvision==0.11.2+cu113 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "ESI_x2upsdf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "weight = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "num = 5\n",
        "img_dir = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/data\"\n",
        "img = glob.glob(f\"{img_dir}/*\")[num]\n",
        "img\n",
        "\n",
        "img = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/runs/detect/exp/APAC_fko0078.jpg\""
      ],
      "metadata": {
        "id": "Bnz_lfT_l7NM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py  --weights $weight  --source $img"
      ],
      "metadata": {
        "id": "kPsBV4Itjrn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要な部分だけ抜粋"
      ],
      "metadata": {
        "id": "5BD1oZ3nFO6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference using Cornea journal dataset**"
      ],
      "metadata": {
        "id": "CGUbtkDXypdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/corneaのスマホ判定_滝先生.xlsx\"\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/3.CorneAIを混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\""
      ],
      "metadata": {
        "id": "dGKLXhUeG5ha"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file starting from row 13 (which is indexed as 12 in Python) for headers\n",
        "df = pd.read_excel(excel_path, header=12)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bjsM7cUC3jFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame and it's already loaded\n",
        "# df = pd.read_csv('your_dataframe.csv')  # Replace with your DataFrame loading method\n",
        "\n",
        "# Directory path\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# Define the columns for the new DataFrame\n",
        "columns = [\"image_num\", \"class\"]\n",
        "\n",
        "# Create an empty DataFrame with the specified columns\n",
        "cornea_journal_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Iterating over each file in the directory\n",
        "for filename in os.listdir(images_dir):\n",
        "    if filename.endswith(\".png\"):\n",
        "        # Extracting the numeric part (image number) from the filename\n",
        "        image_num = int(filename.split('.')[0])\n",
        "\n",
        "        # Match with the DataFrame and extract 'クラス' value\n",
        "        matched_row = df[df['Number'] == image_num]\n",
        "        if not matched_row.empty:\n",
        "            class_value = matched_row['クラス'].iloc[0]\n",
        "            # Creating a new row as a DataFrame\n",
        "            row_data = pd.DataFrame({\"image_num\": [image_num], \"class\": [class_value], \"class_name\": [class_names[class_value]]})\n",
        "            # Concatenating the new row DataFrame with the main DataFrame\n",
        "            cornea_journal_df = pd.concat([cornea_journal_df, row_data], ignore_index=True)\n",
        "\n",
        "# Sort the DataFrame by the image_num column\n",
        "cornea_journal_df.sort_values(by='image_num', inplace=True)\n",
        "\n",
        "# Displaying the sorted DataFrame\n",
        "print(cornea_journal_df)\n"
      ],
      "metadata": {
        "id": "_EPP9cvRAMD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像のクラスを確認\n",
        "# Getting the distribution of the 'class' column and sorting by class values\n",
        "class_distribution = cornea_journal_df['class'].value_counts().sort_index()\n",
        "\n",
        "# Displaying the distribution\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOLIBDKDAxPa",
        "outputId": "1ac4960d-8822-41e5-a34a-9396f9bf21aa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    46\n",
            "1     7\n",
            "2     9\n",
            "3    31\n",
            "4    33\n",
            "5    32\n",
            "8     4\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "# def letterbox_image(image, size=(640, 480)):\n",
        "#     ih, iw = image.shape[:2]\n",
        "#     w, h = size\n",
        "\n",
        "#     # Calculate padding to maintain aspect ratio\n",
        "#     scale = min(w / iw, h / ih)\n",
        "#     nw, nh = int(scale * iw), int(scale * ih)\n",
        "#     image = cv2.resize(image, (nw, nh))\n",
        "\n",
        "#     # Calculate padding dimensions\n",
        "#     top = (h - nh) // 2\n",
        "#     bottom = h - nh - top\n",
        "#     left = (w - nw) // 2\n",
        "#     right = w - nw - left\n",
        "\n",
        "#     # Add padding to the image\n",
        "#     return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "BB9E6zqwD0Py"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple inference\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"][0:5]:\n",
        "      img = f\"{images_dir}/{num}.png\"\n",
        "      inference_top3(img, model)"
      ],
      "metadata": {
        "id": "7UmC2m6cE160"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "vJab4D4MH2MF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(baseline_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_baseline_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "Y6SOama5H51r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# New model\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_mixed_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\"\n",
        "finetune_100ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/100epoch/last.pt\"\n",
        "finetune_150ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/150epoch/last.pt\"\n",
        "finetune_200ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/200epoch/last.pt\"\n",
        "finetune_250ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/250epoch/last.pt\"\n",
        "finetune_300ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/300epoch/last.pt\"\n",
        "finetune_350ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/350epoch/last.pt\"\n",
        "finetune_400ep_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/4.CorneAIを虹彩薄いデータセットで追加学習/400epoch/last.pt\"\n",
        "\"\"\"\n",
        "\n",
        "# Modify the loop to update DataFrame\n",
        "model = DetectMultiBackend(mixed_weight_path, device=\"cpu\", dnn=False)\n",
        "for num in cornea_journal_df[\"image_num\"]:\n",
        "    img = f\"{images_dir}/{num}.png\"\n",
        "    top3_results = inference_top3(img, model)\n",
        "\n",
        "    # Update the DataFrame with the results\n",
        "    for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new'] = class_names[class_num]\n",
        "        cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_new_likelihood'] = likelihood"
      ],
      "metadata": {
        "id": "ZhqjG7deIJkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "C-uVI3bJIOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/baseline_to_mixed.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "PrxCWw7EIQkF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Analyze_results***"
      ],
      "metadata": {
        "id": "CKH8THhJLdyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "#csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/compare_accuracy.csv\"\n",
        "csv_path = dst_path\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "eSFoOT4-LhBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# Create a confusion matrix for top1_baseline\n",
        "cm_baseline = confusion_matrix(df['class_name'], df['top1_baseline'], labels=class_names_order)\n",
        "\n",
        "# Create a confusion matrix for top1_new\n",
        "cm_new = confusion_matrix(df['class_name'], df['top1_new'], labels=class_names_order)\n",
        "\n",
        "\n",
        "# Plot for top1_baseline\n",
        "fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_baseline, annot=True, ax=ax1, fmt='d', cmap='Blues')\n",
        "ax1.set_title('Confusion Matrix for top1_baseline')\n",
        "ax1.set_xlabel('Predicted Labels')\n",
        "ax1.set_ylabel('True Labels')\n",
        "ax1.set_xticklabels(class_names_order, rotation=45)\n",
        "ax1.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot for top1_new\n",
        "fig2, ax2 = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm_new, annot=True, ax=ax2, fmt='d', cmap='Greens')\n",
        "ax2.set_title('Confusion Matrix for top1_new')\n",
        "ax2.set_xlabel('Predicted Labels')\n",
        "ax2.set_ylabel('True Labels')\n",
        "ax2.set_xticklabels(class_names_order, rotation=45)\n",
        "ax2.set_yticklabels(class_names_order, rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yN7BMwQZLmgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for top1_baseline and top1_new\n",
        "accuracy_baseline = np.trace(cm_baseline) / np.sum(cm_baseline)\n",
        "accuracy_new = np.trace(cm_new) / np.sum(cm_new)\n",
        "\n",
        "accuracy_baseline, accuracy_new\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYZWAdVhM87P",
        "outputId": "1a02d428-e668-4758-f2ef-fc0a0197a673"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MultiModel evaluation pipeline**"
      ],
      "metadata": {
        "id": "NrWAyrggO9nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #######################\n",
        "# # MultiModel evaluation pipeline #\n",
        "# #######################\n",
        "\n",
        "# from models.common import DetectMultiBackend\n",
        "# #from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "# from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "#                            increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "# #from utils.plots import Annotator, colors, save_one_box\n",
        "# #from utils.torch_utils import select_device, time_sync\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# from torchvision import models, transforms\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# device = 'cpu'\n",
        "# #model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# # model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "# class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "# def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "#     # Resize and pad image while meeting stride-multiple constraints\n",
        "#     shape = im.shape[:2]  # current shape [height, width]\n",
        "#     if isinstance(new_shape, int):\n",
        "#         new_shape = (new_shape, new_shape)\n",
        "\n",
        "#     # Scale ratio (new / old)\n",
        "#     r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "#     if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "#         r = min(r, 1.0)\n",
        "\n",
        "#     # Compute padding\n",
        "#     # ratio = r, r  # width, height ratios\n",
        "#     new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "#     dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "#     if auto:  # minimum rectangle\n",
        "#         dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "#     elif scaleFill:  # stretch\n",
        "#         dw, dh = 0.0, 0.0\n",
        "#         new_unpad = (new_shape[1], new_shape[0])\n",
        "#         # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "#     dw /= 2  # divide padding into 2 sides\n",
        "#     dh /= 2\n",
        "\n",
        "#     if shape[::-1] != new_unpad:  # resize\n",
        "#         im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "#     top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "#     left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "#     im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "#     return im\n",
        "\n",
        "# def inference_top3(img_path, model):\n",
        "#     img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "#     # Apply letterbox to the image\n",
        "#     img_cv2 = letterbox_image(img_cv2)\n",
        "#     #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "#     # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "#     img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "#     # Get current figure size\n",
        "#     fig_size = plt.gcf().get_size_inches()\n",
        "#     # Set new size (half the current size)\n",
        "#     plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "#     # plt.imshow(img_mpl)\n",
        "#     # plt.axis('off')  # Turn off axis numbers\n",
        "#     # plt.show()\n",
        "\n",
        "#     img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "#     img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "#     img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "#     img_tensor /= 255\n",
        "#     img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "#     pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "#     # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "#     pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "#     # 全てのクラスとlikelihoodのペアを取得\n",
        "#     class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "#     # likelihoodで降順にソート\n",
        "#     class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "#     # 上位3つのクラスとlikelihoodを出力\n",
        "#     print(\"Top 3 Classes and Likelihoods:\")\n",
        "#     for i in range(3):\n",
        "#         likelihood, class_num = class_likelihood_pairs[i]\n",
        "#         print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "#         print(\"\")\n",
        "\n",
        "#     top_classes = []\n",
        "#     for i in range(min(3, len(class_likelihood_pairs))):\n",
        "#         likelihood, class_num = class_likelihood_pairs[i]\n",
        "#         top_classes.append((class_num, likelihood))\n",
        "\n",
        "#     return top_classes\n",
        "\n",
        "# def evaluation(model_path, title):\n",
        "#     model = DetectMultiBackend(model_path, device=\"cpu\", dnn=False)\n",
        "#     for num in cornea_journal_df[\"image_num\"]:\n",
        "#         img = f\"{images_dir}/{num}.png\"\n",
        "#         top3_results = inference_top3(img, model)\n",
        "\n",
        "#         # Update the DataFrame with the results\n",
        "#         for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "#             cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "#             cornea_journal_df.loc[cornea_journal_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "# evaluation(baseline_weight_path, \"baseline\")\n",
        "# evaluation(mixed_weight_path, \"mixed\")\n",
        "# evaluation(finetune_mixed_weight_path, \"finetune_mixed\")\n",
        "# evaluation(finetune_100ep_weight_path, \"finetune_100ep\")\n",
        "# evaluation(finetune_150ep_weight_path, \"finetune_150ep\")\n",
        "# evaluation(finetune_200ep_weight_path, \"finetune_200ep\")\n",
        "# evaluation(finetune_250ep_weight_path, \"finetune_250ep\")\n",
        "# evaluation(finetune_300ep_weight_path, \"finetune_300ep\")\n",
        "# evaluation(finetune_350ep_weight_path, \"finetune_350ep\")\n",
        "# evaluation(finetune_400ep_weight_path, \"finetune_400ep\")"
      ],
      "metadata": {
        "id": "tXmOPrcKL526"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    from utils.general import non_max_suppression\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    top_classes = [(class_num, likelihood) for _, class_num in sorted(class_likelihood_pairs[:3])]\n",
        "    return top_classes\n",
        "\n",
        "def evaluate_model(model_path, data_df, title, images_dir):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "    for num in data_df[\"image_num\"]:\n",
        "        img_path = f\"{images_dir}/{num}.png\"\n",
        "        img_tensor = preprocess_image(img_path)\n",
        "        top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "        for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "            data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}'] = class_names[class_num]\n",
        "            data_df.loc[data_df['image_num'] == num, f'top{i+1}_{title}_likelihood'] = likelihood\n",
        "\n",
        "# Example usage\n",
        "weight_paths = [baseline_weight_path, mixed_weight_path, finetune_mixed_weight_path, finetune_100ep_weight_path, finetune_150ep_weight_path, finetune_200ep_weight_path, finetune_250ep_weight_path, finetune_300ep_weight_path, finetune_350ep_weight_path, finetune_400ep_weight_path]\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "for path, title in zip(weight_paths, titles):\n",
        "    evaluate_model(path, cornea_journal_df, title, images_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "lxMkzMo-jNog",
        "outputId": "909e4b98-b823-43e7-f538-a6e0fda7693e"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7034398 parameters, 0 gradients, 15.8 GFLOPs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-0b4ba3054403>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcornea_journal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-112-0b4ba3054403>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model_path, data_df, title, images_dir)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{images_dir}/{num}.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtop3_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_top3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclass_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop3_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-112-0b4ba3054403>\u001b[0m in \u001b[0;36minference_top3\u001b[0;34m(img_tensor, model)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minference_top3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_det\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im, augment, visualize, val)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[0;31m# batch, channel, height, width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoreml\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# CoreML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/models/yolo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# augmented inference, None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# single-scale inference, train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/models/yolo.py\u001b[0m in \u001b[0;36m_forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/models/common.py\u001b[0m in \u001b[0;36mforward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/cornea_journal_all.csv\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "e2YGhZgdSg1F"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming csv_path is the file path to your CSV file\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/cornea_journal_all.csv\"\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "cornea_journal_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "cornea_journal_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "dYatGMIrolDA",
        "outputId": "22e2ad29-1f03-42d1-ade8-67272edda085"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_num  class     class_name  top1_baseline  top1_baseline_likelihood  \\\n",
              "0          2      0      infection      infection                  0.855779   \n",
              "1          3      0      infection      infection                  0.851234   \n",
              "2          8      2  non-infection  non-infection                  0.932383   \n",
              "3          9      5        deposit        bullous                  0.915752   \n",
              "4         12      3           scar           scar                  0.726584   \n",
              "\n",
              "   top2_baseline  top2_baseline_likelihood  top3_baseline  \\\n",
              "0           scar                  0.027026      infection   \n",
              "1  non-infection                  0.035410        bullous   \n",
              "2      infection                  0.032079        bullous   \n",
              "3           scar                  0.028603         normal   \n",
              "4        deposit                  0.037347  non-infection   \n",
              "\n",
              "   top3_baseline_likelihood     top1_mixed  top1_mixed_likelihood  \\\n",
              "0                  0.007761      infection               0.695250   \n",
              "1                  0.001568      infection               0.655799   \n",
              "2                  0.001097  non-infection               0.926924   \n",
              "3                  0.000560        bullous               0.800950   \n",
              "4                  0.017396           scar               0.855604   \n",
              "\n",
              "      top2_mixed  top2_mixed_likelihood     top3_mixed  top3_mixed_likelihood  \\\n",
              "0  non-infection               0.010663      infection               0.004296   \n",
              "1  non-infection               0.407020        bullous               0.001230   \n",
              "2      infection               0.002863           APAC               0.000703   \n",
              "3           scar               0.406266  non-infection               0.001190   \n",
              "4      infection               0.039542  non-infection               0.014976   \n",
              "\n",
              "  top1_finetune_100ep  top1_finetune_100ep_likelihood top2_finetune_100ep  \\\n",
              "0       non-infection                        0.345170           infection   \n",
              "1           infection                        0.849594       non-infection   \n",
              "2           infection                        0.864857       non-infection   \n",
              "3             bullous                        0.951717                scar   \n",
              "4             deposit                        0.787356       non-infection   \n",
              "\n",
              "   top2_finetune_100ep_likelihood top3_finetune_100ep  \\\n",
              "0                        0.217031                APAC   \n",
              "1                        0.019639           infection   \n",
              "2                        0.089441                APAC   \n",
              "3                        0.007680             deposit   \n",
              "4                        0.169500                scar   \n",
              "\n",
              "   top3_finetune_100ep_likelihood top1_finetune_150ep  \\\n",
              "0                        0.182208           infection   \n",
              "1                        0.001300           infection   \n",
              "2                        0.002789       non-infection   \n",
              "3                        0.003308             bullous   \n",
              "4                        0.020578             deposit   \n",
              "\n",
              "   top1_finetune_150ep_likelihood top2_finetune_150ep  \\\n",
              "0                        0.680266             bullous   \n",
              "1                        0.869913       non-infection   \n",
              "2                        0.847820           infection   \n",
              "3                        0.968104                scar   \n",
              "4                        0.334384       non-infection   \n",
              "\n",
              "   top2_finetune_150ep_likelihood top3_finetune_150ep  \\\n",
              "0                        0.097989       non-infection   \n",
              "1                        0.026295       non-infection   \n",
              "2                        0.101861               tumor   \n",
              "3                        0.009100             deposit   \n",
              "4                        0.259550                scar   \n",
              "\n",
              "   top3_finetune_150ep_likelihood top1_finetune_200ep  \\\n",
              "0                        0.027656       non-infection   \n",
              "1                        0.003879           infection   \n",
              "2                        0.006752           infection   \n",
              "3                        0.003458             bullous   \n",
              "4                        0.199858           infection   \n",
              "\n",
              "   top1_finetune_200ep_likelihood top2_finetune_200ep  \\\n",
              "0                        0.717637           infection   \n",
              "1                        0.768343       non-infection   \n",
              "2                        0.661137       non-infection   \n",
              "3                        0.948472                APAC   \n",
              "4                        0.913641                scar   \n",
              "\n",
              "   top2_finetune_200ep_likelihood top3_finetune_200ep  \\\n",
              "0                        0.205339                APAC   \n",
              "1                        0.188962                APAC   \n",
              "2                        0.224099               tumor   \n",
              "3                        0.025373             deposit   \n",
              "4                        0.054381       non-infection   \n",
              "\n",
              "   top3_finetune_200ep_likelihood top1_finetune_250ep  \\\n",
              "0                        0.009371           infection   \n",
              "1                        0.001271           infection   \n",
              "2                        0.017033           infection   \n",
              "3                        0.015291             bullous   \n",
              "4                        0.027396             deposit   \n",
              "\n",
              "   top1_finetune_250ep_likelihood top2_finetune_250ep  \\\n",
              "0                        0.819436                APAC   \n",
              "1                        0.900374       non-infection   \n",
              "2                        0.788463       non-infection   \n",
              "3                        0.923724             deposit   \n",
              "4                        0.209216           infection   \n",
              "\n",
              "   top2_finetune_250ep_likelihood top3_finetune_250ep  \\\n",
              "0                        0.027146       non-infection   \n",
              "1                        0.002750             bullous   \n",
              "2                        0.112871                APAC   \n",
              "3                        0.156924                scar   \n",
              "4                        0.092830                APAC   \n",
              "\n",
              "   top3_finetune_250ep_likelihood top1_finetune_300ep  \\\n",
              "0                        0.015354           infection   \n",
              "1                        0.000958           infection   \n",
              "2                        0.020996           infection   \n",
              "3                        0.018927             bullous   \n",
              "4                        0.066945           infection   \n",
              "\n",
              "   top1_finetune_300ep_likelihood top2_finetune_300ep  \\\n",
              "0                        0.728658       non-infection   \n",
              "1                        0.695330       non-infection   \n",
              "2                        0.478523       non-infection   \n",
              "3                        0.964682                APAC   \n",
              "4                        0.878005             deposit   \n",
              "\n",
              "   top2_finetune_300ep_likelihood top3_finetune_300ep  \\\n",
              "0                        0.132518             bullous   \n",
              "1                        0.236423             bullous   \n",
              "2                        0.291597                APAC   \n",
              "3                        0.016771             deposit   \n",
              "4                        0.055033       non-infection   \n",
              "\n",
              "   top3_finetune_300ep_likelihood top1_finetune_350ep  \\\n",
              "0                        0.009476           infection   \n",
              "1                        0.001459           infection   \n",
              "2                        0.009853           infection   \n",
              "3                        0.015369             bullous   \n",
              "4                        0.035837           infection   \n",
              "\n",
              "   top1_finetune_350ep_likelihood top2_finetune_350ep  \\\n",
              "0                        0.846253       non-infection   \n",
              "1                        0.824685       non-infection   \n",
              "2                        0.901777       non-infection   \n",
              "3                        0.819569                APAC   \n",
              "4                        0.660798       non-infection   \n",
              "\n",
              "   top2_finetune_350ep_likelihood top3_finetune_350ep  \\\n",
              "0                        0.172660             bullous   \n",
              "1                        0.017642              normal   \n",
              "2                        0.056277                APAC   \n",
              "3                        0.149195             deposit   \n",
              "4                        0.079299                scar   \n",
              "\n",
              "   top3_finetune_350ep_likelihood top1_finetune_400ep  \\\n",
              "0                        0.001184           infection   \n",
              "1                        0.000668           infection   \n",
              "2                        0.017461       non-infection   \n",
              "3                        0.090281             bullous   \n",
              "4                        0.032321           infection   \n",
              "\n",
              "   top1_finetune_400ep_likelihood top2_finetune_400ep  \\\n",
              "0                        0.582444       non-infection   \n",
              "1                        0.940233       non-infection   \n",
              "2                        0.756454               tumor   \n",
              "3                        0.282106             deposit   \n",
              "4                        0.682875       non-infection   \n",
              "\n",
              "   top2_finetune_400ep_likelihood top3_finetune_400ep  \\\n",
              "0                        0.269594             bullous   \n",
              "1                        0.032478             deposit   \n",
              "2                        0.195080           infection   \n",
              "3                        0.156968                APAC   \n",
              "4                        0.086578                scar   \n",
              "\n",
              "   top3_finetune_400ep_likelihood top1_finetune_mixed  \\\n",
              "0                        0.006027           infection   \n",
              "1                        0.001997           infection   \n",
              "2                        0.035135       non-infection   \n",
              "3                        0.114196             bullous   \n",
              "4                        0.068892                scar   \n",
              "\n",
              "   top1_finetune_mixed_likelihood top2_finetune_mixed  \\\n",
              "0                        0.841111       non-infection   \n",
              "1                        0.835178       non-infection   \n",
              "2                        0.716683           infection   \n",
              "3                        0.826806                scar   \n",
              "4                        0.557056           infection   \n",
              "\n",
              "   top2_finetune_mixed_likelihood top3_finetune_mixed  \\\n",
              "0                        0.010078                APAC   \n",
              "1                        0.117021                APAC   \n",
              "2                        0.564749                APAC   \n",
              "3                        0.276979           infection   \n",
              "4                        0.261085       non-infection   \n",
              "\n",
              "   top3_finetune_mixed_likelihood  \n",
              "0                        0.002872  \n",
              "1                        0.000951  \n",
              "2                        0.002504  \n",
              "3                        0.001092  \n",
              "4                        0.120499  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8eaeb667-daea-44f6-9a08-83b5852c212f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_num</th>\n",
              "      <th>class</th>\n",
              "      <th>class_name</th>\n",
              "      <th>top1_baseline</th>\n",
              "      <th>top1_baseline_likelihood</th>\n",
              "      <th>top2_baseline</th>\n",
              "      <th>top2_baseline_likelihood</th>\n",
              "      <th>top3_baseline</th>\n",
              "      <th>top3_baseline_likelihood</th>\n",
              "      <th>top1_mixed</th>\n",
              "      <th>top1_mixed_likelihood</th>\n",
              "      <th>top2_mixed</th>\n",
              "      <th>top2_mixed_likelihood</th>\n",
              "      <th>top3_mixed</th>\n",
              "      <th>top3_mixed_likelihood</th>\n",
              "      <th>top1_finetune_100ep</th>\n",
              "      <th>top1_finetune_100ep_likelihood</th>\n",
              "      <th>top2_finetune_100ep</th>\n",
              "      <th>top2_finetune_100ep_likelihood</th>\n",
              "      <th>top3_finetune_100ep</th>\n",
              "      <th>top3_finetune_100ep_likelihood</th>\n",
              "      <th>top1_finetune_150ep</th>\n",
              "      <th>top1_finetune_150ep_likelihood</th>\n",
              "      <th>top2_finetune_150ep</th>\n",
              "      <th>top2_finetune_150ep_likelihood</th>\n",
              "      <th>top3_finetune_150ep</th>\n",
              "      <th>top3_finetune_150ep_likelihood</th>\n",
              "      <th>top1_finetune_200ep</th>\n",
              "      <th>top1_finetune_200ep_likelihood</th>\n",
              "      <th>top2_finetune_200ep</th>\n",
              "      <th>top2_finetune_200ep_likelihood</th>\n",
              "      <th>top3_finetune_200ep</th>\n",
              "      <th>top3_finetune_200ep_likelihood</th>\n",
              "      <th>top1_finetune_250ep</th>\n",
              "      <th>top1_finetune_250ep_likelihood</th>\n",
              "      <th>top2_finetune_250ep</th>\n",
              "      <th>top2_finetune_250ep_likelihood</th>\n",
              "      <th>top3_finetune_250ep</th>\n",
              "      <th>top3_finetune_250ep_likelihood</th>\n",
              "      <th>top1_finetune_300ep</th>\n",
              "      <th>top1_finetune_300ep_likelihood</th>\n",
              "      <th>top2_finetune_300ep</th>\n",
              "      <th>top2_finetune_300ep_likelihood</th>\n",
              "      <th>top3_finetune_300ep</th>\n",
              "      <th>top3_finetune_300ep_likelihood</th>\n",
              "      <th>top1_finetune_350ep</th>\n",
              "      <th>top1_finetune_350ep_likelihood</th>\n",
              "      <th>top2_finetune_350ep</th>\n",
              "      <th>top2_finetune_350ep_likelihood</th>\n",
              "      <th>top3_finetune_350ep</th>\n",
              "      <th>top3_finetune_350ep_likelihood</th>\n",
              "      <th>top1_finetune_400ep</th>\n",
              "      <th>top1_finetune_400ep_likelihood</th>\n",
              "      <th>top2_finetune_400ep</th>\n",
              "      <th>top2_finetune_400ep_likelihood</th>\n",
              "      <th>top3_finetune_400ep</th>\n",
              "      <th>top3_finetune_400ep_likelihood</th>\n",
              "      <th>top1_finetune_mixed</th>\n",
              "      <th>top1_finetune_mixed_likelihood</th>\n",
              "      <th>top2_finetune_mixed</th>\n",
              "      <th>top2_finetune_mixed_likelihood</th>\n",
              "      <th>top3_finetune_mixed</th>\n",
              "      <th>top3_finetune_mixed_likelihood</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>infection</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.855779</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.027026</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.007761</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.695250</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.010663</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.004296</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.345170</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.217031</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.182208</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.680266</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.097989</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.027656</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.717637</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.205339</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.009371</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.819436</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.027146</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.015354</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.728658</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.132518</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.009476</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.846253</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.172660</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.582444</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.269594</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.006027</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.841111</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.010078</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.002872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>infection</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.851234</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.035410</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.001568</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.655799</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.407020</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.849594</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.019639</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.869913</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.026295</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.003879</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.768343</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.188962</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.001271</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.900374</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.002750</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.695330</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.236423</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.824685</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.017642</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.940233</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.032478</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.001997</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.835178</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.117021</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.000951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.932383</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.032079</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.926924</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.002863</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.864857</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.089441</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.847820</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.101861</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.006752</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.661137</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.224099</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.017033</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.788463</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.112871</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.020996</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.478523</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.291597</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.009853</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.901777</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.056277</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.017461</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.756454</td>\n",
              "      <td>tumor</td>\n",
              "      <td>0.195080</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.035135</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.716683</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.564749</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.002504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>deposit</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.915752</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.028603</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.800950</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.406266</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.951717</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.007680</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.003308</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.968104</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.009100</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.003458</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.948472</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.025373</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.015291</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.923724</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.156924</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.018927</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.964682</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.016771</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.015369</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.819569</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.149195</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.090281</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.282106</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.156968</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.114196</td>\n",
              "      <td>bullous</td>\n",
              "      <td>0.826806</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.276979</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.001092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>scar</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.726584</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.037347</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.017396</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.855604</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.039542</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.014976</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.787356</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.169500</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.020578</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.334384</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.259550</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.199858</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.913641</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.054381</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.027396</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.209216</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.092830</td>\n",
              "      <td>APAC</td>\n",
              "      <td>0.066945</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.878005</td>\n",
              "      <td>deposit</td>\n",
              "      <td>0.055033</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.035837</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.660798</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.079299</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.032321</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.682875</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.086578</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.068892</td>\n",
              "      <td>scar</td>\n",
              "      <td>0.557056</td>\n",
              "      <td>infection</td>\n",
              "      <td>0.261085</td>\n",
              "      <td>non-infection</td>\n",
              "      <td>0.120499</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8eaeb667-daea-44f6-9a08-83b5852c212f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8eaeb667-daea-44f6-9a08-83b5852c212f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8eaeb667-daea-44f6-9a08-83b5852c212f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-edfe7c21-1401-4af4-b8d4-14a91262a425\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-edfe7c21-1401-4af4-b8d4-14a91262a425')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-edfe7c21-1401-4af4-b8d4-14a91262a425 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cornea_journal_df"
      ],
      "metadata": {
        "id": "r8xMxymdUoq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = cornea_journal_df\n",
        "\n",
        "# Titles for the different model evaluations\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "def draw_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for top1_{title}')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Calculate and display confusion matrices\n",
        "for title in titles:\n",
        "    cm = confusion_matrix(df['class_name'], df[f'top1_{title}'], labels=class_names_order)\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    accuracies[title] = accuracy\n",
        "    draw_confusion_matrix(cm, title)\n",
        "\n",
        "# Print all accuracies at the end\n",
        "for title, accuracy in accuracies.items():\n",
        "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "vdMnTluOeH3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference_Maehara_dataset**"
      ],
      "metadata": {
        "id": "bTj36ASWlo3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the zip file and the extraction directory\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット\"\n",
        "\n",
        "# Specify the correspondence table\n",
        "excel_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/対応表2022.xlsx\"\n",
        "\n",
        "baseline_weight_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "new_weight_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/oda_dr_diverse_model/2.始めから混合データセットで学習/last.pt\""
      ],
      "metadata": {
        "id": "aLTTuAJ6lstO"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# DataFrame 'df' にExcelファイルを読み込む\n",
        "df = pd.read_excel(excel_path)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4FM6clySpXX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maehara_dfを作成し、1列目にslit_id、2列目にdisease_Englishを含める\n",
        "maehara_df = df[['basename', 'disease_English']]\n",
        "maehara_df.head()\n"
      ],
      "metadata": {
        "id": "hnDgLBHapiJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ファイルの存在確認\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Excelファイルを読み込む\n",
        "excel_data = pd.read_excel(excel_path)\n",
        "\n",
        "# 'basename'列の値を文字列に変換\n",
        "excel_data['basename'] = excel_data['basename'].astype(str)\n",
        "\n",
        "# チェックするディレクトリのパス\n",
        "image_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット'\n",
        "\n",
        "# エラーメッセージのリスト\n",
        "errors = []\n",
        "\n",
        "# 各basenameに対して、.jpgまたは.pngファイルがあるかどうかを確認\n",
        "for basename in excel_data['basename']:\n",
        "    jpg_file = os.path.join(image_dir, basename + '.jpg')\n",
        "    JPG_file = os.path.join(image_dir, basename + '.JPG')\n",
        "    png_file = os.path.join(image_dir, basename + '.png')\n",
        "\n",
        "    if not os.path.exists(jpg_file) and not os.path.exists(png_file) and not os.path.exists(JPG_file):\n",
        "        errors.append(f\"Error: Neither {basename}.jpg nor {basename}.JPG nor {basename}.png exists in the directory.\")\n",
        "\n",
        "# エラーメッセージを表示\n",
        "for error in errors:\n",
        "    print(error)\n",
        "\n",
        "# エラーがあるかどうかを確認\n",
        "if len(errors) > 0:\n",
        "    print(f\"Total missing files: {len(errors)}\")\n",
        "else:\n",
        "    print(\"All files are present.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uQmF9ZzyazAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "#model = DetectMultiBackend(orig_weight_path, device=\"cpu\", dnn=False)\n",
        "# model = DetectMultiBackend(new_weight_path, device=\"cpu\", dnn=False)\n",
        "\n",
        "class_names = {0:\"infection\", 1:\"normal\", 2:\"non-infection\", 3:\"scar\", 4:\"tumor\", 5:\"deposit\", 6:\"APAC\", 7:\"lens opacity\", 8:\"bullous\"}\n",
        "\n",
        "def letterbox_image(image, size=(640, 480)):\n",
        "    ih, iw = image.shape[:2]\n",
        "    w, h = size\n",
        "\n",
        "    # Calculate padding to maintain aspect ratio\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw, nh = int(scale * iw), int(scale * ih)\n",
        "    image = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    # Calculate padding dimensions\n",
        "    top = (h - nh) // 2\n",
        "    bottom = h - nh - top\n",
        "    left = (w - nw) // 2\n",
        "    right = w - nw - left\n",
        "\n",
        "    # Add padding to the image\n",
        "    return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "\n",
        "def inference_top3(img_path, model):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "\n",
        "    # Apply letterbox to the image\n",
        "    img_cv2 = letterbox_image(img_cv2, size=(640, 480))\n",
        "    #img_cv2 = cv2.resize(img_cv2,(640, 480))\n",
        "\n",
        "    # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # Get current figure size\n",
        "    fig_size = plt.gcf().get_size_inches()\n",
        "    # Set new size (half the current size)\n",
        "    plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    plt.imshow(img_mpl)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    img_tensor /= 255\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    # multi_label=Trueにすることにより全クラスの値のlikelihoodを取得できる。数値が低いものが省略されないようconf_thres=0にしている。\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, classes=None , multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes"
      ],
      "metadata": {
        "id": "RO0ce7B3rPZo"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "nyBUtMe3rPbN"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MultiModel evaluation pipeline #\n",
        "#######################\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "#from utils.torch_utils import select_device, time_sync\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "class_names = {0: \"infection\", 1: \"normal\", 2: \"non-infection\", 3: \"scar\", 4: \"tumor\", 5: \"deposit\", 6: \"APAC\", 7: \"lens opacity\", 8: \"bullous\"}\n",
        "\n",
        "def letterbox_image(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): #Crescoのletterbox\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    # ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        # ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img_cv2 = cv2.imread(img_path)\n",
        "    img_cv2 = letterbox_image(img_cv2)\n",
        "\n",
        "    # # Display the image using Matplotlib（表示させない場合はコメントアウトする）\n",
        "    # img_mpl = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "    # # Get current figure size\n",
        "    # fig_size = plt.gcf().get_size_inches()\n",
        "    # # Set new size (half the current size)\n",
        "    # plt.gcf().set_size_inches(fig_size[0] / 2, fig_size[1] / 2)\n",
        "\n",
        "    # plt.imshow(img_mpl)\n",
        "    # plt.axis('off')  # Turn off axis numbers\n",
        "    # plt.show()\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float() / 255\n",
        "    return torch.unsqueeze(img_tensor, 0)\n",
        "\n",
        "def inference_top3(img_tensor, model):\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "    pred = non_max_suppression(pred, conf_thres=0, iou_thres=0.45, multi_label=True, max_det=1000)\n",
        "\n",
        "    # 全てのクラスとlikelihoodのペアを取得\n",
        "    class_likelihood_pairs = [(pred[0][row][4].item(), int(pred[0][row][5].item())) for row in range(8)]\n",
        "\n",
        "    # likelihoodで降順にソート\n",
        "    class_likelihood_pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # 上位3つのクラスとlikelihoodを出力\n",
        "    # print(\"Top 3 Classes and Likelihoods:\")\n",
        "    for i in range(3):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        # print(f\"Class {class_num} ({class_names[class_num]}): Likelihood {likelihood:.5f}\")\n",
        "        # print(\"\")\n",
        "\n",
        "    top_classes = []\n",
        "    for i in range(min(3, len(class_likelihood_pairs))):\n",
        "        likelihood, class_num = class_likelihood_pairs[i]\n",
        "        top_classes.append((class_num, likelihood))\n",
        "\n",
        "    return top_classes\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model_path, maehara_df, title, images_dir):\n",
        "    model = DetectMultiBackend(model_path, device=device, dnn=False)\n",
        "    results = []\n",
        "\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = maehara_df.copy()\n",
        "\n",
        "    for index, row in tqdm(df_copy.iterrows(), total=df_copy.shape[0], desc=f\"Evaluating {title}\"):\n",
        "        img_id = row['basename']\n",
        "        # Try different file extensions\n",
        "        for ext in ['.jpg', '.png', '.JPG']:\n",
        "            img_path = os.path.join(images_dir, f\"{img_id}{ext}\")\n",
        "            if os.path.exists(img_path):\n",
        "                print(f\"Processing image: {img_path}\")\n",
        "                img_tensor = preprocess_image(img_path)\n",
        "                top3_results = inference_top3(img_tensor, model)\n",
        "\n",
        "                # Store results in a list to avoid SettingWithCopyWarning\n",
        "                for i, (class_num, likelihood) in enumerate(top3_results):\n",
        "                    results.append((index, f'top{i+1}_{title}', class_names[class_num]))\n",
        "                    results.append((index, f'top{i+1}_{title}_likelihood', likelihood))\n",
        "                break\n",
        "        else:\n",
        "            print(f\"No image found for ID {img_id} with any of the extensions .jpg, .png, .JPG\")\n",
        "\n",
        "    # Apply the updates outside the loop\n",
        "    for index, column, value in results:\n",
        "        df_copy.at[index, column] = value\n",
        "\n",
        "    # Return the modified DataFrame\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "bA-QeMo7kX10"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "weight_paths = [baseline_weight_path, mixed_weight_path, finetune_mixed_weight_path, finetune_100ep_weight_path, finetune_150ep_weight_path, finetune_200ep_weight_path, finetune_250ep_weight_path, finetune_300ep_weight_path, finetune_350ep_weight_path, finetune_400ep_weight_path]\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "# weight_paths = [baseline_weight_path]\n",
        "# titles = [\"baseline\"]\n",
        "\n",
        "for path, title in zip(weight_paths, titles):\n",
        "    evaluate_model(path, maehara_df, title, images_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWVXdG61kaXS",
        "outputId": "443c5065-9790-4517-df2f-778300b7153a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating baseline:  15%|█▍        | 36/241 [00:21<02:00,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット/HRS_118R.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating baseline:  15%|█▌        | 37/241 [00:22<01:59,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット/HRS_122R.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating baseline:  16%|█▌        | 38/241 [00:22<02:02,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット/HRS_126L.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating baseline:  16%|█▌        | 39/241 [00:23<02:01,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット/HRS_130R.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating baseline:  17%|█▋        | 40/241 [00:24<02:02,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット/HRS_141R.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating baseline:  17%|█▋        | 41/241 [00:24<02:03,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット/FKS_027_L.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating baseline:  17%|█▋        | 42/241 [00:25<02:00,  1.66it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maehara_df"
      ],
      "metadata": {
        "id": "aCY-E6Bqfmkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define the class names order\n",
        "class_names_order = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "df = maehara_df.dropna() #画像がなく評価が行われていない行を削除\n",
        "\n",
        "# Titles for the different model evaluations\n",
        "titles = [\"baseline\", \"mixed\", \"finetune_mixed\", \"finetune_100ep\", \"finetune_150ep\", \"finetune_200ep\", \"finetune_250ep\", \"finetune_300ep\", \"finetune_350ep\", \"finetune_400ep\"]\n",
        "\n",
        "def draw_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for top1_{title}')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.yticks(ticks=np.arange(len(class_names_order)), labels=class_names_order, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# Calculate and display confusion matrices\n",
        "for title in titles:\n",
        "    cm = confusion_matrix(df['disease_English'], df[f'top1_{title}'], labels=class_names_order)\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    accuracies[title] = accuracy\n",
        "    draw_confusion_matrix(cm, title)\n",
        "\n",
        "# Print all accuracies at the end\n",
        "for title, accuracy in accuracies.items():\n",
        "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "UtMAk6uZrPfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the destination path for the CSV file\n",
        "dst_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/maehara_df.xlsx\"\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "cornea_journal_df.to_csv(dst_path, index=False)"
      ],
      "metadata": {
        "id": "dqlVIZ1zrPhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5zPrMJLrPjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Augmentation_ideas**"
      ],
      "metadata": {
        "id": "OBSNECHYW6zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "# Define the path to your image\n",
        "images_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Iris_color/taki_dr_Cornea/taki_dr_dataset\"\n",
        "image_path = f\"{images_dir}/29.png\"\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "def rgb_to_bgr(img, **kwargs):\n",
        "    \"\"\" Randomly convert image from RGB to BGR with a probability p. \"\"\"\n",
        "    p = kwargs.get('p', 0.5)  # Get the probability value from kwargs, default is 0.5\n",
        "    if np.random.rand() < p:\n",
        "        img = img[..., ::-1]  # Reverse the order of the first and third channel\n",
        "    return img\n",
        "\n",
        "# Define an augmentation pipeline with Albumentations\n",
        "augmentation = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    #A.RandomBrightnessContrast(p=1),\n",
        "    A.ColorJitter(brightness=0.5, hue=0.3, p=0.01),\n",
        "    A.Lambda(image=rgb_to_bgr, p=0.01)  # Custom RGB to BGR conversion with a probability of 0.5\n",
        "])\n",
        "\n",
        "# Apply the augmentation pipeline to the image\n",
        "augmented_img = augmentation(image=img)['image']\n",
        "\n",
        "# Display the augmented image using matplotlib\n",
        "plt.imshow(augmented_img)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GCOz71B2XAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIJREB8qXAXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iq-t00iTXAYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwCaj-QgXAaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}